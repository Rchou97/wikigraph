revid,time,text
1015289346,2021-03-31 17:13:49,"
Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information.[1] BI technologies provide historical, current, and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]
Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an ""intelligence"" that cannot be derived from any singular set of data.[3] Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.[4]
BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as ""BI/DW""[5]
or as ""BIDW"". A data warehouse contains a copy of analytical data that facilitate decision support.
The earliest known use of the term business intelligence is in Richard Millar Devens' Cyclopædia of Commercial and Business Anecdotes (1865). Devens used the term to describe how the banker Sir Henry Furnese gained profit by receiving and acting upon information about his environment, prior to his competitors:
Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the fall of Namur added to his profits, owing to his early receipt of the news.The ability to collect and react accordingly based on the information retrieved, Devens says, is central to business intelligence.[6]
When Hans Peter Luhn, a researcher at IBM, used the term business intelligence in an article published in 1958, he employed the Webster's Dictionary definition of intelligence: ""the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.""[7] 
Business intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s.[8] DSS originated in the computer-aided models created to assist with decision making and planning.[citation needed]
In 1989, Howard Dresner (later a Gartner analyst) proposed business intelligence as an umbrella term to describe ""concepts and methods to improve business decision making by using fact-based support systems.""[9] It was not until the late 1990s that this usage was widespread.[10]
Critics[who?] see BI merely as an evolution of business reporting together with the advent of increasingly powerful and easy-to-use data analysis tools. In this respect it has also been criticized[by whom?] as a marketing buzzword in the context of the ""big data"" surge.[11]
According to Solomon Negash and Paul Gray, business intelligence (BI) can be defined as systems that combine:
with analysis to evaluate complex corporate and competitive information for presentation to planners and decision makers, with the objective of improving the timeliness and the quality of the input to the decision process.""[12]
According to Forrester Research, business intelligence is ""a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.""[13] Under this definition, business intelligence encompasses information management (data integration, data quality, data warehousing, master-data management, text- and content-analytics, et al.). Therefore, Forrester refers to data preparation and data usage as two separate but closely linked segments of the business-intelligence architectural stack.
Some elements of business intelligence are:[citation needed]
Forrester distinguishes this from the business-intelligence market, which is ""just the top layers of the BI architectural stack, such as reporting, analytics, and dashboards.""[14]
Though the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes, and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can include the subset of competitive intelligence.[15]
Business intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions.[16] Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an ""alerts"" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.[17]
Business operations can generate a very large amount of data in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material. According to Merrill Lynch, more than 85% of all business information exists in these forms; a company might only use such a document a single time.[18] Because of the way it is produced and stored, this information is either unstructured or semi-structured.
The management of semi-structured data is an unsolved problem in the information technology industry.[19] According to projections from Gartner (2003), white collar workers spend 30–40% of their time searching, finding, and assessing unstructured data. BI uses both structured and unstructured data. The former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision-making.[19][20] Because of the difficulty of properly searching, finding, and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task, or project. This can ultimately lead to poorly informed decision-making.[18]
Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.[20]
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row[21] or files of natural language that do not have detailed metadata.[22]
Many of these data types, however, like e-mails, word processing text files, PDFs, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database. Therefore, it may be more accurate to talk about this as semi-structured documents or data,[19] but no specific consensus seems to have been reached.
Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.
There are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,[23] some of those are:
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata.[18] Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content – e.g. summaries, topics, people, or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.
Business intelligence can be applied to the following business purposes:[24]
In a 2013 report, Gartner categorized business intelligence vendors as either an independent ""pure-play"" vendor or a consolidated ""megavendor"".[25][26] In 2012 business intelligence services received $13.1 billion in revenue.[27] In 2019, the BI market was shaken within Europe for the new legislation of GDPR (General Data Protection Regulation) which puts the responsibility of data collection and storage onto the data user with strict laws in place to make sure the data is compliant. Growth within Europe has steadily increased since May 2019 when GDPR was brought. The legislation refocused companies to look at their own data from a compliance perspective but also revealed future opportunity using personalization and external BI providers to increase market share.[28] 
"
1015289679,2021-03-31 17:15:55,"
Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information.[1] BI technologies provide historical, current, and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]
Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an ""intelligence"" that cannot be derived from any singular set of data.[3] Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.[4]
BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as ""BI/DW""[5]
or as ""BIDW"". A data warehouse contains a copy of analytical data that facilitate decision support.
The earliest known use of the term business intelligence is in Richard Millar Devens' Cyclopædia of Commercial and Business Anecdotes (1865). Devens used the term to describe how the banker Sir Henry Furnese gained profit by receiving and acting upon information about his environment, prior to his competitors:
Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the fall of Namur added to his profits, owing to his early receipt of the news.The ability to collect and react accordingly based on the information retrieved, Devens says, is central to business intelligence.[6]
When Hans Peter Luhn, a researcher at IBM, used the term business intelligence in an article published in 1958, he employed the Webster's Dictionary definition of intelligence: ""the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.""[7] 
Business intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s.[citation needed] DSS originated in the computer-aided models created to assist with decision making and planning.[citation needed]
In 1989, Howard Dresner (later a Gartner analyst) proposed business intelligence as an umbrella term to describe ""concepts and methods to improve business decision making by using fact-based support systems.""[8] It was not until the late 1990s that this usage was widespread.[9]
Critics[who?] see BI merely as an evolution of business reporting together with the advent of increasingly powerful and easy-to-use data analysis tools. In this respect it has also been criticized[by whom?] as a marketing buzzword in the context of the ""big data"" surge.[10]
According to Solomon Negash and Paul Gray, business intelligence (BI) can be defined as systems that combine:
with analysis to evaluate complex corporate and competitive information for presentation to planners and decision makers, with the objective of improving the timeliness and the quality of the input to the decision process.""[11]
According to Forrester Research, business intelligence is ""a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.""[12] Under this definition, business intelligence encompasses information management (data integration, data quality, data warehousing, master-data management, text- and content-analytics, et al.). Therefore, Forrester refers to data preparation and data usage as two separate but closely linked segments of the business-intelligence architectural stack.
Some elements of business intelligence are:[citation needed]
Forrester distinguishes this from the business-intelligence market, which is ""just the top layers of the BI architectural stack, such as reporting, analytics, and dashboards.""[13]
Though the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes, and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can include the subset of competitive intelligence.[14]
Business intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions.[15] Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an ""alerts"" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.[16]
Business operations can generate a very large amount of data in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material. According to Merrill Lynch, more than 85% of all business information exists in these forms; a company might only use such a document a single time.[17] Because of the way it is produced and stored, this information is either unstructured or semi-structured.
The management of semi-structured data is an unsolved problem in the information technology industry.[18] According to projections from Gartner (2003), white collar workers spend 30–40% of their time searching, finding, and assessing unstructured data. BI uses both structured and unstructured data. The former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision-making.[18][19] Because of the difficulty of properly searching, finding, and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task, or project. This can ultimately lead to poorly informed decision-making.[17]
Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.[19]
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row[20] or files of natural language that do not have detailed metadata.[21]
Many of these data types, however, like e-mails, word processing text files, PDFs, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database. Therefore, it may be more accurate to talk about this as semi-structured documents or data,[18] but no specific consensus seems to have been reached.
Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.
There are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,[22] some of those are:
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata.[17] Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content – e.g. summaries, topics, people, or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.
Business intelligence can be applied to the following business purposes:[23]
In a 2013 report, Gartner categorized business intelligence vendors as either an independent ""pure-play"" vendor or a consolidated ""megavendor"".[24][25] In 2012 business intelligence services received $13.1 billion in revenue.[26] In 2019, the BI market was shaken within Europe for the new legislation of GDPR (General Data Protection Regulation) which puts the responsibility of data collection and storage onto the data user with strict laws in place to make sure the data is compliant. Growth within Europe has steadily increased since May 2019 when GDPR was brought. The legislation refocused companies to look at their own data from a compliance perspective but also revealed future opportunity using personalization and external BI providers to increase market share.[27] 
"
1015815891,2021-04-03 18:42:29,"
Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information.[1] BI technologies provide historical, current, and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]
Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an ""intelligence"" that cannot be derived from any singular set of data.[3] Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.[4]
BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as ""BI/DW""[5]
or as ""BIDW"". A data warehouse contains a copy of analytical data that facilitate decision support.
The earliest known use of the term business intelligence is in Richard Millar Devens' Cyclopædia of Commercial and Business Anecdotes (1865). Devens used the term to describe how the banker Sir Henry Furnese gained profit by receiving and acting upon information about his environment, prior to his competitors:
Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the fall of Namur added to his profits, owing to his early receipt of the news.The ability to collect and react accordingly based on the information retrieved, Devens says, is central to business intelligence.[6]
When Hans Peter Luhn, a researcher at IBM, used the term business intelligence in an article published in 1958, he employed the Webster's Dictionary definition of intelligence: ""the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.""[7] 
Business intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s.[citation needed] DSS originated in the computer-aided models created to assist with decision making and planning.[citation needed]
In 1989, Howard Dresner (later a Gartner analyst) proposed business intelligence as an umbrella term to describe ""concepts and methods to improve business decision making by using fact-based support systems.""[8] It was not until the late 1990s that this usage was widespread.[9]
Critics[who?] see BI merely as an evolution of business reporting together with the advent of increasingly powerful and easy-to-use data analysis tools. In this respect it has also been criticized[by whom?] as a marketing buzzword in the context of the ""big data"" surge.[10]
According to Solomon Negash and Paul Gray, business intelligence (BI) can be defined as systems that combine:
with analysis to evaluate complex corporate and competitive information for presentation to planners and decision makers, with the objective of improving the timeliness and the quality of the input to the decision process.""[11]
According to Forrester Research, business intelligence is ""a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.""[12] Under this definition, business intelligence encompasses information management (data integration, data quality, data warehousing, master-data management, text- and content-analytics, et al.). Therefore, Forrester refers to data preparation and data usage as two separate but closely linked segments of the business-intelligence architectural stack.
Some elements of business intelligence are:[citation needed]
Forrester distinguishes this from the business-intelligence market, which is ""just the top layers of the BI architectural stack, such as reporting, analytics, and dashboards.""[13]
Though the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes, and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can be considered as a subset of competitive intelligence.[14]
Business intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions.[15] Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an ""alerts"" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.[16]
Business operations can generate a very large amount of data in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material. According to Merrill Lynch, more than 85% of all business information exists in these forms; a company might only use such a document a single time.[17] Because of the way it is produced and stored, this information is either unstructured or semi-structured.
The management of semi-structured data is an unsolved problem in the information technology industry.[18] According to projections from Gartner (2003), white collar workers spend 30–40% of their time searching, finding, and assessing unstructured data. BI uses both structured and unstructured data. The former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision-making.[18][19] Because of the difficulty of properly searching, finding, and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task, or project. This can ultimately lead to poorly informed decision-making.[17]
Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.[19]
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row[20] or files of natural language that do not have detailed metadata.[21]
Many of these data types, however, like e-mails, word processing text files, PDFs, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database. Therefore, it may be more accurate to talk about this as semi-structured documents or data,[18] but no specific consensus seems to have been reached.
Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.
There are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,[22] some of those are:
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata.[17] Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content – e.g. summaries, topics, people, or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.
Business intelligence can be applied to the following business purposes:[23]
In a 2013 report, Gartner categorized business intelligence vendors as either an independent ""pure-play"" vendor or a consolidated ""megavendor"".[24][25] In 2012 business intelligence services received $13.1 billion in revenue.[26] In 2019, the BI market was shaken within Europe for the new legislation of GDPR (General Data Protection Regulation) which puts the responsibility of data collection and storage onto the data user with strict laws in place to make sure the data is compliant. Growth within Europe has steadily increased since May 2019 when GDPR was brought. The legislation refocused companies to look at their own data from a compliance perspective but also revealed future opportunity using personalization and external BI providers to increase market share.[27] 
"
1018870665,2021-04-20 10:10:39,"
Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information.[1] BI technologies provide historical, current, and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]
Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an ""intelligence"" that cannot be derived from any singular set of data.[3] Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.[4]
BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as ""BI/DW""[5]
or as ""BIDW"". A data warehouse contains a copy of analytical data that facilitate decision support.
The earliest known use of the term business intelligence is in Richard Millar Devens' Cyclopædia of Commercial and Business Anecdotes (1865). Devens used the term to describe how the banker Sir Henry Furnese gained profit by receiving and acting upon information about his environment, prior to his competitors:
Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the fall of Namur added to his profits, owing to his early receipt of the news.The ability to collect and react accordingly based on the information retrieved, Devens says, is central to business intelligence.[6]
When Hans Peter Luhn, a researcher at IBM, used the term business intelligence in an article published in 1958, he employed the Webster's Dictionary definition of intelligence: ""the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.""[7] 
Business intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s.[citation needed] DSS originated in the computer-aided models created to assist with decision making and planning.[citation needed]
In 1989, Howard Dresner (later a Gartner analyst) proposed business intelligence as an umbrella term to describe ""concepts and methods to improve business decision making by using fact-based support systems.""[8] It was not until the late 1990s that this usage was widespread.[9]
Critics[who?] see BI merely as an evolution of business reporting together with the advent of increasingly powerful and easy-to-use data analysis tools. In this respect it has also been criticized[by whom?] as a marketing buzzword in the context of the ""big data"" surge.[10]
According to Solomon Negash and Paul Gray, business intelligence (BI) can be defined as systems that combine:
with analysis to evaluate complex corporate and competitive information for presentation to planners and decision makers, with the objective of improving the timeliness and the quality of the input to the decision process.""[11]
According to Forrester Research, business intelligence is ""a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.""[12] Under this definition, business intelligence encompasses information management (data integration, data quality, data warehousing, master-data management, text- and content-analytics, et al.). Therefore, Forrester refers to data preparation and data usage as two separate but closely linked segments of the business-intelligence architectural stack.
Some elements of business intelligence are:[citation needed]
Forrester distinguishes this from the business-intelligence market, which is ""just the top layers of the BI architectural stack, such as reporting, analytics, and dashboards.""[13]
Though the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes, and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can be considered as a subset of competitive intelligence.[14]
Business intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions.[15] Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an ""alerts"" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.[16]
Business operations can generate a very large amount of data in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material. According to Merrill Lynch, more than 85% of all business information exists in these forms; a company might only use such a document a single time.[17] Because of the way it is produced and stored, this information is either unstructured or semi-structured.
The management of semi-structured data is an unsolved problem in the information technology industry.[18] According to projections from Gartner (2003), white collar workers spend 30–40% of their time searching, finding, and assessing unstructured data. BI uses both structured and unstructured data. The former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision-making.[18][19] Because of the difficulty of properly searching, finding, and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task, or project. This can ultimately lead to poorly informed decision-making.[17]
Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.[19]
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row[20] or files of natural language that do not have detailed metadata.[21]
Many of these data types, however, like e-mails, word processing text files, PDFs, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database. Therefore, it may be more accurate to talk about this as semi-structured documents or data,[18] but no specific consensus seems to have been reached.
Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.
There are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,[22] some of those are:
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata.[17] Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content – e.g. summaries, topics, people, or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.
Business intelligence can be applied to the following business purposes:[23]
In a 2013 report, Gartner categorized business intelligence vendors as either an independent ""pure-play"" vendor or a consolidated ""megavendor"".[24][25] In 2012 business intelligence services received $13.1 billion in revenue.[26] In 2019, the BI market was shaken within Europe for the new legislation of GDPR (General Data Protection Regulation) which puts the responsibility of data collection and storage onto the data user with strict laws in place to make sure the data is compliant. Growth within Europe has steadily increased since May 2019 when GDPR was brought. The legislation refocused companies to look at their own data from a compliance perspective but also revealed future opportunity using personalization and external BI providers to increase market share.[27] 
"
1020704764,2021-04-30 15:57:42,"
Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information.[1] BI technologies provide historical, current, and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]
Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an ""intelligence"" that cannot be derived from any singular set of data.[3] Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.[4]
BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as ""BI/DW""[5]
or as ""BIDW"". A data warehouse contains a copy of analytical data that facilitate decision support.
The earliest known use of the term business intelligence is in Richard Millar Devens' Cyclopædia of Commercial and Business Anecdotes (1865). Devens used the term to describe how the banker Sir Henry Furnese gained profit by receiving and acting upon information about his environment, prior to his competitors:
Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the fall of Namur added to his profits, owing to his early receipt of the news.The ability to collect and react accordingly based on the information retrieved, Devens says, is central to business intelligence.[6]
When Hans Peter Luhn, a researcher at IBM, used the term business intelligence in an article published in 1958, he employed the Webster's Dictionary definition of intelligence: ""the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.""[7] 
Business intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s.[citation needed] DSS originated in the computer-aided models created to assist with decision making and planning.[citation needed]
In 1989, Howard Dresner (later a Gartner analyst) proposed business intelligence as an umbrella term to describe ""concepts and methods to improve business decision making by using fact-based support systems.""[8] It was not until the late 1990s that this usage was widespread.[9]
Critics[who?] see BI merely as an evolution of business reporting together with the advent of increasingly powerful and easy-to-use data analysis tools. In this respect it has also been criticized[by whom?] as a marketing buzzword in the context of the ""big data"" surge.[10]
According to Solomon Negash and Paul Gray, business intelligence (BI) can be defined as systems that combine:
with analysis to evaluate complex corporate and competitive information for presentation to planners and decision makers, with the objective of improving the timeliness and the quality of the input to the decision process.""[11]
According to Forrester Research, business intelligence is ""a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.""[12] Under this definition, business intelligence encompasses information management (data integration, data quality, data warehousing, master-data management, text- and content-analytics, et al.). Therefore, Forrester refers to data preparation and data usage as two separate but closely linked segments of the business-intelligence architectural stack.
Some elements of business intelligence are:[citation needed]
Forrester distinguishes this from the business-intelligence market, which is ""just the top layers of the BI architectural stack, such as reporting, analytics, and dashboards.""[13]
Though the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes, and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can be considered as a subset of competitive intelligence.[14]
Business intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions.[15] Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an ""alerts"" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.[16]
Business operations can generate a very large amount of data in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material. According to Merrill Lynch, more than 85% of all business information exists in these forms; a company might only use such a document a single time.[17] Because of the way it is produced and stored, this information is either unstructured or semi-structured.
The management of semi-structured data is an unsolved problem in the information technology industry.[18] According to projections from Gartner (2003), white collar workers spend 30–40% of their time searching, finding, and assessing unstructured data. BI uses both structured and unstructured data. The former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision-making.[18][19] Because of the difficulty of properly searching, finding, and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task, or project. This can ultimately lead to poorly informed decision-making.[17]
Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.[19]
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row[20] or files of natural language that do not have detailed metadata.[21]
Many of these data types, however, like e-mails, word processing text files, PDFs, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database. Therefore, it may be more accurate to talk about this as semi-structured documents or data,[18] but no specific consensus seems to have been reached.
Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.
There are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,[22] some of those are:
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata.[17] Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content – e.g. summaries, topics, people, or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.
Business intelligence can be applied to the following business purposes:[23]
In a 2013 report, Gartner categorized business intelligence vendors as either an independent ""pure-play"" vendor or a consolidated ""megavendor"".[24][25] In 2012 business intelligence services received $13.1 billion in revenue.[26] In 2019, the BI market was shaken within Europe for the new legislation of GDPR (General Data Protection Regulation) which puts the responsibility of data collection and storage onto the data user with strict laws in place to make sure the data is compliant. Growth within Europe has steadily increased since May 2019 when GDPR was brought. The legislation refocused companies to look at their own data from a compliance perspective but also revealed future opportunity using personalization and external BI providers to increase market share.[27] 
"
1021188377,2021-05-03 11:27:57,"
Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information.[1] BI technologies provide historical, current, and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]
Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an ""intelligence"" that cannot be derived from any singular set of data.[3] Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.[4]
BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as ""BI/DW""[5]
or as ""BIDW"". A data warehouse contains a copy of analytical data that facilitate decision support.
The earliest known use of the term business intelligence is in Richard Millar Devens' Cyclopædia of Commercial and Business Anecdotes (1865). Devens used the term to describe how the banker Sir Henry Furnese gained profit by receiving and acting upon information about his environment, prior to his competitors:
Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the fall of Namur added to his profits, owing to his early receipt of the news.The ability to collect and react accordingly based on the information retrieved, Devens says, is central to business intelligence.[6]
When Hans Peter Luhn, a researcher at IBM, used the term business intelligence in an article published in 1958, he employed the Webster's Dictionary definition of intelligence: ""the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.""[7] 
Business intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s.[citation needed] DSS originated in the computer-aided models created to assist with decision making and planning.[citation needed]
In 1989, Howard Dresner (later a Gartner analyst) proposed business intelligence as an umbrella term to describe ""concepts and methods to improve business decision making by using fact-based support systems.""[8] It was not until the late 1990s that this usage was widespread.[9]
Critics[who?] see BI merely as an evolution of business reporting together with the advent of increasingly powerful and easy-to-use data analysis tools. In this respect it has also been criticized[by whom?] as a marketing buzzword in the context of the ""big data"" surge.[10]
According to Solomon Negash and Paul Gray, business intelligence (BI) can be defined as systems that combine:
with analysis to evaluate complex corporate and competitive information for presentation to planners and decision makers, with the objective of improving the timeliness and the quality of the input to the decision process.""[11]
According to Forrester Research, business intelligence is ""a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.""[12] Under this definition, business intelligence encompasses information management (data integration, data quality, data warehousing, master-data management, text- and content-analytics, et al.). Therefore, Forrester refers to data preparation and data usage as two separate but closely linked segments of the business-intelligence architectural stack.
Some elements of business intelligence are:[citation needed]
Forrester distinguishes this from the business-intelligence market, which is ""just the top layers of the BI architectural stack, such as reporting, analytics, and dashboards.""[13]
Though the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes, and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can be considered as a subset of competitive intelligence.[14]
Business intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions.[15] Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an ""alerts"" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.[16]
Business operations can generate a very large amount of data in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material. According to Merrill Lynch, more than 85% of all business information exists in these forms; a company might only use such a document a single time.[17] Because of the way it is produced and stored, this information is either unstructured or semi-structured.
The management of semi-structured data is an unsolved problem in the information technology industry.[18] According to projections from Gartner (2003), white collar workers spend 30–40% of their time searching, finding, and assessing unstructured data. BI uses both structured and unstructured data. The former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision-making.[18][19] Because of the difficulty of properly searching, finding, and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task, or project. This can ultimately lead to poorly informed decision-making.[17]
Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.[19]
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row[20] or files of natural language that do not have detailed metadata.[21]
Many of these data types, however, like e-mails, word processing text files, PDFs, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database. Therefore, it may be more accurate to talk about this as semi-structured documents or data,[18] but no specific consensus seems to have been reached.
Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.
There are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,[22] some of those are:
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata.[17] Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content – e.g. summaries, topics, people, or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.
Business intelligence can be applied to the following business purposes:[23]
In a 2013 report, Gartner categorized business intelligence vendors as either an independent ""pure-play"" vendor or a consolidated ""megavendor"".[24][25] In 2012 business intelligence services received $13.1 billion in revenue.[26] In 2019, the BI market was shaken within Europe for the new legislation of GDPR (General Data Protection Regulation) which puts the responsibility of data collection and storage onto the data user with strict laws in place to make sure the data is compliant. Growth within Europe has steadily increased since May 2019 when GDPR was brought. The legislation refocused companies to look at their own data from a compliance perspective but also revealed future opportunity using personalization and external BI providers to increase market share.[27] 
https://prod-apnortheast-a.online.tableau.com/t/mistestcom/views/WorldIndicators/HealthIndicators?:showAppBanner=false&:display_count=n&:showVizHome=n&:origin=viz_share_link
"
1021188497,2021-05-03 11:28:58,"
Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information.[1] BI technologies provide historical, current, and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]
Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an ""intelligence"" that cannot be derived from any singular set of data.[3] Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.[4]
BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as ""BI/DW""[5]
or as ""BIDW"". A data warehouse contains a copy of analytical data that facilitate decision support.
The earliest known use of the term business intelligence is in Richard Millar Devens' Cyclopædia of Commercial and Business Anecdotes (1865). Devens used the term to describe how the banker Sir Henry Furnese gained profit by receiving and acting upon information about his environment, prior to his competitors:
Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the fall of Namur added to his profits, owing to his early receipt of the news.The ability to collect and react accordingly based on the information retrieved, Devens says, is central to business intelligence.[6]
When Hans Peter Luhn, a researcher at IBM, used the term business intelligence in an article published in 1958, he employed the Webster's Dictionary definition of intelligence: ""the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.""[7] 
Business intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s.[citation needed] DSS originated in the computer-aided models created to assist with decision making and planning.[citation needed]
In 1989, Howard Dresner (later a Gartner analyst) proposed business intelligence as an umbrella term to describe ""concepts and methods to improve business decision making by using fact-based support systems.""[8] It was not until the late 1990s that this usage was widespread.[9]
Critics[who?] see BI merely as an evolution of business reporting together with the advent of increasingly powerful and easy-to-use data analysis tools. In this respect it has also been criticized[by whom?] as a marketing buzzword in the context of the ""big data"" surge.[10]
According to Solomon Negash and Paul Gray, business intelligence (BI) can be defined as systems that combine:
with analysis to evaluate complex corporate and competitive information for presentation to planners and decision makers, with the objective of improving the timeliness and the quality of the input to the decision process.""[11]
According to Forrester Research, business intelligence is ""a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.""[12] Under this definition, business intelligence encompasses information management (data integration, data quality, data warehousing, master-data management, text- and content-analytics, et al.). Therefore, Forrester refers to data preparation and data usage as two separate but closely linked segments of the business-intelligence architectural stack.
Some elements of business intelligence are:[citation needed]
Forrester distinguishes this from the business-intelligence market, which is ""just the top layers of the BI architectural stack, such as reporting, analytics, and dashboards.""[13]
Though the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes, and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can be considered as a subset of competitive intelligence.[14]
Business intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions.[15] Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an ""alerts"" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.[16]
Business operations can generate a very large amount of data in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material. According to Merrill Lynch, more than 85% of all business information exists in these forms; a company might only use such a document a single time.[17] Because of the way it is produced and stored, this information is either unstructured or semi-structured.
The management of semi-structured data is an unsolved problem in the information technology industry.[18] According to projections from Gartner (2003), white collar workers spend 30–40% of their time searching, finding, and assessing unstructured data. BI uses both structured and unstructured data. The former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision-making.[18][19] Because of the difficulty of properly searching, finding, and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task, or project. This can ultimately lead to poorly informed decision-making.[17]
Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.[19]
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row[20] or files of natural language that do not have detailed metadata.[21]
Many of these data types, however, like e-mails, word processing text files, PDFs, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database. Therefore, it may be more accurate to talk about this as semi-structured documents or data,[18] but no specific consensus seems to have been reached.
Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.
There are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,[22] some of those are:
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata.[17] Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content – e.g. summaries, topics, people, or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.
Business intelligence can be applied to the following business purposes:[23]
In a 2013 report, Gartner categorized business intelligence vendors as either an independent ""pure-play"" vendor or a consolidated ""megavendor"".[24][25] In 2012 business intelligence services received $13.1 billion in revenue.[26] In 2019, the BI market was shaken within Europe for the new legislation of GDPR (General Data Protection Regulation) which puts the responsibility of data collection and storage onto the data user with strict laws in place to make sure the data is compliant. Growth within Europe has steadily increased since May 2019 when GDPR was brought. The legislation refocused companies to look at their own data from a compliance perspective but also revealed future opportunity using personalization and external BI providers to increase market share.[27] 
"
1021653584,2021-05-05 22:29:46,"
Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information.[1] BI technologies provide historical, current, and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]
Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an ""intelligence"" that cannot be derived from any singular set of data.[3] Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.[4]
BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as ""BI/DW""[5]
or as ""BIDW"". A data warehouse contains a copy of analytical data that facilitate decision support.
The earliest known use of the term business intelligence is in Richard Millar Devens' Cyclopædia of Commercial and Business Anecdotes (1865). Devens used the term to describe how the banker Sir Henry Furnese gained profit by receiving and acting upon information about his environment, prior to his competitors:
Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the fall of Namur added to his profits, owing to his early receipt of the news.The ability to collect and react accordingly based on the information retrieved, Devens says, is central to business intelligence.[6]
When Hans Peter Luhn, a researcher at IBM, used the term business intelligence in an article published in 1958, he employed the Webster's Dictionary definition of intelligence: ""the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.""[7] 
Business intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s.[citation needed] DSS originated in the computer-aided models created to assist with decision making and planning.[citation needed]
In 1989, Howard Dresner (later a Gartner analyst) proposed business intelligence as an umbrella term to describe ""concepts and methods to improve business decision making by using fact-based support systems.""[8] It was not until the late 1990s that this usage was widespread.[9]
Critics[who?] see BI merely as an evolution of business reporting together with the advent of increasingly powerful and easy-to-use data analysis tools. In this respect it has also been criticized[by whom?] as a marketing buzzword in the context of the ""big data"" surge.[10]
According to Solomon Negash and Paul Gray, business intelligence (BI) can be defined as systems that combine:
with analysis to evaluate complex corporate and competitive information for presentation to planners and decision makers, with the objective of improving the timeliness and the quality of the input to the decision process.""[11]
According to Forrester Research, business intelligence is ""a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.""[12] Under this definition, business intelligence encompasses information management (data integration, data quality, data warehousing, master-data management, text- and content-analytics, et al.). Therefore, Forrester refers to data preparation and data usage as two separate but closely linked segments of the business-intelligence architectural stack.
Some elements of business intelligence are:[citation needed]
Forrester distinguishes this from the business-intelligence market, which is ""just the top layers of the BI architectural stack, such as reporting, analytics, and dashboards.""[13]
Though the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes, and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can be considered as a subset of competitive intelligence.[14]
Business intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions.[15] Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an ""alerts"" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.[16]
Business operations can generate a very large amount of data in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material. According to Merrill Lynch, more than 85% of all business information exists in these forms; a company might only use such a document a single time.[17] Because of the way it is produced and stored, this information is either unstructured or semi-structured.
The management of semi-structured data is an unsolved problem in the information technology industry.[18] According to projections from Gartner (2003), white collar workers spend 30–40% of their time searching, finding, and assessing unstructured data. BI uses both structured and unstructured data. The former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision-making.[18][19] Because of the difficulty of properly searching, finding, and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task, or project. This can ultimately lead to poorly informed decision-making.[17]
Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.[19]
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row[20] or files of natural language that do not have detailed metadata.[21]
Many of these data types, however, like e-mails, word processing text files, PDFs, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database. Therefore, it may be more accurate to talk about this as semi-structured documents or data,[18] but no specific consensus seems to have been reached.
Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.
There are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,[22] some of those are:
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata.[17] Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content – e.g. summaries, topics, people, or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.
Business intelligence can be applied to the following business purposes:[23]
In a 2013 report, Gartner categorized business intelligence vendors as either an independent ""pure-play"" vendor or a consolidated ""megavendor"".[24][25] In 2012 business intelligence services received $13.1 billion in revenue.[26] In 2019, the BI market was shaken within Europe for the new legislation of GDPR (General Data Protection Regulation) which puts the responsibility of data collection and storage onto the data user with strict laws in place to make sure the data is compliant. Growth within Europe has steadily increased since May 2019 when GDPR was brought. The legislation refocused companies to look at their own data from a compliance perspective but also revealed future opportunity using personalization and external BI providers to increase market share.[27] 
"
1021727076,2021-05-06 10:19:16,"
Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information.[1] BI technologies provide historical, current, and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]
Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an ""intelligence"" that cannot be derived from any singular set of data.[3] Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.[4]
BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as ""BI/DW""[5]
or as ""BIDW"". A data warehouse contains a copy of analytical data that facilitate decision support. Recently the term and matter area of data-driven management has gained more importance and visibility due to the Fujitsu 2021 study ""The road to becoming a data-driven business"" based on which only 5 percent of organizations today can be considered as ""data-driven"". In this context the development of Business Intelligence was argued towards Predictive Intelligence (Seebacher 2021) as objective for Business Intelligence activities in order to enable Business Intelligence to not only reactive but also proactively predict future business decisions for data-driven management. 
The earliest known use of the term business intelligence is in Richard Millar Devens' Cyclopædia of Commercial and Business Anecdotes (1865). Devens used the term to describe how the banker Sir Henry Furnese gained profit by receiving and acting upon information about his environment, prior to his competitors:
Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the fall of Namur added to his profits, owing to his early receipt of the news.The ability to collect and react accordingly based on the information retrieved, Devens says, is central to business intelligence.[6]
When Hans Peter Luhn, a researcher at IBM, used the term business intelligence in an article published in 1958, he employed the Webster's Dictionary definition of intelligence: ""the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.""[7] 
Business intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s.[citation needed] DSS originated in the computer-aided models created to assist with decision making and planning.[citation needed]
In 1989, Howard Dresner (later a Gartner analyst) proposed business intelligence as an umbrella term to describe ""concepts and methods to improve business decision making by using fact-based support systems.""[8] It was not until the late 1990s that this usage was widespread.[9]
Critics[who?] see BI merely as an evolution of business reporting together with the advent of increasingly powerful and easy-to-use data analysis tools. In this respect it has also been criticized[by whom?] as a marketing buzzword in the context of the ""big data"" surge.[10]
According to Solomon Negash and Paul Gray, business intelligence (BI) can be defined as systems that combine:
with analysis to evaluate complex corporate and competitive information for presentation to planners and decision makers, with the objective of improving the timeliness and the quality of the input to the decision process.""[11]
According to Forrester Research, business intelligence is ""a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.""[12] Under this definition, business intelligence encompasses information management (data integration, data quality, data warehousing, master-data management, text- and content-analytics, et al.). Therefore, Forrester refers to data preparation and data usage as two separate but closely linked segments of the business-intelligence architectural stack.
Some elements of business intelligence are:[citation needed]
Forrester distinguishes this from the business-intelligence market, which is ""just the top layers of the BI architectural stack, such as reporting, analytics, and dashboards.""[13]
Though the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes, and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can be considered as a subset of competitive intelligence.[14]
Business intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions.[15] Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an ""alerts"" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.[16]
Business operations can generate a very large amount of data in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material. According to Merrill Lynch, more than 85% of all business information exists in these forms; a company might only use such a document a single time.[17] Because of the way it is produced and stored, this information is either unstructured or semi-structured.
The management of semi-structured data is an unsolved problem in the information technology industry.[18] According to projections from Gartner (2003), white collar workers spend 30–40% of their time searching, finding, and assessing unstructured data. BI uses both structured and unstructured data. The former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision-making.[18][19] Because of the difficulty of properly searching, finding, and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task, or project. This can ultimately lead to poorly informed decision-making.[17]
Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.[19]
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row[20] or files of natural language that do not have detailed metadata.[21]
Many of these data types, however, like e-mails, word processing text files, PDFs, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database. Therefore, it may be more accurate to talk about this as semi-structured documents or data,[18] but no specific consensus seems to have been reached.
Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.
There are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,[22] some of those are:
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata.[17] Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content – e.g. summaries, topics, people, or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.
Business intelligence can be applied to the following business purposes:[23]
In a 2013 report, Gartner categorized business intelligence vendors as either an independent ""pure-play"" vendor or a consolidated ""megavendor"".[24][25] In 2012 business intelligence services received $13.1 billion in revenue.[26] In 2019, the BI market was shaken within Europe for the new legislation of GDPR (General Data Protection Regulation) which puts the responsibility of data collection and storage onto the data user with strict laws in place to make sure the data is compliant. Growth within Europe has steadily increased since May 2019 when GDPR was brought. The legislation refocused companies to look at their own data from a compliance perspective but also revealed future opportunity using personalization and external BI providers to increase market share.[27] 
"
1021727734,2021-05-06 10:26:17,"
Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information.[1] BI technologies provide historical, current, and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]
Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an ""intelligence"" that cannot be derived from any singular set of data.[3] Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.[4]
BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as ""BI/DW""[5]
or as ""BIDW"". A data warehouse contains a copy of analytical data that facilitate decision support. Recently the term and matter area of data-driven management has gained more importance and visibility due to the Fujitsu 2021 study ""The road to becoming a data-driven business"" based on which only 5 percent of organizations today can be considered as ""data-driven"". In this context the development of Business Intelligence was argued towards Predictive Intelligence[6] as objective for Business Intelligence activities in order to enable Business Intelligence to not only reactive but also proactively predict future business decisions for data-driven management [7]
The earliest known use of the term business intelligence is in Richard Millar Devens' Cyclopædia of Commercial and Business Anecdotes (1865). Devens used the term to describe how the banker Sir Henry Furnese gained profit by receiving and acting upon information about his environment, prior to his competitors:
Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the fall of Namur added to his profits, owing to his early receipt of the news.The ability to collect and react accordingly based on the information retrieved, Devens says, is central to business intelligence.[8]
When Hans Peter Luhn, a researcher at IBM, used the term business intelligence in an article published in 1958, he employed the Webster's Dictionary definition of intelligence: ""the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.""[9] 
Business intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s.[citation needed] DSS originated in the computer-aided models created to assist with decision making and planning.[citation needed]
In 1989, Howard Dresner (later a Gartner analyst) proposed business intelligence as an umbrella term to describe ""concepts and methods to improve business decision making by using fact-based support systems.""[10] It was not until the late 1990s that this usage was widespread.[11]
Critics[who?] see BI merely as an evolution of business reporting together with the advent of increasingly powerful and easy-to-use data analysis tools. In this respect it has also been criticized[by whom?] as a marketing buzzword in the context of the ""big data"" surge.[12]
According to Solomon Negash and Paul Gray, business intelligence (BI) can be defined as systems that combine:
with analysis to evaluate complex corporate and competitive information for presentation to planners and decision makers, with the objective of improving the timeliness and the quality of the input to the decision process.""[13]
According to Forrester Research, business intelligence is ""a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.""[14] Under this definition, business intelligence encompasses information management (data integration, data quality, data warehousing, master-data management, text- and content-analytics, et al.). Therefore, Forrester refers to data preparation and data usage as two separate but closely linked segments of the business-intelligence architectural stack.
Some elements of business intelligence are:[citation needed]
Forrester distinguishes this from the business-intelligence market, which is ""just the top layers of the BI architectural stack, such as reporting, analytics, and dashboards.""[15]
Though the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes, and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can be considered as a subset of competitive intelligence.[16]
Business intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions.[17] Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an ""alerts"" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.[18]
Business operations can generate a very large amount of data in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material. According to Merrill Lynch, more than 85% of all business information exists in these forms; a company might only use such a document a single time.[19] Because of the way it is produced and stored, this information is either unstructured or semi-structured.
The management of semi-structured data is an unsolved problem in the information technology industry.[20] According to projections from Gartner (2003), white collar workers spend 30–40% of their time searching, finding, and assessing unstructured data. BI uses both structured and unstructured data. The former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision-making.[20][21] Because of the difficulty of properly searching, finding, and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task, or project. This can ultimately lead to poorly informed decision-making.[19]
Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.[21]
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row[22] or files of natural language that do not have detailed metadata.[23]
Many of these data types, however, like e-mails, word processing text files, PDFs, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database. Therefore, it may be more accurate to talk about this as semi-structured documents or data,[20] but no specific consensus seems to have been reached.
Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.
There are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,[24] some of those are:
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata.[19] Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content – e.g. summaries, topics, people, or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.
Business intelligence can be applied to the following business purposes:[25]
In a 2013 report, Gartner categorized business intelligence vendors as either an independent ""pure-play"" vendor or a consolidated ""megavendor"".[26][27] In 2012 business intelligence services received $13.1 billion in revenue.[28] In 2019, the BI market was shaken within Europe for the new legislation of GDPR (General Data Protection Regulation) which puts the responsibility of data collection and storage onto the data user with strict laws in place to make sure the data is compliant. Growth within Europe has steadily increased since May 2019 when GDPR was brought. The legislation refocused companies to look at their own data from a compliance perspective but also revealed future opportunity using personalization and external BI providers to increase market share.[29] 
"
1021731652,2021-05-06 11:01:15,"
Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information.[1] BI technologies provide historical, current, and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]
Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an ""intelligence"" that cannot be derived from any singular set of data.[3] Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.[4]
BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as ""BI/DW""[5]
or as ""BIDW"". A data warehouse contains a copy of analytical data that facilitate decision support. Recently the term and matter area of data-driven management has gained more importance and visibility due to the Fujitsu 2021 study ""The road to becoming a data-driven business"" based on which only 5 percent of organizations today can be considered as ""data-driven"". In this context the development of Business Intelligence was argued towards Predictive Intelligence[6] as objective for Business Intelligence activities in order to enable Business Intelligence to not only reactive but also proactively predict future business decisions for data-driven management [7]
The earliest known use of the term business intelligence is in Richard Millar Devens' Cyclopædia of Commercial and Business Anecdotes (1865). Devens used the term to describe how the banker Sir Henry Furnese gained profit by receiving and acting upon information about his environment, prior to his competitors:
Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the fall of Namur added to his profits, owing to his early receipt of the news.The ability to collect and react accordingly based on the information retrieved, Devens says, is central to business intelligence.[8]
When Hans Peter Luhn, a researcher at IBM, used the term business intelligence in an article published in 1958, he employed the Webster's Dictionary definition of intelligence: ""the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.""[9] 
Business intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s.[citation needed] DSS originated in the computer-aided models created to assist with decision making and planning.[citation needed]
In 1989, Howard Dresner (later a Gartner analyst) proposed business intelligence as an umbrella term to describe ""concepts and methods to improve business decision making by using fact-based support systems.""[10] It was not until the late 1990s that this usage was widespread.[11]
Critics[who?] see BI merely as an evolution of business reporting together with the advent of increasingly powerful and easy-to-use data analysis tools. In this respect it has also been criticized[by whom?] as a marketing buzzword in the context of the ""big data"" surge.[12]
According to Solomon Negash and Paul Gray, business intelligence (BI) can be defined as systems that combine:
with analysis to evaluate complex corporate and competitive information for presentation to planners and decision makers, with the objective of improving the timeliness and the quality of the input to the decision process.""[13]
According to Forrester Research, business intelligence is ""a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.""[14] Under this definition, business intelligence encompasses information management (data integration, data quality, data warehousing, master-data management, text- and content-analytics, et al.). Therefore, Forrester refers to data preparation and data usage as two separate but closely linked segments of the business-intelligence architectural stack.
Some elements of business intelligence are:[citation needed]
Forrester distinguishes this from the business-intelligence market, which is ""just the top layers of the BI architectural stack, such as reporting, analytics, and dashboards.""[15]
Though the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes, and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can be considered as a subset of competitive intelligence.[16]
Business intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions.[17] Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an ""alerts"" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.[18]
Business Intelligence works with data from the past to provide a valide basis for decision making. Predictive Intelligence[19] uses a multi-dimensional data model in the form of an N-2-N data cube together with primary, secondary and third-level algorithms for extrapolation and thus predicting data-based developments interactively and scenario-based. Therefore a specific PITechStack[20] has to be developed alongside a specific PI-HRStack[21] in the sense of a concise PI team.
Business operations can generate a very large amount of data in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material. According to Merrill Lynch, more than 85% of all business information exists in these forms; a company might only use such a document a single time.[22] Because of the way it is produced and stored, this information is either unstructured or semi-structured.
The management of semi-structured data is an unsolved problem in the information technology industry.[23] According to projections from Gartner (2003), white collar workers spend 30–40% of their time searching, finding, and assessing unstructured data. BI uses both structured and unstructured data. The former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision-making.[23][24] Because of the difficulty of properly searching, finding, and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task, or project. This can ultimately lead to poorly informed decision-making.[22]
Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.[24]
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row[25] or files of natural language that do not have detailed metadata.[26]
Many of these data types, however, like e-mails, word processing text files, PDFs, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database. Therefore, it may be more accurate to talk about this as semi-structured documents or data,[23] but no specific consensus seems to have been reached.
Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.
There are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,[27] some of those are:
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata.[22] Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content – e.g. summaries, topics, people, or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.
Business intelligence can be applied to the following business purposes:[28]
In a 2013 report, Gartner categorized business intelligence vendors as either an independent ""pure-play"" vendor or a consolidated ""megavendor"".[29][30] In 2012 business intelligence services received $13.1 billion in revenue.[31] In 2019, the BI market was shaken within Europe for the new legislation of GDPR (General Data Protection Regulation) which puts the responsibility of data collection and storage onto the data user with strict laws in place to make sure the data is compliant. Growth within Europe has steadily increased since May 2019 when GDPR was brought. The legislation refocused companies to look at their own data from a compliance perspective but also revealed future opportunity using personalization and external BI providers to increase market share.[32] 
"
1021731680,2021-05-06 11:01:26,"
Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information.[1] BI technologies provide historical, current, and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]
Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an ""intelligence"" that cannot be derived from any singular set of data.[3] Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.[4]
BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as ""BI/DW""[5]
or as ""BIDW"". A data warehouse contains a copy of analytical data that facilitate decision support.
The earliest known use of the term business intelligence is in Richard Millar Devens' Cyclopædia of Commercial and Business Anecdotes (1865). Devens used the term to describe how the banker Sir Henry Furnese gained profit by receiving and acting upon information about his environment, prior to his competitors:
Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the fall of Namur added to his profits, owing to his early receipt of the news.The ability to collect and react accordingly based on the information retrieved, Devens says, is central to business intelligence.[6]
When Hans Peter Luhn, a researcher at IBM, used the term business intelligence in an article published in 1958, he employed the Webster's Dictionary definition of intelligence: ""the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.""[7] 
Business intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s.[citation needed] DSS originated in the computer-aided models created to assist with decision making and planning.[citation needed]
In 1989, Howard Dresner (later a Gartner analyst) proposed business intelligence as an umbrella term to describe ""concepts and methods to improve business decision making by using fact-based support systems.""[8] It was not until the late 1990s that this usage was widespread.[9]
Critics[who?] see BI merely as an evolution of business reporting together with the advent of increasingly powerful and easy-to-use data analysis tools. In this respect it has also been criticized[by whom?] as a marketing buzzword in the context of the ""big data"" surge.[10]
According to Solomon Negash and Paul Gray, business intelligence (BI) can be defined as systems that combine:
with analysis to evaluate complex corporate and competitive information for presentation to planners and decision makers, with the objective of improving the timeliness and the quality of the input to the decision process.""[11]
According to Forrester Research, business intelligence is ""a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.""[12] Under this definition, business intelligence encompasses information management (data integration, data quality, data warehousing, master-data management, text- and content-analytics, et al.). Therefore, Forrester refers to data preparation and data usage as two separate but closely linked segments of the business-intelligence architectural stack.
Some elements of business intelligence are:[citation needed]
Forrester distinguishes this from the business-intelligence market, which is ""just the top layers of the BI architectural stack, such as reporting, analytics, and dashboards.""[13]
Though the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes, and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can be considered as a subset of competitive intelligence.[14]
Business intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions.[15] Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an ""alerts"" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.[16]
Business operations can generate a very large amount of data in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material. According to Merrill Lynch, more than 85% of all business information exists in these forms; a company might only use such a document a single time.[17] Because of the way it is produced and stored, this information is either unstructured or semi-structured.
The management of semi-structured data is an unsolved problem in the information technology industry.[18] According to projections from Gartner (2003), white collar workers spend 30–40% of their time searching, finding, and assessing unstructured data. BI uses both structured and unstructured data. The former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision-making.[18][19] Because of the difficulty of properly searching, finding, and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task, or project. This can ultimately lead to poorly informed decision-making.[17]
Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.[19]
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row[20] or files of natural language that do not have detailed metadata.[21]
Many of these data types, however, like e-mails, word processing text files, PDFs, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database. Therefore, it may be more accurate to talk about this as semi-structured documents or data,[18] but no specific consensus seems to have been reached.
Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.
There are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,[22] some of those are:
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata.[17] Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content – e.g. summaries, topics, people, or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.
Business intelligence can be applied to the following business purposes:[23]
In a 2013 report, Gartner categorized business intelligence vendors as either an independent ""pure-play"" vendor or a consolidated ""megavendor"".[24][25] In 2012 business intelligence services received $13.1 billion in revenue.[26] In 2019, the BI market was shaken within Europe for the new legislation of GDPR (General Data Protection Regulation) which puts the responsibility of data collection and storage onto the data user with strict laws in place to make sure the data is compliant. Growth within Europe has steadily increased since May 2019 when GDPR was brought. The legislation refocused companies to look at their own data from a compliance perspective but also revealed future opportunity using personalization and external BI providers to increase market share.[27] 
"
1021732883,2021-05-06 11:12:20,"
Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information.[1] BI technologies provide historical, current, and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]
Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an ""intelligence"" that cannot be derived from any singular set of data.[3] Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.[4]
BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as ""BI/DW""[5]
or as ""BIDW"". A data warehouse contains a copy of analytical data that facilitate decision support.Recently the term and matter area of data-driven management has gained more importance and visibility due to the Fujitsu 2021 study ""The road to becoming a data-driven business"" based on which only 5 percent of organizations today can be considered as ""data-driven"". In this context the development of Business Intelligence was argued towards Predictive Intelligence[6] as objective for Business Intelligence activities in order to enable Business Intelligence to not only reactive but also proactively predict future business decisions for data-driven management [7]
The earliest known use of the term business intelligence is in Richard Millar Devens' Cyclopædia of Commercial and Business Anecdotes (1865). Devens used the term to describe how the banker Sir Henry Furnese gained profit by receiving and acting upon information about his environment, prior to his competitors:
Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the fall of Namur added to his profits, owing to his early receipt of the news.The ability to collect and react accordingly based on the information retrieved, Devens says, is central to business intelligence.[8]
When Hans Peter Luhn, a researcher at IBM, used the term business intelligence in an article published in 1958, he employed the Webster's Dictionary definition of intelligence: ""the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.""[9] 
Business intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s.[citation needed] DSS originated in the computer-aided models created to assist with decision making and planning.[citation needed]
In 1989, Howard Dresner (later a Gartner analyst) proposed business intelligence as an umbrella term to describe ""concepts and methods to improve business decision making by using fact-based support systems.""[10] It was not until the late 1990s that this usage was widespread.[11]
Critics[who?] see BI merely as an evolution of business reporting together with the advent of increasingly powerful and easy-to-use data analysis tools. In this respect it has also been criticized[by whom?] as a marketing buzzword in the context of the ""big data"" surge.[12]
According to Solomon Negash and Paul Gray, business intelligence (BI) can be defined as systems that combine:
with analysis to evaluate complex corporate and competitive information for presentation to planners and decision makers, with the objective of improving the timeliness and the quality of the input to the decision process.""[13]
According to Forrester Research, business intelligence is ""a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.""[14] Under this definition, business intelligence encompasses information management (data integration, data quality, data warehousing, master-data management, text- and content-analytics, et al.). Therefore, Forrester refers to data preparation and data usage as two separate but closely linked segments of the business-intelligence architectural stack.
Some elements of business intelligence are:[citation needed]
Forrester distinguishes this from the business-intelligence market, which is ""just the top layers of the BI architectural stack, such as reporting, analytics, and dashboards.""[15]
Though the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes, and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can be considered as a subset of competitive intelligence.[16]
Business intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions.[17] Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an ""alerts"" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.[18]
Business operations can generate a very large amount of data in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material. According to Merrill Lynch, more than 85% of all business information exists in these forms; a company might only use such a document a single time.[19] Because of the way it is produced and stored, this information is either unstructured or semi-structured.
The management of semi-structured data is an unsolved problem in the information technology industry.[20] According to projections from Gartner (2003), white collar workers spend 30–40% of their time searching, finding, and assessing unstructured data. BI uses both structured and unstructured data. The former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision-making.[20][21] Because of the difficulty of properly searching, finding, and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task, or project. This can ultimately lead to poorly informed decision-making.[19]
Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.[21]
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row[22] or files of natural language that do not have detailed metadata.[23]
Many of these data types, however, like e-mails, word processing text files, PDFs, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database. Therefore, it may be more accurate to talk about this as semi-structured documents or data,[20] but no specific consensus seems to have been reached.
Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.
There are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,[24] some of those are:
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata.[19] Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content – e.g. summaries, topics, people, or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.
Business intelligence can be applied to the following business purposes:[25]
In a 2013 report, Gartner categorized business intelligence vendors as either an independent ""pure-play"" vendor or a consolidated ""megavendor"".[26][27] In 2012 business intelligence services received $13.1 billion in revenue.[28] In 2019, the BI market was shaken within Europe for the new legislation of GDPR (General Data Protection Regulation) which puts the responsibility of data collection and storage onto the data user with strict laws in place to make sure the data is compliant. Growth within Europe has steadily increased since May 2019 when GDPR was brought. The legislation refocused companies to look at their own data from a compliance perspective but also revealed future opportunity using personalization and external BI providers to increase market share.[29] 
"
1021733731,2021-05-06 11:19:39,"
Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information.[1] BI technologies provide historical, current, and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]
Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an ""intelligence"" that cannot be derived from any singular set of data.[3] Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.[4]
BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as ""BI/DW""[5]
or as ""BIDW"". A data warehouse contains a copy of analytical data that facilitate decision support.Recently the term and matter area of data-driven management has gained more importance and visibility due to the Fujitsu 2021 study ""The road to becoming a data-driven business"" based on which only 5 percent of organizations today can be considered as ""data-driven"". In this context the development of Business Intelligence was argued towards Predictive Intelligence[6] as objective for Business Intelligence activities in order to enable Business Intelligence to not only reactive but also proactively predict future business decisions for data-driven management [7]
The earliest known use of the term business intelligence is in Richard Millar Devens' Cyclopædia of Commercial and Business Anecdotes (1865). Devens used the term to describe how the banker Sir Henry Furnese gained profit by receiving and acting upon information about his environment, prior to his competitors:
Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the fall of Namur added to his profits, owing to his early receipt of the news.The ability to collect and react accordingly based on the information retrieved, Devens says, is central to business intelligence.[8]
When Hans Peter Luhn, a researcher at IBM, used the term business intelligence in an article published in 1958, he employed the Webster's Dictionary definition of intelligence: ""the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.""[9] 
Business intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s.[citation needed] DSS originated in the computer-aided models created to assist with decision making and planning.[citation needed]
In 1989, Howard Dresner (later a Gartner analyst) proposed business intelligence as an umbrella term to describe ""concepts and methods to improve business decision making by using fact-based support systems.""[10] It was not until the late 1990s that this usage was widespread.[11]
Critics[who?] see BI merely as an evolution of business reporting together with the advent of increasingly powerful and easy-to-use data analysis tools. In this respect it has also been criticized[by whom?] as a marketing buzzword in the context of the ""big data"" surge.[12]
According to Solomon Negash and Paul Gray, business intelligence (BI) can be defined as systems that combine:
with analysis to evaluate complex corporate and competitive information for presentation to planners and decision makers, with the objective of improving the timeliness and the quality of the input to the decision process.""[13]
According to Forrester Research, business intelligence is ""a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.""[14] Under this definition, business intelligence encompasses information management (data integration, data quality, data warehousing, master-data management, text- and content-analytics, et al.). Therefore, Forrester refers to data preparation and data usage as two separate but closely linked segments of the business-intelligence architectural stack.
Some elements of business intelligence are:[citation needed]
Forrester distinguishes this from the business-intelligence market, which is ""just the top layers of the BI architectural stack, such as reporting, analytics, and dashboards.""[15]
Though the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes, and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can be considered as a subset of competitive intelligence.[16]
Business intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions.[17] Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an ""alerts"" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.[18]
Business intelligence and business analytics do not use data for predictive analytics as the terms per se define. Thus, the difference between BI and BA in regard to Predictive Intelligence (PI) is the fact that PI uses primary, secondary and third-level algorithms – also AI-based – for extrapolating data enabling the prediction of developments. This happens dynamically and scenario-based, using a specific IT-Stack[19] for PI and a certain nine PI competences[20]. BI in this context provides the essential basis for the growth towards Predictive Intelligence.

Business operations can generate a very large amount of data in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material. According to Merrill Lynch, more than 85% of all business information exists in these forms; a company might only use such a document a single time.[21] Because of the way it is produced and stored, this information is either unstructured or semi-structured.
The management of semi-structured data is an unsolved problem in the information technology industry.[22] According to projections from Gartner (2003), white collar workers spend 30–40% of their time searching, finding, and assessing unstructured data. BI uses both structured and unstructured data. The former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision-making.[22][23] Because of the difficulty of properly searching, finding, and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task, or project. This can ultimately lead to poorly informed decision-making.[21]
Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.[23]
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row[24] or files of natural language that do not have detailed metadata.[25]
Many of these data types, however, like e-mails, word processing text files, PDFs, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database. Therefore, it may be more accurate to talk about this as semi-structured documents or data,[22] but no specific consensus seems to have been reached.
Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.
There are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,[26] some of those are:
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata.[21] Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content – e.g. summaries, topics, people, or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.
Business intelligence can be applied to the following business purposes:[27]
In a 2013 report, Gartner categorized business intelligence vendors as either an independent ""pure-play"" vendor or a consolidated ""megavendor"".[28][29] In 2012 business intelligence services received $13.1 billion in revenue.[30] In 2019, the BI market was shaken within Europe for the new legislation of GDPR (General Data Protection Regulation) which puts the responsibility of data collection and storage onto the data user with strict laws in place to make sure the data is compliant. Growth within Europe has steadily increased since May 2019 when GDPR was brought. The legislation refocused companies to look at their own data from a compliance perspective but also revealed future opportunity using personalization and external BI providers to increase market share.[31] 
"
1021734055,2021-05-06 11:22:17,"
Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information.[1] BI technologies provide historical, current, and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]
Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an ""intelligence"" that cannot be derived from any singular set of data.[3] Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.[4]
BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as ""BI/DW""[5]
or as ""BIDW"". A data warehouse contains a copy of analytical data that facilitate decision support.Recently the term and matter area of data-driven management has gained more importance and visibility due to the Fujitsu 2021 study ""The road to becoming a data-driven business"" based on which only 5 percent of organizations today can be considered as ""data-driven"". In this context the development of Business Intelligence was argued towards Predictive Intelligence[6] as objective for Business Intelligence activities in order to enable Business Intelligence to not only reactive but also proactively predict future business decisions for data-driven management [7]
The earliest known use of the term business intelligence is in Richard Millar Devens' Cyclopædia of Commercial and Business Anecdotes (1865). Devens used the term to describe how the banker Sir Henry Furnese gained profit by receiving and acting upon information about his environment, prior to his competitors:
Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the fall of Namur added to his profits, owing to his early receipt of the news.The ability to collect and react accordingly based on the information retrieved, Devens says, is central to business intelligence.[8]
When Hans Peter Luhn, a researcher at IBM, used the term business intelligence in an article published in 1958, he employed the Webster's Dictionary definition of intelligence: ""the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.""[9] 
Business intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s.[citation needed] DSS originated in the computer-aided models created to assist with decision making and planning.[citation needed]
In 1989, Howard Dresner (later a Gartner analyst) proposed business intelligence as an umbrella term to describe ""concepts and methods to improve business decision making by using fact-based support systems.""[10] It was not until the late 1990s that this usage was widespread.[11]
Critics[who?] see BI merely as an evolution of business reporting together with the advent of increasingly powerful and easy-to-use data analysis tools. In this respect it has also been criticized[by whom?] as a marketing buzzword in the context of the ""big data"" surge.[12]
According to Solomon Negash and Paul Gray, business intelligence (BI) can be defined as systems that combine:
with analysis to evaluate complex corporate and competitive information for presentation to planners and decision makers, with the objective of improving the timeliness and the quality of the input to the decision process.""[13]
According to Forrester Research, business intelligence is ""a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.""[14] Under this definition, business intelligence encompasses information management (data integration, data quality, data warehousing, master-data management, text- and content-analytics, et al.). Therefore, Forrester refers to data preparation and data usage as two separate but closely linked segments of the business-intelligence architectural stack.
Some elements of business intelligence are:[citation needed]
Forrester distinguishes this from the business-intelligence market, which is ""just the top layers of the BI architectural stack, such as reporting, analytics, and dashboards.""[15]
Though the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes, and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can be considered as a subset of competitive intelligence.[16]
Business intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions.[17] Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an ""alerts"" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.[18]
Business intelligence and business analytics do not use data for predictive analytics as the terms per se define. Thus, the difference between BI and BA in regard to Predictive Intelligence (PI) is the fact that PI uses primary, secondary and third-level algorithms – also AI-based – for extrapolating data enabling the prediction of developments. This happens dynamically and scenario-based, using a specific IT-Stack[19] for PI and specific nine PI competences[20]. BI in this context provides the essential basis for the growth towards Predictive Intelligence.
Business operations can generate a very large amount of data in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material. According to Merrill Lynch, more than 85% of all business information exists in these forms; a company might only use such a document a single time.[21] Because of the way it is produced and stored, this information is either unstructured or semi-structured.
The management of semi-structured data is an unsolved problem in the information technology industry.[22] According to projections from Gartner (2003), white collar workers spend 30–40% of their time searching, finding, and assessing unstructured data. BI uses both structured and unstructured data. The former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision-making.[22][23] Because of the difficulty of properly searching, finding, and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task, or project. This can ultimately lead to poorly informed decision-making.[21]
Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.[23]
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row[24] or files of natural language that do not have detailed metadata.[25]
Many of these data types, however, like e-mails, word processing text files, PDFs, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database. Therefore, it may be more accurate to talk about this as semi-structured documents or data,[22] but no specific consensus seems to have been reached.
Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.
There are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,[26] some of those are:
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata.[21] Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content – e.g. summaries, topics, people, or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.
Business intelligence can be applied to the following business purposes:[27]
In a 2013 report, Gartner categorized business intelligence vendors as either an independent ""pure-play"" vendor or a consolidated ""megavendor"".[28][29] In 2012 business intelligence services received $13.1 billion in revenue.[30] In 2019, the BI market was shaken within Europe for the new legislation of GDPR (General Data Protection Regulation) which puts the responsibility of data collection and storage onto the data user with strict laws in place to make sure the data is compliant. Growth within Europe has steadily increased since May 2019 when GDPR was brought. The legislation refocused companies to look at their own data from a compliance perspective but also revealed future opportunity using personalization and external BI providers to increase market share.[31] 
"
1021754882,2021-05-06 14:07:17,"
Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information.[1] BI technologies provide historical, current, and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]
Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an ""intelligence"" that cannot be derived from any singular set of data.[3] Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.[4]
BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as ""BI/DW""[5]
or as ""BIDW"". A data warehouse contains a copy of analytical data that facilitate decision support.
The earliest known use of the term business intelligence is in Richard Millar Devens' Cyclopædia of Commercial and Business Anecdotes (1865). Devens used the term to describe how the banker Sir Henry Furnese gained profit by receiving and acting upon information about his environment, prior to his competitors:
Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the fall of Namur added to his profits, owing to his early receipt of the news.The ability to collect and react accordingly based on the information retrieved, Devens says, is central to business intelligence.[6]
When Hans Peter Luhn, a researcher at IBM, used the term business intelligence in an article published in 1958, he employed the Webster's Dictionary definition of intelligence: ""the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.""[7] 
Business intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s.[citation needed] DSS originated in the computer-aided models created to assist with decision making and planning.[citation needed]
In 1989, Howard Dresner (later a Gartner analyst) proposed business intelligence as an umbrella term to describe ""concepts and methods to improve business decision making by using fact-based support systems.""[8] It was not until the late 1990s that this usage was widespread.[9]
Critics[who?] see BI merely as an evolution of business reporting together with the advent of increasingly powerful and easy-to-use data analysis tools. In this respect it has also been criticized[by whom?] as a marketing buzzword in the context of the ""big data"" surge.[10]
According to Solomon Negash and Paul Gray, business intelligence (BI) can be defined as systems that combine:
with analysis to evaluate complex corporate and competitive information for presentation to planners and decision makers, with the objective of improving the timeliness and the quality of the input to the decision process.""[11]
According to Forrester Research, business intelligence is ""a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.""[12] Under this definition, business intelligence encompasses information management (data integration, data quality, data warehousing, master-data management, text- and content-analytics, et al.). Therefore, Forrester refers to data preparation and data usage as two separate but closely linked segments of the business-intelligence architectural stack.
Some elements of business intelligence are:[citation needed]
Forrester distinguishes this from the business-intelligence market, which is ""just the top layers of the BI architectural stack, such as reporting, analytics, and dashboards.""[13]
Though the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes, and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can be considered as a subset of competitive intelligence.[14]
Business intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions.[15] Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an ""alerts"" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.[16]
Business operations can generate a very large amount of data in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material. According to Merrill Lynch, more than 85% of all business information exists in these forms; a company might only use such a document a single time.[17] Because of the way it is produced and stored, this information is either unstructured or semi-structured.
The management of semi-structured data is an unsolved problem in the information technology industry.[18] According to projections from Gartner (2003), white collar workers spend 30–40% of their time searching, finding, and assessing unstructured data. BI uses both structured and unstructured data. The former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision-making.[18][19] Because of the difficulty of properly searching, finding, and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task, or project. This can ultimately lead to poorly informed decision-making.[17]
Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.[19]
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row[20] or files of natural language that do not have detailed metadata.[21]
Many of these data types, however, like e-mails, word processing text files, PDFs, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database. Therefore, it may be more accurate to talk about this as semi-structured documents or data,[18] but no specific consensus seems to have been reached.
Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.
There are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,[22] some of those are:
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata.[17] Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content – e.g. summaries, topics, people, or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.
Business intelligence can be applied to the following business purposes:[23]
In a 2013 report, Gartner categorized business intelligence vendors as either an independent ""pure-play"" vendor or a consolidated ""megavendor"".[24][25] In 2012 business intelligence services received $13.1 billion in revenue.[26] In 2019, the BI market was shaken within Europe for the new legislation of GDPR (General Data Protection Regulation) which puts the responsibility of data collection and storage onto the data user with strict laws in place to make sure the data is compliant. Growth within Europe has steadily increased since May 2019 when GDPR was brought. The legislation refocused companies to look at their own data from a compliance perspective but also revealed future opportunity using personalization and external BI providers to increase market share.[27] 
"
1025280293,2021-05-26 18:27:02,"
Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information.[1] BI technologies provide historical, current, and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]
Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an ""intelligence"" that cannot be derived from any singular set of data.[3] Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.[4]
BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as ""BI/DW""[5]
or as ""BIDW"". A data warehouse contains a copy of analytical data that facilitate decision support.
The earliest known use of the term business intelligence is in Richard Millar Devens' Cyclopædia of Commercial and Business Anecdotes (1865). Devens used the term to describe how the banker Sir Henry Furnese gained profit by receiving and acting upon information about his environment, prior to his competitors:
Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the fall of Namur added to his profits, owing to his early receipt of the news.The ability to collect and react accordingly based on the information retrieved, Devens says, is central to business intelligence.[6]
When Hans Peter Luhn, a researcher at IBM, used the term business intelligence in an article published in 1958, he employed the Webster's Dictionary definition of intelligence: ""the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.""[7] 
Business intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s.[citation needed] DSS originated in the computer-aided models created to assist with decision making and planning.[citation needed]
In 1989, Howard Dresner (later a Gartner analyst) proposed business intelligence as an umbrella term to describe ""concepts and methods to improve business decision making by using fact-based support systems.""[8] It was not until the late 1990s that this usage was widespread.[9]
Critics[who?] see BI merely as an evolution of business reporting together with the advent of increasingly powerful and easy-to-use data analysis tools. In this respect it has also been criticized[by whom?] as a marketing buzzword in the context of the ""big data"" surge.[10]
According to Solomon Negash and Paul Gray, business intelligence (BI) can be defined as systems that combine:
with analysis to evaluate complex corporate and competitive information for presentation to planners and decision makers, with the objective of improving the timeliness and the quality of the input to the decision process.""[11]
According to Forrester Research, business intelligence is ""a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.""[12] Under this definition, business intelligence encompasses information management (data integration, data quality, data warehousing, master-data management, text- and content-analytics, et al.). Therefore, Forrester refers to data preparation and data usage as two separate but closely linked segments of the business-intelligence architectural stack.
Some elements of business intelligence are:[citation needed]
Forrester distinguishes this from the business-intelligence market, which is ""just the top layers of the BI architectural stack, such as reporting, analytics, and dashboards.""[13]
Though the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes, and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can be considered as a subset of competitive intelligence.[14]
Business intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions.[15] Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an ""alerts"" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.[16]
Business operations can generate a very large amount of data in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material. According to Merrill Lynch, more than 85% of all business information exists in these forms; a company might only use such a document a single time.[17] Because of the way it is produced and stored, this information is either unstructured or semi-structured.
The management of semi-structured data is an unsolved problem in the information technology industry.[18] According to projections from Gartner (2003), white collar workers spend 30–40% of their time searching, finding, and assessing unstructured data. BI uses both structured and unstructured data. The former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision-making.[18][19] Because of the difficulty of properly searching, finding, and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task, or project. This can ultimately lead to poorly informed decision-making.[17]
Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row[20] or files of natural language that do not have detailed metadata.[21]
Many of these data types, however, like e-mails, word processing text files, PDFs, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database. Therefore, it may be more accurate to talk about this as semi-structured documents or data,[18] but no specific consensus seems to have been reached.
Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.
There are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,[22] some of those are:
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata.[17] Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content – e.g. summaries, topics, people, or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.
Business intelligence can be applied to the following business purposes:[23]
In a 2013 report, Gartner categorized business intelligence vendors as either an independent ""pure-play"" vendor or a consolidated ""megavendor"".[24][25] In 2012 business intelligence services received $13.1 billion in revenue.[26] In 2019, the BI market was shaken within Europe for the new legislation of GDPR (General Data Protection Regulation) which puts the responsibility of data collection and storage onto the data user with strict laws in place to make sure the data is compliant. Growth within Europe has steadily increased since May 2019 when GDPR was brought. The legislation refocused companies to look at their own data from a compliance perspective but also revealed future opportunity using personalization and external BI providers to increase market share.[27] 
"
925217532,2019-11-08 16:40:54,"Competitive intelligence (CI) is the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers in strategic decision making for an organization.
CI means understanding and learning what is happening in the world outside the business to increase one's competitiveness. It means learning as much as possible, as soon as possible, about one's external environment including one's industry in general and relevant competitors.[1]
Key points:
Another definition of CI regards it as the organizational function responsible for the early identification of risks and opportunities in the market before they become obvious (""early signal analysis""). This definition focuses attention on the difference between dissemination of widely available factual information (such as market statistics, financial reports, newspaper clippings) performed by functions such as libraries and information centers, and competitive intelligence which is a perspective on developments and events aimed at yielding a competitive edge.[5]
The term CI is often viewed as synonymous with competitor analysis, but competitive intelligence is more than analyzing competitors; it embraces the entire environment and stakeholders: customers, competitors, distributors, technologies, and macroeconomic data.
CI literature is best exemplified by the bibliographies that were published in the Society of Competitive Intelligence Professionals' academic journal The Journal of Competitive Intelligence and Management.[6][7][8][9] Although elements of organizational intelligence collection have been a part of business for many years, the history of competitive intelligence arguably began in the U.S. in the 1970s, although the literature on the field pre-dates this time by at least several decades.[9] In 1980, Michael Porter published the study Competitive-Strategy: Techniques for Analyzing Industries and Competitors which is widely viewed as the foundation of modern competitive intelligence. This has since been extended most notably by the pair of Craig Fleisher and Babette Bensoussan, who through several popular books on competitive analysis have added 48 commonly applied competitive intelligence analysis techniques to the practitioner's tool box.[10][11] In 1985, Leonard Fuld published his best selling book dedicated to competitor intelligence.[12] However, the institutionalization of CI as a formal activity among American corporations can be traced to 1988, when Ben and Tamar Gilad published the first organizational model of a formal corporate CI function, which was then adopted widely by US companies.[13] The first professional certification program (CIP) was created in 1996 with the establishment of The Fuld-Gilad-Herring Academy of Competitive Intelligence in Cambridge, Massachusetts.
In 1986 the Society of Competitive Intelligence Professionals (SCIP) was founded in the United States and grew in the late 1990s to around 6,000 members worldwide, mainly in the United States and Canada, but with large numbers especially in the UK and Australia. Due to financial difficulties in 2009, the organization merged with Frost & Sullivan under the Frost & Sullivan Institute. SCIP has since been renamed ""Strategic & Competitive Intelligence Professionals"" to emphasise the strategic nature of the subject, and also to refocus the organisation's general approach, while keeping the existing SCIP brandname and logo. A number of efforts have been made to discuss the field's advances in post-secondary (university) education, covered by several authors including Blenkhorn & Fleisher,[14] Fleisher,[15] Fuld,[16] Prescott,[17] and McGonagle.[18] Although the general view would be that competitive intelligence concepts can be readily found and taught in many business schools around the globe, there are still relatively few dedicated academic programs, majors, or degrees in the field, a concern to academics in the field who would like to see it further researched.[15] These issues were widely discussed by over a dozen knowledgeable individuals in a special edition of the Competitive Intelligence Magazine that was dedicated to this topic.[19] In France, a Specialized Master in Economic Intelligence and Knowledge Management was created in 1995 within the CERAM Business School, now SKEMA Business School, in Paris, with the objective of delivering a full and professional training in Economic Intelligence. A Centre for Global Intelligence and Influence was created in September 2011 in the same School.
On the other hand, practitioners and companies regard professional accreditation as especially important in this field.[20] In 2011, SCIP recognized the Fuld-Gilad-Herring Academy of Competitive Intelligence's CIP certification process as its global, dual-level (CIP-I and CIP-II) certification program.
Global developments have also been uneven in competitive intelligence.[21] Several academic journals, particularly the Journal of Competitive Intelligence and Management in its third volume, provided coverage of the field's global development.[22] For example, in 1997 the École de guerre économique [fr] (School of economic warfare) was founded in Paris, France. It is the first European institution which teaches the tactics of economic warfare within a globalizing world. In Germany, competitive intelligence was unattended until the early 1990s. The term ""competitive intelligence"" first appeared in German literature in 1997. In 1995 a German SCIP chapter was founded, which is now second in terms of membership in Europe. In summer 2004 the Institute for Competitive Intelligence was founded, which provides a postgraduate certification program for Competitive Intelligence Professionals. Japan is currently the only country that officially maintains an economic intelligence agency (JETRO). It was founded by the Ministry of International Trade and Industry in 1958.
Accepting the importance of competitive intelligence, major multinational corporations, such as ExxonMobil, Procter & Gamble, and Johnson and Johnson, have created formal CI units.[citation needed] Importantly, organizations execute competitive intelligence activities not only as a safeguard to protect against market threats and changes, but also as a method for finding new opportunities and trends.[23]
Organizations use competitive intelligence to compare themselves to other organizations (""competitive benchmarking""), to identify risks and opportunities in their markets, and to pressure-test their plans against market response (business wargaming),[24] which enable them to make informed decisions. Most firms today realize the importance of knowing what their competitors are doing and how the industry is changing, and the information gathered allows organizations to understand their strengths and weaknesses.
One of the major activities involved in corporate competitive intelligence is use of ratio analysis, using key performance indicators (KPI). Organizations compare annual reports of their competitors on certain KPI and ratios, which are intrinsic to their industry. This helps them track their performance, vis-a-vis their competitors.
The actual importance of these categories of information to an organization depends on the contestability of its markets, the organizational culture, the personality and biases of its top decision makers, and the reporting structure of competitive intelligence within the company.
Strategic Intelligence (SI) focuses on the longer term, looking at issues affecting a company's competitiveness over the course of a couple of years. The actual time horizon for SI ultimately depends on the industry and how quickly it's changing. The general questions that SI answers are, ‘Where should we as a company be in X years?' and 'What are the strategic risks and opportunities facing us?' This type of intelligence work involves among others the identification of weak signals and application of methodology and process called Strategic Early Warning (SEW), first introduced by Gilad,[25][26][27] followed by Steven Shaker and Victor Richardson,[28] Alessandro Comai and Joaquin Tena,[29][30] and others. According to Gilad, 20% of the work of competitive intelligence practitioners should be dedicated to strategic early identification of weak signals within a SEW framework.
Tactical Intelligence: the focus is on providing information designed to improve shorter-term decisions, most often related with the intent of growing market share or revenues. Generally, it is the type of information that you would need to support the sales process in an organization. It investigates various aspects of a product/product line marketing:
With the right amount of information, organizations can avoid unpleasant surprises by anticipating competitors' moves and decreasing response time. Examples of competitive intelligence research is evident in daily newspapers, such as the Wall Street Journal, Business Week, and Fortune. Major airlines change hundreds of fares daily in response to competitors' tactics. They use information to plan their own marketing, pricing, and production strategies.
Resources, such as the Internet, have made gathering information on competitors easy. With a click of a button, analysts can discover future trends and market requirements. However competitive intelligence is much more than this, as the ultimate aim is to lead to competitive advantage. As the Internet is mostly public domain material, information gathered is less likely to result in insights that will be unique to the company. In fact there is a risk that information gathered from the Internet will be misinformation and mislead users, so competitive intelligence researchers are often wary of using such information.
As a result, although the Internet is viewed as a key source, most CI professionals should spend their time and budget gathering intelligence using primary research—networking with industry experts, from trade shows and conferences, from their own customers and suppliers, and so on. Where the Internet is used, it is to gather sources for primary research as well as information on what the company says about itself and its online presence (in the form of links to other companies, its strategy regarding search engines and online advertising, mentions in discussion forums and on blogs, etc.). Also, important are online subscription databases and news aggregation sources which have simplified the secondary source collection process. Social media sources are also becoming important—providing potential interviewee names, as well as opinions and attitudes, and sometimes breaking news (e.g., via Twitter).
Organizations must be careful not to spend too much time and effort on old competitors without realizing the existence of any new competitors. Knowing more about your competitors will allow your business to grow and succeed. The practice of competitive intelligence is growing every year, and most companies and business students now realize the importance of knowing their competitors.
According to Arjan Singh and Andrew Beurschgens in their 2006 article in the Competitive Intelligence Review, there are four stages of development of a competitive intelligence capability with a firm. It starts with ""stick fetching"", where a CI department is very reactive, up to ""world class"", where it is completely integrated in the decision-making process.
The technical advances in massive parallel processing offered by the Hadoop ""big data"" architecture has allowed the creation of multiple platforms for named-entity recognition such as the Apache Projects OpenNLP and Apache Stanbol. The former includes pre-trained statistical parsers that can discern elements key to establishing trends and evaluating competitive position and responding appropriately.[31] Public information mining from SEC.gov, Federal Contract Awards, social media (Twitter, Reddit, Facebook, and others), vendors, and competitor websites now permit real-time counterintelligence as a strategy for horizontal and vertical market expansion and product positioning. This occurs in an automated fashion on massive marketplaces such as Amazon.com and their classification and prediction of product associations and purchase probability.
Competitive intelligence has been influenced by national strategic intelligence. Although national intelligence was researched 50 years ago, competitive intelligence was introduced during the 1990s. Competitive-intelligence professionals can learn from national-intelligence experts, especially in the analysis of complex situations.[32] Competitive intelligence may be confused with (or seen to overlap) environmental scanning, business intelligence and market research.[33] Craig Fleisher[33] questions the appropriateness of the term, comparing it to business intelligence, competitor intelligence, knowledge management, market intelligence, marketing research and strategic intelligence.[34]
Fleisher[34][verification needed] suggests that business intelligence has two forms. Its narrow (contemporary) form is more focused on information technology and internal focus than CI, while its broader (historical) definition is more inclusive than CI. Knowledge management (KM), when improperly achieved, is seen as an information-technology driven organizational practice relying on data mining, corporate intranets and mapping organizational assets to make it accessible to organization members for decision-making. CI shares some aspects of KM; they are human-intelligence- and experience-based for a more-sophisticated qualitative analysis. km is essential for effective change. A key effective factor is a powerful, dedicated IT system executing the full intelligence cycle.[35]
Market intelligence (MI) is industry-targeted intelligence developed in real-time aspects of competitive events taking place among the four Ps of the marketing mix (pricing, place, promotion and product) in the product (or service) marketplace to better understand the market's attractiveness.[36] A time-based competitive tactic, MI is used by marketing and sales managers to respond to consumers more quickly in the marketplace. Fleisher suggests it is not distributed as widely as some forms of CI, which are also distributed to non-marketing decision-makers.[34][verification needed] Market intelligence has a shorter time horizon than other intelligence areas, and is measured in days, weeks, or (in slower-moving industries) months.
Market research is a tactical, method-driven field consisting of neutral, primary research of customer data (beliefs and perceptions) gathered in surveys or focus groups, and is analyzed with statistical-research techniques.[37] CI draws on a wider variety (primary and secondary) of sources from a wider range of stakeholders (suppliers, competitors, distributors, substitutes and media) to answer existing questions, raise new ones and guide action.[34][verification needed]
Ben Gilad and Jan Herring lay down a set of prerequisites defining CI, distinguishing it from other information-rich disciplines such as market research or business development. They show that a common body of knowledge and a unique set of tools (key intelligence topics, business war games and blindspots analysis) distinguish CI; while other sensory activities in a commercial firm focus on one segment of the market (customers, suppliers or acquisition targets), CI synthesizes data from all high-impact players (HIP).[20]
Gilad later focused his delineation of CI on the difference between information and intelligence. According to him, the common denominator among organizational sensory functions (whether they are called market research, business intelligence or market intelligence) is that they deliver information rather than intelligence. Intelligence, says Gilad, is a perspective on facts rather than the facts themselves. Unique among corporate functions, competitive intelligence has a perspective of risks and opportunities for a firm's performance; as such, it (not information activities) is part of an organization's risk-management activity.[38]
Ethics has been a long-held issue of discussion among CI practitioners.[33] The questions revolve around what is and is not allowable in terms of CI activity. A number of scholarly treatments have been generated on this topic, most prominently addressed through Society of Competitive Intelligence Professionals publications.[39] The book Competitive Intelligence Ethics: Navigating the Gray Zone provides nearly twenty separate views about ethics in CI, as well as another 10 codes used by various individuals or organizations.[39] Combining that with the over two dozen scholarly articles or studies found within the various CI bibliographic entries,[40][verification needed][8][9][41] it is clear that no shortage of study has gone into better classifying, understanding and addressing CI ethics.
Competitive information may be obtained from public or subscription sources, from networking with competitor staff or customers, disassembly of competitor products or from field research interviews. Competitive intelligence research is distinguishable from industrial espionage, as CI practitioners generally abide by local legal guidelines and ethical business norms.[42]
Outsourcing has become a big business for competitive intelligence professionals. There are many different companies in this field, including market research and consulting firms.[43]
"
926152545,2019-11-14 15:40:00,"
Competitive intelligence (CI) is the systematic collection and analysis of information from multiple sources, and a coordinated CI program. [1] It is the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers in strategic decision making for an organization.
CI means understanding and learning what is happening in the world outside the business to increase one's competitiveness. It means learning as much as possible, as soon as possible, about one's external environment including one's industry in general and relevant competitors.[2]
Key points:
Another definition of CI regards it as the organizational function responsible for the early identification of risks and opportunities in the market before they become obvious (""early signal analysis""). This definition focuses attention on the difference between dissemination of widely available factual information (such as market statistics, financial reports, newspaper clippings) performed by functions such as libraries and information centers, and competitive intelligence which is a perspective on developments and events aimed at yielding a competitive edge.[6]
The term CI is often viewed as synonymous with competitor analysis, but competitive intelligence is more than analyzing competitors; it embraces the entire environment and stakeholders: customers, competitors, distributors, technologies, and macroeconomic data.
CI literature is best exemplified by the bibliographies that were published in the Society of Competitive Intelligence Professionals' academic journal The Journal of Competitive Intelligence and Management.[7][8][9][10] Although elements of organizational intelligence collection have been a part of business for many years, the history of competitive intelligence arguably began in the U.S. in the 1970s, although the literature on the field pre-dates this time by at least several decades.[10] In 1980, Michael Porter published the study Competitive-Strategy: Techniques for Analyzing Industries and Competitors which is widely viewed as the foundation of modern competitive intelligence. This has since been extended most notably by the pair of Craig Fleisher and Babette Bensoussan, who through several popular books on competitive analysis have added 48 commonly applied competitive intelligence analysis techniques to the practitioner's tool box.[11][12] In 1985, Leonard Fuld published his best selling book dedicated to competitor intelligence.[13] However, the institutionalization of CI as a formal activity among American corporations can be traced to 1988, when Ben and Tamar Gilad published the first organizational model of a formal corporate CI function, which was then adopted widely by US companies.[14] The first professional certification program (CIP) was created in 1996 with the establishment of The Fuld-Gilad-Herring Academy of Competitive Intelligence in Cambridge, Massachusetts.
In 1986 the Society of Competitive Intelligence Professionals (SCIP) was founded in the United States and grew in the late 1990s to around 6,000 members worldwide, mainly in the United States and Canada, but with large numbers especially in the UK and Australia. Due to financial difficulties in 2009, the organization merged with Frost & Sullivan under the Frost & Sullivan Institute. SCIP has since been renamed ""Strategic & Competitive Intelligence Professionals"" to emphasise the strategic nature of the subject, and also to refocus the organisation's general approach, while keeping the existing SCIP brandname and logo. A number of efforts have been made to discuss the field's advances in post-secondary (university) education, covered by several authors including Blenkhorn & Fleisher,[15] Fleisher,[16] Fuld,[17] Prescott,[18] and McGonagle.[19] Although the general view would be that competitive intelligence concepts can be readily found and taught in many business schools around the globe, there are still relatively few dedicated academic programs, majors, or degrees in the field, a concern to academics in the field who would like to see it further researched.[16] These issues were widely discussed by over a dozen knowledgeable individuals in a special edition of the Competitive Intelligence Magazine that was dedicated to this topic.[20] In France, a Specialized Master in Economic Intelligence and Knowledge Management was created in 1995 within the CERAM Business School, now SKEMA Business School, in Paris, with the objective of delivering a full and professional training in Economic Intelligence. A Centre for Global Intelligence and Influence was created in September 2011 in the same School.
On the other hand, practitioners and companies regard professional accreditation as especially important in this field.[21] In 2011, SCIP recognized the Fuld-Gilad-Herring Academy of Competitive Intelligence's CIP certification process as its global, dual-level (CIP-I and CIP-II) certification program.
Global developments have also been uneven in competitive intelligence.[22] Several academic journals, particularly the Journal of Competitive Intelligence and Management in its third volume, provided coverage of the field's global development.[23] For example, in 1997 the École de guerre économique [fr] (School of economic warfare) was founded in Paris, France. It is the first European institution which teaches the tactics of economic warfare within a globalizing world. In Germany, competitive intelligence was unattended until the early 1990s. The term ""competitive intelligence"" first appeared in German literature in 1997. In 1995 a German SCIP chapter was founded, which is now second in terms of membership in Europe. In summer 2004 the Institute for Competitive Intelligence was founded, which provides a postgraduate certification program for Competitive Intelligence Professionals. Japan is currently the only country that officially maintains an economic intelligence agency (JETRO). It was founded by the Ministry of International Trade and Industry in 1958.
Accepting the importance of competitive intelligence, major multinational corporations, such as ExxonMobil, Procter & Gamble, and Johnson and Johnson, have created formal CI units.[citation needed] Importantly, organizations execute competitive intelligence activities not only as a safeguard to protect against market threats and changes, but also as a method for finding new opportunities and trends.[24]
Organizations use competitive intelligence to compare themselves to other organizations (""competitive benchmarking""), to identify risks and opportunities in their markets, and to pressure-test their plans against market response (business wargaming),[25] which enable them to make informed decisions. Most firms today realize the importance of knowing what their competitors are doing and how the industry is changing, and the information gathered allows organizations to understand their strengths and weaknesses.
One of the major activities involved in corporate competitive intelligence is use of ratio analysis, using key performance indicators (KPI). Organizations compare annual reports of their competitors on certain KPI and ratios, which are intrinsic to their industry. This helps them track their performance, vis-a-vis their competitors.
The actual importance of these categories of information to an organization depends on the contestability of its markets, the organizational culture, the personality and biases of its top decision makers, and the reporting structure of competitive intelligence within the company.
Strategic Intelligence (SI) focuses on the longer term, looking at issues affecting a company's competitiveness over the course of a couple of years. The actual time horizon for SI ultimately depends on the industry and how quickly it's changing. The general questions that SI answers are, ‘Where should we as a company be in X years?' and 'What are the strategic risks and opportunities facing us?' This type of intelligence work involves among others the identification of weak signals and application of methodology and process called Strategic Early Warning (SEW), first introduced by Gilad,[26][27][28] followed by Steven Shaker and Victor Richardson,[29] Alessandro Comai and Joaquin Tena,[30][31] and others. According to Gilad, 20% of the work of competitive intelligence practitioners should be dedicated to strategic early identification of weak signals within a SEW framework.
Tactical Intelligence: the focus is on providing information designed to improve shorter-term decisions, most often related with the intent of growing market share or revenues. Generally, it is the type of information that you would need to support the sales process in an organization. It investigates various aspects of a product/product line marketing:
With the right amount of information, organizations can avoid unpleasant surprises by anticipating competitors' moves and decreasing response time. Examples of competitive intelligence research is evident in daily newspapers, such as the Wall Street Journal, Business Week, and Fortune. Major airlines change hundreds of fares daily in response to competitors' tactics. They use information to plan their own marketing, pricing, and production strategies.
Resources, such as the Internet, have made gathering information on competitors easy. With a click of a button, analysts can discover future trends and market requirements. However competitive intelligence is much more than this, as the ultimate aim is to lead to competitive advantage. As the Internet is mostly public domain material, information gathered is less likely to result in insights that will be unique to the company. In fact there is a risk that information gathered from the Internet will be misinformation and mislead users, so competitive intelligence researchers are often wary of using such information.
As a result, although the Internet is viewed as a key source, most CI professionals should spend their time and budget gathering intelligence using primary research—networking with industry experts, from trade shows and conferences, from their own customers and suppliers, and so on. Where the Internet is used, it is to gather sources for primary research as well as information on what the company says about itself and its online presence (in the form of links to other companies, its strategy regarding search engines and online advertising, mentions in discussion forums and on blogs, etc.). Also, important are online subscription databases and news aggregation sources which have simplified the secondary source collection process. Social media sources are also becoming important—providing potential interviewee names, as well as opinions and attitudes, and sometimes breaking news (e.g., via Twitter).
Organizations must be careful not to spend too much time and effort on old competitors without realizing the existence of any new competitors. Knowing more about your competitors will allow your business to grow and succeed. The practice of competitive intelligence is growing every year, and most companies and business students now realize the importance of knowing their competitors.
According to Arjan Singh and Andrew Beurschgens in their 2006 article in the Competitive Intelligence Review, there are four stages of development of a competitive intelligence capability with a firm. It starts with ""stick fetching"", where a CI department is very reactive, up to ""world class"", where it is completely integrated in the decision-making process.
The technical advances in massive parallel processing offered by the Hadoop ""big data"" architecture has allowed the creation of multiple platforms for named-entity recognition such as the Apache Projects OpenNLP and Apache Stanbol. The former includes pre-trained statistical parsers that can discern elements key to establishing trends and evaluating competitive position and responding appropriately.[32] Public information mining from SEC.gov, Federal Contract Awards, social media (Twitter, Reddit, Facebook, and others), vendors, and competitor websites now permit real-time counterintelligence as a strategy for horizontal and vertical market expansion and product positioning. This occurs in an automated fashion on massive marketplaces such as Amazon.com and their classification and prediction of product associations and purchase probability.
Competitive intelligence has been influenced by national strategic intelligence. Although national intelligence was researched 50 years ago, competitive intelligence was introduced during the 1990s. Competitive-intelligence professionals can learn from national-intelligence experts, especially in the analysis of complex situations.[33] Competitive intelligence may be confused with (or seen to overlap) environmental scanning, business intelligence and market research.[34] Craig Fleisher[34] questions the appropriateness of the term, comparing it to business intelligence, competitor intelligence, knowledge management, market intelligence, marketing research and strategic intelligence.[35]
Fleisher[35][verification needed] suggests that business intelligence has two forms. Its narrow (contemporary) form is more focused on information technology and internal focus than CI, while its broader (historical) definition is more inclusive than CI. Knowledge management (KM), when improperly achieved, is seen as an information-technology driven organizational practice relying on data mining, corporate intranets and mapping organizational assets to make it accessible to organization members for decision-making. CI shares some aspects of KM; they are human-intelligence- and experience-based for a more-sophisticated qualitative analysis. km is essential for effective change. A key effective factor is a powerful, dedicated IT system executing the full intelligence cycle.[36]
Market intelligence (MI) is industry-targeted intelligence developed in real-time aspects of competitive events taking place among the four Ps of the marketing mix (pricing, place, promotion and product) in the product (or service) marketplace to better understand the market's attractiveness.[37] A time-based competitive tactic, MI is used by marketing and sales managers to respond to consumers more quickly in the marketplace. Fleisher suggests it is not distributed as widely as some forms of CI, which are also distributed to non-marketing decision-makers.[35][verification needed] Market intelligence has a shorter time horizon than other intelligence areas, and is measured in days, weeks, or (in slower-moving industries) months.
Market research is a tactical, method-driven field consisting of neutral, primary research of customer data (beliefs and perceptions) gathered in surveys or focus groups, and is analyzed with statistical-research techniques.[38] CI draws on a wider variety (primary and secondary) of sources from a wider range of stakeholders (suppliers, competitors, distributors, substitutes and media) to answer existing questions, raise new ones and guide action.[35][verification needed]
Ben Gilad and Jan Herring lay down a set of prerequisites defining CI, distinguishing it from other information-rich disciplines such as market research or business development. They show that a common body of knowledge and a unique set of tools (key intelligence topics, business war games and blindspots analysis) distinguish CI; while other sensory activities in a commercial firm focus on one segment of the market (customers, suppliers or acquisition targets), CI synthesizes data from all high-impact players (HIP).[21]
Gilad later focused his delineation of CI on the difference between information and intelligence. According to him, the common denominator among organizational sensory functions (whether they are called market research, business intelligence or market intelligence) is that they deliver information rather than intelligence. Intelligence, says Gilad, is a perspective on facts rather than the facts themselves. Unique among corporate functions, competitive intelligence has a perspective of risks and opportunities for a firm's performance; as such, it (not information activities) is part of an organization's risk-management activity.[39]
Ethics has been a long-held issue of discussion among CI practitioners.[34] The questions revolve around what is and is not allowable in terms of CI activity. A number of scholarly treatments have been generated on this topic, most prominently addressed through Society of Competitive Intelligence Professionals publications.[40] The book Competitive Intelligence Ethics: Navigating the Gray Zone provides nearly twenty separate views about ethics in CI, as well as another 10 codes used by various individuals or organizations.[40] Combining that with the over two dozen scholarly articles or studies found within the various CI bibliographic entries,[41][verification needed][9][10][42] it is clear that no shortage of study has gone into better classifying, understanding and addressing CI ethics.
Competitive information may be obtained from public or subscription sources, from networking with competitor staff or customers, disassembly of competitor products or from field research interviews. Competitive intelligence research is distinguishable from industrial espionage, as CI practitioners generally abide by local legal guidelines and ethical business norms.[43]
Outsourcing has become a big business for competitive intelligence professionals. There are many different companies in this field, including market research and consulting firms.[44]
"
930572575,2019-12-13 11:40:14,"
Competitive intelligence (CI) is the systematic collection and analysis of information from multiple sources, and a coordinated CI program. [1] It is the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers in strategic decision making for an organization.
CI means understanding and learning what is happening in the world outside the business to increase one's competitiveness. It means learning as much as possible, as soon as possible, about one's external environment including one's industry in general and relevant competitors.[2]
Key points:
Another definition of CI regards it as the organizational function responsible for the early identification of risks and opportunities in the market before they become obvious (""early signal analysis""). This definition focuses attention on the difference between dissemination of widely available factual information (such as market statistics, financial reports, newspaper clippings) performed by functions such as libraries and information centers, and competitive intelligence which is a perspective on developments and events aimed at yielding a competitive edge.[6]
The term CI is often viewed as synonymous with competitor analysis, but competitive intelligence is more than analyzing competitors; it embraces the entire environment and stakeholders: customers, competitors, distributors, technologies, and macroeconomic data.
CI literature is best exemplified by the bibliographies that were published in the Society of Competitive Intelligence Professionals' academic journal The Journal of Competitive Intelligence and Management.[7][8][9][10] Although elements of organizational intelligence collection have been a part of business for many years, the history of competitive intelligence arguably began in the U.S. in the 1970s, although the literature on the field pre-dates this time by at least several decades.[10] In 1980, Michael Porter published the study Competitive-Strategy: Techniques for Analyzing Industries and Competitors which is widely viewed as the foundation of modern competitive intelligence. This has since been extended most notably by the pair of Craig Fleisher and Babette Bensoussan, who through several popular books on competitive analysis have added 48 commonly applied competitive intelligence analysis techniques to the practitioner's tool box.[11][12] In 1985, Leonard Fuld published his best selling book dedicated to competitor intelligence.[13] However, the institutionalization of CI as a formal activity among American corporations can be traced to 1988, when Ben and Tamar Gilad published the first organizational model of a formal corporate CI function, which was then adopted widely by US companies.[14] The first professional certification program (CIP) was created in 1996 with the establishment of The Fuld-Gilad-Herring Academy of Competitive Intelligence in Cambridge, Massachusetts.
In 1986 the Society of Competitive Intelligence Professionals (SCIP) was founded in the United States and grew in the late 1990s to around 6,000 members worldwide, mainly in the United States and Canada, but with large numbers especially in the UK and Australia. Due to financial difficulties in 2009, the organization merged with Frost & Sullivan under the Frost & Sullivan Institute. SCIP has since been renamed ""Strategic & Competitive Intelligence Professionals"" to emphasise the strategic nature of the subject, and also to refocus the organisation's general approach, while keeping the existing SCIP brandname and logo. A number of efforts have been made to discuss the field's advances in post-secondary (university) education, covered by several authors including Blenkhorn & Fleisher,[15] Fleisher,[16] Fuld,[17] Prescott,[18] and McGonagle.[19] Although the general view would be that competitive intelligence concepts can be readily found and taught in many business schools around the globe, there are still relatively few dedicated academic programs, majors, or degrees in the field, a concern to academics in the field who would like to see it further researched.[16] These issues were widely discussed by over a dozen knowledgeable individuals in a special edition of the Competitive Intelligence Magazine that was dedicated to this topic.[20] In France, a Specialized Master in Economic Intelligence and Knowledge Management was created in 1995 within the CERAM Business School, now SKEMA Business School, in Paris, with the objective of delivering a full and professional training in Economic Intelligence. A Centre for Global Intelligence and Influence was created in September 2011 in the same School.
On the other hand, practitioners and companies regard professional accreditation as especially important in this field.[21] In 2011, SCIP recognized the Fuld-Gilad-Herring Academy of Competitive Intelligence's CIP certification process as its global, dual-level (CIP-I and CIP-II) certification program.
Global developments have also been uneven in competitive intelligence.[22] Several academic journals, particularly the Journal of Competitive Intelligence and Management in its third volume, provided coverage of the field's global development.[23] For example, in 1997 the École de guerre économique [fr] (School of economic warfare) was founded in Paris, France. It is the first European institution which teaches the tactics of economic warfare within a globalizing world. In Germany, competitive intelligence was unattended until the early 1990s. The term ""competitive intelligence"" first appeared in German literature in 1997. In 1995 a German SCIP chapter was founded, which is now second in terms of membership in Europe. In summer 2004 the Institute for Competitive Intelligence was founded, which provides a postgraduate certification program for Competitive Intelligence Professionals. Japan is currently the only country that officially maintains an economic intelligence agency (JETRO). It was founded by the Ministry of International Trade and Industry in 1958.
Accepting the importance of competitive intelligence, major multinational corporations, such as ExxonMobil, Procter & Gamble, and Johnson and Johnson, have created formal CI units.[citation needed] Importantly, organizations execute competitive intelligence activities not only as a safeguard to protect against market threats and changes, but also as a method for finding new opportunities and trends.[24]
Organizations use competitive intelligence to compare themselves to other organizations (""competitive benchmarking""), to identify risks and opportunities in their markets, and to pressure-test their plans against market response (business wargaming),[25] which enable them to make informed decisions. Most firms today realize the importance of knowing what their competitors are doing and how the industry is changing, and the information gathered allows organizations to understand their strengths and weaknesses.
One of the major activities involved in corporate competitive intelligence is use of ratio analysis, using key performance indicators (KPI). Organizations compare annual reports of their competitors on certain KPI and ratios, which are intrinsic to their industry. This helps them track their performance, vis-a-vis their competitors.
The actual importance of these categories of information to an organization depends on the contestability of its markets, the organizational culture, the personality and biases of its top decision makers, and the reporting structure of competitive intelligence within the company.
Strategic Intelligence (SI) focuses on the longer term, looking at issues affecting a company's competitiveness over the course of a couple of years. The actual time horizon for SI ultimately depends on the industry and how quickly it's changing. The general questions that SI answers are, ‘Where should we as a company be in X years?' and 'What are the strategic risks and opportunities facing us?' This type of intelligence work involves among others the identification of weak signals and application of methodology and process called Strategic Early Warning (SEW), first introduced by Gilad,[26][27][28] followed by Steven Shaker and Victor Richardson,[29] Alessandro Comai and Joaquin Tena,[30][31] and others. According to Gilad, 20% of the work of competitive intelligence practitioners should be dedicated to strategic early identification of weak signals within a SEW framework.
Tactical Intelligence: the focus is on providing information designed to improve shorter-term decisions, most often related with the intent of growing market share or revenues. Generally, it is the type of information that you would need to support the sales process in an organization. It investigates various aspects of a product/product line marketing:
With the right amount of information, organizations can avoid unpleasant surprises by anticipating competitors' moves and decreasing response time. Examples of competitive intelligence research is evident in daily newspapers, such as the Wall Street Journal, Business Week, and Fortune. Major airlines change hundreds of fares daily in response to competitors' tactics. They use information to plan their own marketing, pricing, and production strategies.
Resources, such as the Internet, have made gathering information on competitors easy. With a click of a button, analysts can discover future trends and market requirements. However competitive intelligence is much more than this, as the ultimate aim is to lead to competitive advantage. As the Internet is mostly public domain material, information gathered is less likely to result in insights that will be unique to the company. In fact there is a risk that information gathered from the Internet will be misinformation and mislead users, so competitive intelligence researchers are often wary of using such information.
As a result, although the Internet is viewed as a key source, most CI professionals should spend their time and budget gathering intelligence using primary research—networking with industry experts, from trade shows and conferences, from their own customers and suppliers, and so on. Where the Internet is used, it is to gather sources for primary research as well as information on what the company says about itself and its online presence (in the form of links to other companies, its strategy regarding search engines and online advertising, mentions in discussion forums and on blogs, etc.). Also, important are online subscription databases and news aggregation sources which have simplified the secondary source collection process. Social media sources are also becoming important—providing potential interviewee names, as well as opinions and attitudes, and sometimes breaking news (e.g., via Twitter).
Organizations must be careful not to spend too much time and effort on old competitors without realizing the existence of any new competitors. Knowing more about your competitors will allow your business to grow and succeed. The practice of competitive intelligence is growing every year, and most companies and business students now realize the importance of knowing their competitors.
According to Arjan Singh and Andrew Beurschgens in their 2006 article in the Competitive Intelligence Review, there are four stages of development of a competitive intelligence capability with a firm. It starts with ""stick fetching"", where a CI department is very reactive, up to ""world class"", where it is completely integrated in the decision-making process.
The technical advances in massive parallel processing offered by the Hadoop ""big data"" architecture has allowed the creation of multiple platforms for named-entity recognition such as the Apache Projects OpenNLP and Apache Stanbol. The former includes pre-trained statistical parsers that can discern elements key to establishing trends and evaluating competitive position and responding appropriately.[32] Public information mining from SEC.gov, Federal Contract Awards, social media (Twitter, Reddit, Facebook, and others), vendors, and competitor websites now permit real-time counterintelligence as a strategy for horizontal and vertical market expansion and product positioning. This occurs in an automated fashion on massive marketplaces such as Amazon.com and their classification and prediction of product associations and purchase probability.
Competitive intelligence has been influenced by national strategic intelligence. Although national intelligence was researched 50 years ago, competitive intelligence was introduced during the 1990s. Competitive-intelligence professionals can learn from national-intelligence experts, especially in the analysis of complex situations.[33] Competitive intelligence may be confused with (or seen to overlap) environmental scanning, business intelligence and market research.[34] Craig Fleisher[34] questions the appropriateness of the term, comparing it to business intelligence, competitor intelligence, knowledge management, market intelligence, marketing research and strategic intelligence.[35]
Fleisher[35][verification needed] suggests that business intelligence has two forms. Its narrow (contemporary) form is more focused on information technology and internal focus than CI, while its broader (historical) definition is more inclusive than CI. Knowledge management (KM), when improperly achieved, is seen as an information-technology driven organizational practice relying on data mining, corporate intranets and mapping organizational assets to make it accessible to organization members for decision-making. CI shares some aspects of KM; they are human-intelligence- and experience-based for a more-sophisticated qualitative analysis. km is essential for effective change. A key effective factor is a powerful, dedicated IT system executing the full intelligence cycle.[36]
Market intelligence (MI) is industry-targeted intelligence developed in real-time aspects of competitive events taking place among the four Ps of the marketing mix (pricing, place, promotion and product) in the product (or service) marketplace to better understand the market's attractiveness.[37] A time-based competitive tactic, MI is used by marketing and sales managers to respond to consumers more quickly in the marketplace. Fleisher suggests it is not distributed as widely as some forms of CI, which are also distributed to non-marketing decision-makers.[35][verification needed] Market intelligence has a shorter time horizon than other intelligence areas, and is measured in days, weeks, or (in slower-moving industries) months.
Market research is a tactical, method-driven field consisting of neutral, primary research of customer data (beliefs and perceptions) gathered in surveys or focus groups, and is analyzed with statistical-research techniques.[38] CI draws on a wider variety (primary and secondary) of sources from a wider range of stakeholders (suppliers, competitors, distributors, substitutes and media) to answer existing questions, raise new ones and guide action.[35][verification needed]
Ben Gilad and Jan Herring lay down a set of prerequisites defining CI, distinguishing it from other information-rich disciplines such as market research or business development. They show that a common body of knowledge and a unique set of tools (key intelligence topics, business war games and blindspots analysis) distinguish CI; while other sensory activities in a commercial firm focus on one segment of the market (customers, suppliers or acquisition targets), CI synthesizes data from all high-impact players (HIP).[21]
Gilad later focused his delineation of CI on the difference between information and intelligence. According to him, the common denominator among organizational sensory functions (whether they are called market research, business intelligence or market intelligence) is that they deliver information rather than intelligence. Intelligence, says Gilad, is a perspective on facts rather than the facts themselves. Unique among corporate functions, competitive intelligence has a perspective of risks and opportunities for a firm's performance; as such, it (not information activities) is part of an organization's risk-management activity.[39]
Ethics has been a long-held issue of discussion among CI practitioners.[34] The questions revolve around what is and is not allowable in terms of CI activity. A number of scholarly treatments have been generated on this topic, most prominently addressed through Society of Competitive Intelligence Professionals publications.[40] The book Competitive Intelligence Ethics: Navigating the Gray Zone provides nearly twenty separate views about ethics in CI, as well as another 10 codes used by various individuals or organizations.[40] Combining that with the over two dozen scholarly articles or studies found within the various CI bibliographic entries,[41][verification needed][9][10][42] it is clear that no shortage of study has gone into better classifying, understanding and addressing CI ethics.
Competitive information may be obtained from public or subscription sources, from networking with competitor staff or customers, disassembly of competitor products or from field research interviews. Competitive intelligence research is distinguishable from industrial espionage, as CI practitioners generally abide by local legal guidelines and ethical business norms.[43]
Outsourcing has become a big business for competitive intelligence professionals. There are many different companies in this field, including market research and consulting firms.[44]
"
932493922,2019-12-26 09:23:36,"Competitive intelligence (CI) is the systematic collection and analysis of information from multiple sources, and a coordinated CI program. [1] It is the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers in strategic decision making for an organization.
CI means understanding and learning what is happening in the world outside the business to increase one's competitiveness. It means learning as much as possible, as soon as possible, about one's external environment including one's industry in general and relevant competitors.[2]
Key points:
Another definition of CI regards it as the organizational function responsible for the early identification of risks and opportunities in the market before they become obvious (""early signal analysis""). This definition focuses attention on the difference between dissemination of widely available factual information (such as market statistics, financial reports, newspaper clippings) performed by functions such as libraries and information centers, and competitive intelligence which is a perspective on developments and events aimed at yielding a competitive edge.[6]
The term CI is often viewed as synonymous with competitor analysis, but competitive intelligence is more than analyzing competitors; it embraces the entire environment and stakeholders: customers, competitors, distributors, technologies, and macroeconomic data.
CI literature is best exemplified by the bibliographies that were published in the Society of Competitive Intelligence Professionals' academic journal The Journal of Competitive Intelligence and Management.[7][8][9][10] Although elements of organizational intelligence collection have been a part of business for many years, the history of competitive intelligence arguably began in the U.S. in the 1970s, although the literature on the field pre-dates this time by at least several decades.[10] In 1980, Michael Porter published the study Competitive-Strategy: Techniques for Analyzing Industries and Competitors which is widely viewed as the foundation of modern competitive intelligence. This has since been extended most notably by the pair of Craig Fleisher and Babette Bensoussan, who through several popular books on competitive analysis have added 48 commonly applied competitive intelligence analysis techniques to the practitioner's tool box.[11][12] In 1985, Leonard Fuld published his best selling book dedicated to competitor intelligence.[13] However, the institutionalization of CI as a formal activity among American corporations can be traced to 1988, when Ben and Tamar Gilad published the first organizational model of a formal corporate CI function, which was then adopted widely by US companies.[14] The first professional certification program (CIP) was created in 1996 with the establishment of The Fuld-Gilad-Herring Academy of Competitive Intelligence in Cambridge, Massachusetts.
In 1986 the Society of Competitive Intelligence Professionals (SCIP) was founded in the United States and grew in the late 1990s to around 6,000 members worldwide, mainly in the United States and Canada, but with large numbers especially in the UK and Australia. Due to financial difficulties in 2009, the organization merged with Frost & Sullivan under the Frost & Sullivan Institute. SCIP has since been renamed ""Strategic & Competitive Intelligence Professionals"" to emphasise the strategic nature of the subject, and also to refocus the organisation's general approach, while keeping the existing SCIP brandname and logo. A number of efforts have been made to discuss the field's advances in post-secondary (university) education, covered by several authors including Blenkhorn & Fleisher,[15] Fleisher,[16] Fuld,[17] Prescott,[18] and McGonagle.[19] Although the general view would be that competitive intelligence concepts can be readily found and taught in many business schools around the globe, there are still relatively few dedicated academic programs, majors, or degrees in the field, a concern to academics in the field who would like to see it further researched.[16] These issues were widely discussed by over a dozen knowledgeable individuals in a special edition of the Competitive Intelligence Magazine that was dedicated to this topic.[20] In France, a Specialized Master in Economic Intelligence and Knowledge Management was created in 1995 within the CERAM Business School, now SKEMA Business School, in Paris, with the objective of delivering a full and professional training in Economic Intelligence. A Centre for Global Intelligence and Influence was created in September 2011 in the same School.
On the other hand, practitioners and companies regard professional accreditation as especially important in this field.[21] In 2011, SCIP recognized the Fuld-Gilad-Herring Academy of Competitive Intelligence's CIP certification process as its global, dual-level (CIP-I and CIP-II) certification program.
Global developments have also been uneven in competitive intelligence.[22] Several academic journals, particularly the Journal of Competitive Intelligence and Management in its third volume, provided coverage of the field's global development.[23] For example, in 1997 the École de guerre économique [fr] (School of economic warfare) was founded in Paris, France. It is the first European institution which teaches the tactics of economic warfare within a globalizing world. In Germany, competitive intelligence was unattended until the early 1990s. The term ""competitive intelligence"" first appeared in German literature in 1997. In 1995 a German SCIP chapter was founded, which is now second in terms of membership in Europe. In summer 2004 the Institute for Competitive Intelligence was founded, which provides a postgraduate certification program for Competitive Intelligence Professionals. Japan is currently the only country that officially maintains an economic intelligence agency (JETRO). It was founded by the Ministry of International Trade and Industry in 1958.
Accepting the importance of competitive intelligence, major multinational corporations, such as ExxonMobil, Procter & Gamble, and Johnson and Johnson, have created formal CI units.[citation needed] Importantly, organizations execute competitive intelligence activities not only as a safeguard to protect against market threats and changes, but also as a method for finding new opportunities and trends.[24]
Organizations use competitive intelligence to compare themselves to other organizations (""competitive benchmarking""), to identify risks and opportunities in their markets, and to pressure-test their plans against market response (business wargaming),[25] which enable them to make informed decisions. Most firms today realize the importance of knowing what their competitors are doing and how the industry is changing, and the information gathered allows organizations to understand their strengths and weaknesses.
One of the major activities involved in corporate competitive intelligence is use of ratio analysis, using key performance indicators (KPI). Organizations compare annual reports of their competitors on certain KPI and ratios, which are intrinsic to their industry. This helps them track their performance, vis-a-vis their competitors.
The actual importance of these categories of information to an organization depends on the contestability of its markets, the organizational culture, the personality and biases of its top decision makers, and the reporting structure of competitive intelligence within the company.
Strategic Intelligence (SI) focuses on the longer term, looking at issues affecting a company's competitiveness over the course of a couple of years. The actual time horizon for SI ultimately depends on the industry and how quickly it's changing. The general questions that SI answers are, ‘Where should we as a company be in X years?' and 'What are the strategic risks and opportunities facing us?' This type of intelligence work involves among others the identification of weak signals and application of methodology and process called Strategic Early Warning (SEW), first introduced by Gilad,[26][27][28] followed by Steven Shaker and Victor Richardson,[29] Alessandro Comai and Joaquin Tena,[30][31] and others. According to Gilad, 20% of the work of competitive intelligence practitioners should be dedicated to strategic early identification of weak signals within a SEW framework.
Tactical Intelligence: the focus is on providing information designed to improve shorter-term decisions, most often related with the intent of growing market share or revenues. Generally, it is the type of information that you would need to support the sales process in an organization. It investigates various aspects of a product/product line marketing:
With the right amount of information, organizations can avoid unpleasant surprises by anticipating competitors' moves and decreasing response time. Examples of competitive intelligence research is evident in daily newspapers, such as the Wall Street Journal, Business Week, and Fortune. Major airlines change hundreds of fares daily in response to competitors' tactics. They use information to plan their own marketing, pricing, and production strategies.
Resources, such as the Internet, have made gathering information on competitors easy. With a click of a button, analysts can discover future trends and market requirements. However competitive intelligence is much more than this, as the ultimate aim is to lead to competitive advantage. As the Internet is mostly public domain material, information gathered is less likely to result in insights that will be unique to the company. In fact there is a risk that information gathered from the Internet will be misinformation and mislead users, so competitive intelligence researchers are often wary of using such information.
As a result, although the Internet is viewed as a key source, most CI professionals should spend their time and budget gathering intelligence using primary research—networking with industry experts, from trade shows and conferences, from their own customers and suppliers, and so on. Where the Internet is used, it is to gather sources for primary research as well as information on what the company says about itself and its online presence (in the form of links to other companies, its strategy regarding search engines and online advertising, mentions in discussion forums and on blogs, etc.). Also, important are online subscription databases and news aggregation sources which have simplified the secondary source collection process. Social media sources are also becoming important—providing potential interviewee names, as well as opinions and attitudes, and sometimes breaking news (e.g., via Twitter).
Organizations must be careful not to spend too much time and effort on old competitors without realizing the existence of any new competitors. Knowing more about your competitors will allow your business to grow and succeed. The practice of competitive intelligence is growing every year, and most companies and business students now realize the importance of knowing their competitors.
According to Arjan Singh and Andrew Beurschgens in their 2006 article in the Competitive Intelligence Review, there are four stages of development of a competitive intelligence capability with a firm. It starts with ""stick fetching"", where a CI department is very reactive, up to ""world class"", where it is completely integrated in the decision-making process.
The technical advances in massive parallel processing offered by the Hadoop ""big data"" architecture has allowed the creation of multiple platforms for named-entity recognition such as the Apache Projects OpenNLP and Apache Stanbol. The former includes pre-trained statistical parsers that can discern elements key to establishing trends and evaluating competitive position and responding appropriately.[32] Public information mining from SEC.gov, Federal Contract Awards, social media (Twitter, Reddit, Facebook, and others), vendors, and competitor websites now permit real-time counterintelligence as a strategy for horizontal and vertical market expansion and product positioning. This occurs in an automated fashion on massive marketplaces such as Amazon.com and their classification and prediction of product associations and purchase probability.
Competitive intelligence has been influenced by national strategic intelligence. Although national intelligence was researched 50 years ago, competitive intelligence was introduced during the 1990s. Competitive-intelligence professionals can learn from national-intelligence experts, especially in the analysis of complex situations.[33] Competitive intelligence may be confused with (or seen to overlap) environmental scanning, business intelligence and market research.[34] Craig Fleisher[34] questions the appropriateness of the term, comparing it to business intelligence, competitor intelligence, knowledge management, market intelligence, marketing research and strategic intelligence.[35]
Fleisher[35][verification needed] suggests that business intelligence has two forms. Its narrow (contemporary) form is more focused on information technology and internal focus than CI, while its broader (historical) definition is more inclusive than CI. Knowledge management (KM), when improperly achieved, is seen as an information-technology driven organizational practice relying on data mining, corporate intranets and mapping organizational assets to make it accessible to organization members for decision-making. CI shares some aspects of KM; they are human-intelligence- and experience-based for a more-sophisticated qualitative analysis. km is essential for effective change. A key effective factor is a powerful, dedicated IT system executing the full intelligence cycle.[36]
Market intelligence (MI) is industry-targeted intelligence developed in real-time aspects of competitive events taking place among the four Ps of the marketing mix (pricing, place, promotion and product) in the product (or service) marketplace to better understand the market's attractiveness.[37] A time-based competitive tactic, MI is used by marketing and sales managers to respond to consumers more quickly in the marketplace. Fleisher suggests it is not distributed as widely as some forms of CI, which are also distributed to non-marketing decision-makers.[35][verification needed] Market intelligence has a shorter time horizon than other intelligence areas, and is measured in days, weeks, or (in slower-moving industries) months.
Market research is a tactical, method-driven field consisting of neutral, primary research of customer data (beliefs and perceptions) gathered in surveys or focus groups, and is analyzed with statistical-research techniques.[38] CI draws on a wider variety (primary and secondary) of sources from a wider range of stakeholders (suppliers, competitors, distributors, substitutes and media) to answer existing questions, raise new ones and guide action.[35][verification needed]
Ben Gilad and Jan Herring lay down a set of prerequisites defining CI, distinguishing it from other information-rich disciplines such as market research or business development. They show that a common body of knowledge and a unique set of tools (key intelligence topics, business war games and blindspots analysis) distinguish CI; while other sensory activities in a commercial firm focus on one segment of the market (customers, suppliers or acquisition targets), CI synthesizes data from all high-impact players (HIP).[21]
Gilad later focused his delineation of CI on the difference between information and intelligence. According to him, the common denominator among organizational sensory functions (whether they are called market research, business intelligence or market intelligence) is that they deliver information rather than intelligence. Intelligence, says Gilad, is a perspective on facts rather than the facts themselves. Unique among corporate functions, competitive intelligence has a perspective of risks and opportunities for a firm's performance; as such, it (not information activities) is part of an organization's risk-management activity.[39]
Ethics has been a long-held issue of discussion among CI practitioners.[34] The questions revolve around what is and is not allowable in terms of CI activity. A number of scholarly treatments have been generated on this topic, most prominently addressed through Society of Competitive Intelligence Professionals publications.[40] The book Competitive Intelligence Ethics: Navigating the Gray Zone provides nearly twenty separate views about ethics in CI, as well as another 10 codes used by various individuals or organizations.[40] Combining that with the over two dozen scholarly articles or studies found within the various CI bibliographic entries,[41][verification needed][9][10][42] it is clear that no shortage of study has gone into better classifying, understanding and addressing CI ethics.
Competitive information may be obtained from public or subscription sources, from networking with competitor staff or customers, disassembly of competitor products or from field research interviews. Competitive intelligence research is distinguishable from industrial espionage, as CI practitioners generally abide by local legal guidelines and ethical business norms.[43]
Outsourcing has become a big business for competitive intelligence professionals. There are many different companies in this field, including market research and consulting firms.[44]
"
933659723,2020-01-02 09:25:02,"Competitive intelligence (CI) is the systematic collection and analysis of information from multiple sources, and a coordinated CI program. [1] It is the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers in strategic decision making for an organization.
CI means understanding and learning what is happening in the world outside the business to increase one's competitiveness. It means learning as much as possible, as soon as possible, about one's external environment including one's industry in general and relevant competitors.[2]
Key points:
Another definition of CI regards it as the organizational function responsible for the early identification of risks and opportunities in the market before they become obvious (""early signal analysis""). This definition focuses attention on the difference between dissemination of widely available factual information (such as market statistics, financial reports, newspaper clippings) performed by functions such as libraries and information centers, and competitive intelligence which is a perspective on developments and events aimed at yielding a competitive edge.[6]
The term CI is often viewed as synonymous with competitor analysis, but competitive intelligence is more than analyzing competitors; it embraces the entire environment and stakeholders: customers, competitors, distributors, technologies, and macroeconomic data.
CI literature is best exemplified by the bibliographies that were published in the Society of Competitive Intelligence Professionals' academic journal The Journal of Competitive Intelligence and Management.[7][8][9][10] Although elements of organizational intelligence collection have been a part of business for many years, the history of competitive intelligence arguably began in the U.S. in the 1970s, although the literature on the field pre-dates this time by at least several decades.[10] In 1980, Michael Porter published the study Competitive-Strategy: Techniques for Analyzing Industries and Competitors which is widely viewed as the foundation of modern competitive intelligence. This has since been extended most notably by the pair of Craig Fleisher and Babette Bensoussan, who through several popular books on competitive analysis have added 48 commonly applied competitive intelligence analysis techniques to the practitioner's tool box.[11][12] In 1985, Leonard Fuld published his best selling book dedicated to competitor intelligence.[13] However, the institutionalization of CI as a formal activity among American corporations can be traced to 1988, when Ben and Tamar Gilad published the first organizational model of a formal corporate CI function, which was then adopted widely by US companies.[14] The first professional certification program (CIP) was created in 1996 with the establishment of The Fuld-Gilad-Herring Academy of Competitive Intelligence in Cambridge, Massachusetts.
In 1986 the Society of Competitive Intelligence Professionals (SCIP) was founded in the United States and grew in the late 1990s to around 6,000 members worldwide, mainly in the United States and Canada, but with large numbers especially in the UK and Australia. Due to financial difficulties in 2009, the organization merged with Frost & Sullivan under the Frost & Sullivan Institute. SCIP has since been renamed ""Strategic & Competitive Intelligence Professionals"" to emphasise the strategic nature of the subject, and also to refocus the organisation's general approach, while keeping the existing SCIP brandname and logo. A number of efforts have been made to discuss the field's advances in post-secondary (university) education, covered by several authors including Blenkhorn & Fleisher,[15] Fleisher,[16] Fuld,[17] Prescott,[18] and McGonagle.[19] Although the general view would be that competitive intelligence concepts can be readily found and taught in many business schools around the globe, there are still relatively few dedicated academic programs, majors, or degrees in the field, a concern to academics in the field who would like to see it further researched.[16] These issues were widely discussed by over a dozen knowledgeable individuals in a special edition of the Competitive Intelligence Magazine that was dedicated to this topic.[20] In France, a Specialized Master in Economic Intelligence and Knowledge Management was created in 1995 within the CERAM Business School, now SKEMA Business School, in Paris, with the objective of delivering a full and professional training in Economic Intelligence. A Centre for Global Intelligence and Influence was created in September 2011 in the same School.
On the other hand, practitioners and companies regard professional accreditation as especially important in this field.[21] In 2011, SCIP recognized the Fuld-Gilad-Herring Academy of Competitive Intelligence's CIP certification process as its global, dual-level (CIP-I and CIP-II) certification program.
Global developments have also been uneven in competitive intelligence.[22] Several academic journals, particularly the Journal of Competitive Intelligence and Management in its third volume, provided coverage of the field's global development.[23] For example, in 1997 the École de guerre économique [fr] (School of economic warfare) was founded in Paris, France. It is the first European institution which teaches the tactics of economic warfare within a globalizing world. In Germany, competitive intelligence was unattended until the early 1990s. The term ""competitive intelligence"" first appeared in German literature in 1997. In 1995 a German SCIP chapter was founded, which is now second in terms of membership in Europe. In summer 2004 the Institute for Competitive Intelligence was founded, which provides a postgraduate certification program for Competitive Intelligence Professionals. Japan is currently the only country that officially maintains an economic intelligence agency (JETRO). It was founded by the Ministry of International Trade and Industry in 1958.
Accepting the importance of competitive intelligence, major multinational corporations, such as ExxonMobil, Procter & Gamble, and Johnson and Johnson, have created formal CI units.[citation needed] Importantly, organizations execute competitive intelligence activities not only as a safeguard to protect against market threats and changes, but also as a method for finding new opportunities and trends.[24]
Organizations use competitive intelligence to compare themselves to other organizations (""competitive benchmarking""), to identify risks and opportunities in their markets, and to pressure-test their plans against market response (business wargaming),[25] which enable them to make informed decisions. Most firms today realize the importance of knowing what their competitors are doing and how the industry is changing, and the information gathered allows organizations to understand their strengths and weaknesses.
One of the major activities involved in corporate competitive intelligence is use of ratio analysis, using key performance indicators (KPI). Organizations compare annual reports of their competitors on certain KPI and ratios, which are intrinsic to their industry. This helps them track their performance, vis-a-vis their competitors.
The actual importance of these categories of information to an organization depends on the contestability of its markets, the organizational culture, the personality and biases of its top decision makers, and the reporting structure of competitive intelligence within the company.
Strategic Intelligence (SI) focuses on the longer term, looking at issues affecting a company's competitiveness over the course of a couple of years. The actual time horizon for SI ultimately depends on the industry and how quickly it's changing. The general questions that SI answers are, ‘Where should we as a company be in X years?' and 'What are the strategic risks and opportunities facing us?' This type of intelligence work involves among others the identification of weak signals and application of methodology and process called Strategic Early Warning (SEW), first introduced by Gilad,[26][27][28] followed by Steven Shaker and Victor Richardson,[29] Alessandro Comai and Joaquin Tena,[30][31] and others. According to Gilad, 20% of the work of competitive intelligence practitioners should be dedicated to strategic early identification of weak signals within a SEW framework.
Tactical Intelligence: the focus is on providing information designed to improve shorter-term decisions, most often related with the intent of growing market share or revenues. Generally, it is the type of information that you would need to support the sales process in an organization. It investigates various aspects of a product/product line marketing:
With the right amount of information, organizations can avoid unpleasant surprises by anticipating competitors' moves and decreasing response time. Examples of competitive intelligence research is evident in daily newspapers, such as the Wall Street Journal, Business Week, and Fortune. Major airlines change hundreds of fares daily in response to competitors' tactics. They use information to plan their own marketing, pricing, and production strategies.
Resources, such as the Internet, have made gathering information on competitors easy. With a click of a button, analysts can discover future trends and market requirements. However competitive intelligence is much more than this, as the ultimate aim is to lead to competitive advantage. As the Internet is mostly public domain material, information gathered is less likely to result in insights that will be unique to the company. In fact there is a risk that information gathered from the Internet will be misinformation and mislead users, so competitive intelligence researchers are often wary of using such information.
As a result, although the Internet is viewed as a key source, most CI professionals should spend their time and budget gathering intelligence using primary research—networking with industry experts, from trade shows and conferences, from their own customers and suppliers, and so on. Where the Internet is used, it is to gather sources for primary research as well as information on what the company says about itself and its online presence (in the form of links to other companies, its strategy regarding search engines and online advertising, mentions in discussion forums and on blogs, etc.). Also, important are online subscription databases and news aggregation sources which have simplified the secondary source collection process. Social media sources are also becoming important—providing potential interviewee names, as well as opinions and attitudes, and sometimes breaking news (e.g., via Twitter).
Organizations must be careful not to spend too much time and effort on old competitors without realizing the existence of any new competitors. Knowing more about your competitors will allow your business to grow and succeed. The practice of competitive intelligence is growing every year, and most companies and business students now realize the importance of knowing their competitors.
According to Arjan Singh and Andrew Beurschgens in their 2006 article in the Competitive Intelligence Review, there are four stages of development of a competitive intelligence capability with a firm. It starts with ""stick fetching"", where a CI department is very reactive, up to ""world class"", where it is completely integrated in the decision-making process.
The technical advances in massive parallel processing offered by the Hadoop ""big data"" architecture has allowed the creation of multiple platforms for named-entity recognition such as the Apache Projects OpenNLP and Apache Stanbol. The former includes pre-trained statistical parsers that can discern elements key to establishing trends and evaluating competitive position and responding appropriately.[32] Public information mining from SEC.gov, Federal Contract Awards, social media (Twitter, Reddit, Facebook, and others), vendors, and competitor websites now permit real-time counterintelligence as a strategy for horizontal and vertical market expansion and product positioning. This occurs in an automated fashion on massive marketplaces such as Amazon.com and their classification and prediction of product associations and purchase probability. Recently, online data, such as online reviews from Yelp.com, have been used for competitive intelligence purposes for local businesses.[33]  In this case, Yelp data allows for analyzing changes in the competitive environment of a business in a local market nearly at real time and offers a consistent data generating mechanism across disparate markets.
Competitive intelligence has been influenced by national strategic intelligence. Although national intelligence was researched 50 years ago, competitive intelligence was introduced during the 1990s. Competitive-intelligence professionals can learn from national-intelligence experts, especially in the analysis of complex situations.[34] Competitive intelligence may be confused with (or seen to overlap) environmental scanning, business intelligence and market research.[35] Craig Fleisher[35] questions the appropriateness of the term, comparing it to business intelligence, competitor intelligence, knowledge management, market intelligence, marketing research and strategic intelligence.[36]
Fleisher[36][verification needed] suggests that business intelligence has two forms. Its narrow (contemporary) form is more focused on information technology and internal focus than CI, while its broader (historical) definition is more inclusive than CI. Knowledge management (KM), when improperly achieved, is seen as an information-technology driven organizational practice relying on data mining, corporate intranets and mapping organizational assets to make it accessible to organization members for decision-making. CI shares some aspects of KM; they are human-intelligence- and experience-based for a more-sophisticated qualitative analysis. km is essential for effective change. A key effective factor is a powerful, dedicated IT system executing the full intelligence cycle.[37]
Market intelligence (MI) is industry-targeted intelligence developed in real-time aspects of competitive events taking place among the four Ps of the marketing mix (pricing, place, promotion and product) in the product (or service) marketplace to better understand the market's attractiveness.[38] A time-based competitive tactic, MI is used by marketing and sales managers to respond to consumers more quickly in the marketplace. Fleisher suggests it is not distributed as widely as some forms of CI, which are also distributed to non-marketing decision-makers.[36][verification needed] Market intelligence has a shorter time horizon than other intelligence areas, and is measured in days, weeks, or (in slower-moving industries) months.
Market research is a tactical, method-driven field consisting of neutral, primary research of customer data (beliefs and perceptions) gathered in surveys or focus groups, and is analyzed with statistical-research techniques.[39] CI draws on a wider variety (primary and secondary) of sources from a wider range of stakeholders (suppliers, competitors, distributors, substitutes and media) to answer existing questions, raise new ones and guide action.[36][verification needed]
Ben Gilad and Jan Herring lay down a set of prerequisites defining CI, distinguishing it from other information-rich disciplines such as market research or business development. They show that a common body of knowledge and a unique set of tools (key intelligence topics, business war games and blindspots analysis) distinguish CI; while other sensory activities in a commercial firm focus on one segment of the market (customers, suppliers or acquisition targets), CI synthesizes data from all high-impact players (HIP).[21]
Gilad later focused his delineation of CI on the difference between information and intelligence. According to him, the common denominator among organizational sensory functions (whether they are called market research, business intelligence or market intelligence) is that they deliver information rather than intelligence. Intelligence, says Gilad, is a perspective on facts rather than the facts themselves. Unique among corporate functions, competitive intelligence has a perspective of risks and opportunities for a firm's performance; as such, it (not information activities) is part of an organization's risk-management activity.[40]
Ethics has been a long-held issue of discussion among CI practitioners.[35] The questions revolve around what is and is not allowable in terms of CI activity. A number of scholarly treatments have been generated on this topic, most prominently addressed through Society of Competitive Intelligence Professionals publications.[41] The book Competitive Intelligence Ethics: Navigating the Gray Zone provides nearly twenty separate views about ethics in CI, as well as another 10 codes used by various individuals or organizations.[41] Combining that with the over two dozen scholarly articles or studies found within the various CI bibliographic entries,[42][verification needed][9][10][43] it is clear that no shortage of study has gone into better classifying, understanding and addressing CI ethics.
Competitive information may be obtained from public or subscription sources, from networking with competitor staff or customers, disassembly of competitor products or from field research interviews. Competitive intelligence research is distinguishable from industrial espionage, as CI practitioners generally abide by local legal guidelines and ethical business norms.[44]
Outsourcing has become a big business for competitive intelligence professionals. There are many different companies in this field, including market research and consulting firms.[45]
"
933697724,2020-01-02 15:06:06,"Competitive intelligence (CI) is the systematic collection and analysis of information from multiple sources, and a coordinated CI program. [1] It is the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers in strategic decision making for an organization.
CI means understanding and learning what is happening in the world outside the business to increase one's competitiveness. It means learning as much as possible, as soon as possible, about one's external environment including one's industry in general and relevant competitors.[2]
Key points:
Another definition of CI regards it as the organizational function responsible for the early identification of risks and opportunities in the market before they become obvious (""early signal analysis""). This definition focuses attention on the difference between dissemination of widely available factual information (such as market statistics, financial reports, newspaper clippings) performed by functions such as libraries and information centers, and competitive intelligence which is a perspective on developments and events aimed at yielding a competitive edge.[6]
The term CI is often viewed as synonymous with competitor analysis, but competitive intelligence is more than analyzing competitors; it embraces the entire environment and stakeholders: customers, competitors, distributors, technologies, and macroeconomic data.
CI literature is best exemplified by the bibliographies that were published in the Society of Competitive Intelligence Professionals' academic journal The Journal of Competitive Intelligence and Management.[7][8][9][10] Although elements of organizational intelligence collection have been a part of business for many years, the history of competitive intelligence arguably began in the U.S. in the 1970s, although the literature on the field pre-dates this time by at least several decades.[10] In 1980, Michael Porter published the study Competitive-Strategy: Techniques for Analyzing Industries and Competitors which is widely viewed as the foundation of modern competitive intelligence. This has since been extended most notably by the pair of Craig Fleisher and Babette Bensoussan, who through several popular books on competitive analysis have added 48 commonly applied competitive intelligence analysis techniques to the practitioner's tool box.[11][12] In 1985, Leonard Fuld published his best selling book dedicated to competitor intelligence.[13] However, the institutionalization of CI as a formal activity among American corporations can be traced to 1988, when Ben and Tamar Gilad published the first organizational model of a formal corporate CI function, which was then adopted widely by US companies.[14] The first professional certification program (CIP) was created in 1996 with the establishment of The Fuld-Gilad-Herring Academy of Competitive Intelligence in Cambridge, Massachusetts.
In 1986 the Society of Competitive Intelligence Professionals (SCIP) was founded in the United States and grew in the late 1990s to around 6,000 members worldwide, mainly in the United States and Canada, but with large numbers especially in the UK and Australia. Due to financial difficulties in 2009, the organization merged with Frost & Sullivan under the Frost & Sullivan Institute. SCIP has since been renamed ""Strategic & Competitive Intelligence Professionals"" to emphasise the strategic nature of the subject, and also to refocus the organisation's general approach, while keeping the existing SCIP brandname and logo. A number of efforts have been made to discuss the field's advances in post-secondary (university) education, covered by several authors including Blenkhorn & Fleisher,[15] Fleisher,[16] Fuld,[17] Prescott,[18] and McGonagle.[19] Although the general view would be that competitive intelligence concepts can be readily found and taught in many business schools around the globe, there are still relatively few dedicated academic programs, majors, or degrees in the field, a concern to academics in the field who would like to see it further researched.[16] These issues were widely discussed by over a dozen knowledgeable individuals in a special edition of the Competitive Intelligence Magazine that was dedicated to this topic.[20] In France, a Specialized Master in Economic Intelligence and Knowledge Management was created in 1995 within the CERAM Business School, now SKEMA Business School, in Paris, with the objective of delivering a full and professional training in Economic Intelligence. A Centre for Global Intelligence and Influence was created in September 2011 in the same School.
On the other hand, practitioners and companies regard professional accreditation as especially important in this field.[21] In 2011, SCIP recognized the Fuld-Gilad-Herring Academy of Competitive Intelligence's CIP certification process as its global, dual-level (CIP-I and CIP-II) certification program.
Global developments have also been uneven in competitive intelligence.[22] Several academic journals, particularly the Journal of Competitive Intelligence and Management in its third volume, provided coverage of the field's global development.[23] For example, in 1997 the École de guerre économique [fr] (School of economic warfare) was founded in Paris, France. It is the first European institution which teaches the tactics of economic warfare within a globalizing world. In Germany, competitive intelligence was unattended until the early 1990s. The term ""competitive intelligence"" first appeared in German literature in 1997. In 1995 a German SCIP chapter was founded, which is now second in terms of membership in Europe. In summer 2004 the Institute for Competitive Intelligence was founded, which provides a postgraduate certification program for Competitive Intelligence Professionals. Japan is currently the only country that officially maintains an economic intelligence agency (JETRO). It was founded by the Ministry of International Trade and Industry in 1958.
Accepting the importance of competitive intelligence, major multinational corporations, such as ExxonMobil, Procter & Gamble, and Johnson and Johnson, have created formal CI units.[citation needed] Importantly, organizations execute competitive intelligence activities not only as a safeguard to protect against market threats and changes, but also as a method for finding new opportunities and trends.[24]
Organizations use competitive intelligence to compare themselves to other organizations (""competitive benchmarking""), to identify risks and opportunities in their markets, and to pressure-test their plans against market response (business wargaming),[25] which enable them to make informed decisions. Most firms today realize the importance of knowing what their competitors are doing and how the industry is changing, and the information gathered allows organizations to understand their strengths and weaknesses.
One of the major activities involved in corporate competitive intelligence is use of ratio analysis, using key performance indicators (KPI). Organizations compare annual reports of their competitors on certain KPI and ratios, which are intrinsic to their industry. This helps them track their performance, vis-a-vis their competitors.
The actual importance of these categories of information to an organization depends on the contestability of its markets, the organizational culture, the personality and biases of its top decision makers, and the reporting structure of competitive intelligence within the company.
Strategic Intelligence (SI) focuses on the longer term, looking at issues affecting a company's competitiveness over the course of a couple of years. The actual time horizon for SI ultimately depends on the industry and how quickly it's changing. The general questions that SI answers are, ‘Where should we as a company be in X years?' and 'What are the strategic risks and opportunities facing us?' This type of intelligence work involves among others the identification of weak signals and application of methodology and process called Strategic Early Warning (SEW), first introduced by Gilad,[26][27][28] followed by Steven Shaker and Victor Richardson,[29] Alessandro Comai and Joaquin Tena,[30][31] and others. According to Gilad, 20% of the work of competitive intelligence practitioners should be dedicated to strategic early identification of weak signals within a SEW framework.
Tactical Intelligence: the focus is on providing information designed to improve shorter-term decisions, most often related with the intent of growing market share or revenues. Generally, it is the type of information that you would need to support the sales process in an organization. It investigates various aspects of a product/product line marketing:
With the right amount of information, organizations can avoid unpleasant surprises by anticipating competitors' moves and decreasing response time. Examples of competitive intelligence research is evident in daily newspapers, such as the Wall Street Journal, Business Week, and Fortune. Major airlines change hundreds of fares daily in response to competitors' tactics. They use information to plan their own marketing, pricing, and production strategies.
Resources, such as the Internet, have made gathering information on competitors easy. With a click of a button, analysts can discover future trends and market requirements. However competitive intelligence is much more than this, as the ultimate aim is to lead to competitive advantage. As the Internet is mostly public domain material, information gathered is less likely to result in insights that will be unique to the company. In fact there is a risk that information gathered from the Internet will be misinformation and mislead users, so competitive intelligence researchers are often wary of using such information.
As a result, although the Internet is viewed as a key source, most CI professionals should spend their time and budget gathering intelligence using primary research—networking with industry experts, from trade shows and conferences, from their own customers and suppliers, and so on. Where the Internet is used, it is to gather sources for primary research as well as information on what the company says about itself and its online presence (in the form of links to other companies, its strategy regarding search engines and online advertising, mentions in discussion forums and on blogs, etc.). Also, important are online subscription databases and news aggregation sources which have simplified the secondary source collection process. Social media sources are also becoming important—providing potential interviewee names, as well as opinions and attitudes, and sometimes breaking news (e.g., via Twitter).
Organizations must be careful not to spend too much time and effort on old competitors without realizing the existence of any new competitors. Knowing more about your competitors will allow your business to grow and succeed. The practice of competitive intelligence is growing every year, and most companies and business students now realize the importance of knowing their competitors.
According to Arjan Singh and Andrew Beurschgens in their 2006 article in the Competitive Intelligence Review, there are four stages of development of a competitive intelligence capability with a firm. It starts with ""stick fetching"", where a CI department is very reactive, up to ""world class"", where it is completely integrated in the decision-making process.
The technical advances in massive parallel processing offered by the Hadoop ""big data"" architecture has allowed the creation of multiple platforms for named-entity recognition such as the Apache Projects OpenNLP and Apache Stanbol. The former includes pre-trained statistical parsers that can discern elements key to establishing trends and evaluating competitive position and responding appropriately.[32] Public information mining from SEC.gov, Federal Contract Awards, social media (Twitter, Reddit, Facebook, and others), vendors, and competitor websites now permit real-time counterintelligence as a strategy for horizontal and vertical market expansion and product positioning. This occurs in an automated fashion on massive marketplaces such as Amazon.com and their classification and prediction of product associations and purchase probability.
Competitive intelligence has been influenced by national strategic intelligence. Although national intelligence was researched 50 years ago, competitive intelligence was introduced during the 1990s. Competitive-intelligence professionals can learn from national-intelligence experts, especially in the analysis of complex situations.[33] Competitive intelligence may be confused with (or seen to overlap) environmental scanning, business intelligence and market research.[34] Craig Fleisher[34] questions the appropriateness of the term, comparing it to business intelligence, competitor intelligence, knowledge management, market intelligence, marketing research and strategic intelligence.[35]
Fleisher[35][verification needed] suggests that business intelligence has two forms. Its narrow (contemporary) form is more focused on information technology and internal focus than CI, while its broader (historical) definition is more inclusive than CI. Knowledge management (KM), when improperly achieved, is seen as an information-technology driven organizational practice relying on data mining, corporate intranets and mapping organizational assets to make it accessible to organization members for decision-making. CI shares some aspects of KM; they are human-intelligence- and experience-based for a more-sophisticated qualitative analysis. km is essential for effective change. A key effective factor is a powerful, dedicated IT system executing the full intelligence cycle.[36]
Market intelligence (MI) is industry-targeted intelligence developed in real-time aspects of competitive events taking place among the four Ps of the marketing mix (pricing, place, promotion and product) in the product (or service) marketplace to better understand the market's attractiveness.[37] A time-based competitive tactic, MI is used by marketing and sales managers to respond to consumers more quickly in the marketplace. Fleisher suggests it is not distributed as widely as some forms of CI, which are also distributed to non-marketing decision-makers.[35][verification needed] Market intelligence has a shorter time horizon than other intelligence areas, and is measured in days, weeks, or (in slower-moving industries) months.
Market research is a tactical, method-driven field consisting of neutral, primary research of customer data (beliefs and perceptions) gathered in surveys or focus groups, and is analyzed with statistical-research techniques.[38] CI draws on a wider variety (primary and secondary) of sources from a wider range of stakeholders (suppliers, competitors, distributors, substitutes and media) to answer existing questions, raise new ones and guide action.[35][verification needed]
Ben Gilad and Jan Herring lay down a set of prerequisites defining CI, distinguishing it from other information-rich disciplines such as market research or business development. They show that a common body of knowledge and a unique set of tools (key intelligence topics, business war games and blindspots analysis) distinguish CI; while other sensory activities in a commercial firm focus on one segment of the market (customers, suppliers or acquisition targets), CI synthesizes data from all high-impact players (HIP).[21]
Gilad later focused his delineation of CI on the difference between information and intelligence. According to him, the common denominator among organizational sensory functions (whether they are called market research, business intelligence or market intelligence) is that they deliver information rather than intelligence. Intelligence, says Gilad, is a perspective on facts rather than the facts themselves. Unique among corporate functions, competitive intelligence has a perspective of risks and opportunities for a firm's performance; as such, it (not information activities) is part of an organization's risk-management activity.[39]
Ethics has been a long-held issue of discussion among CI practitioners.[34] The questions revolve around what is and is not allowable in terms of CI activity. A number of scholarly treatments have been generated on this topic, most prominently addressed through Society of Competitive Intelligence Professionals publications.[40] The book Competitive Intelligence Ethics: Navigating the Gray Zone provides nearly twenty separate views about ethics in CI, as well as another 10 codes used by various individuals or organizations.[40] Combining that with the over two dozen scholarly articles or studies found within the various CI bibliographic entries,[41][verification needed][9][10][42] it is clear that no shortage of study has gone into better classifying, understanding and addressing CI ethics.
Competitive information may be obtained from public or subscription sources, from networking with competitor staff or customers, disassembly of competitor products or from field research interviews. Competitive intelligence research is distinguishable from industrial espionage, as CI practitioners generally abide by local legal guidelines and ethical business norms.[43]
Outsourcing has become a big business for competitive intelligence professionals. There are many different companies in this field, including market research and consulting firms.[44]
"
934729034,2020-01-08 03:30:22,"Competitive intelligence (CI) is the systematic collection and analysis of information from multiple sources, and a coordinated CI program. [1] It is the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers in strategic decision making for an organization.
CI means understanding and learning what is happening in the world outside the business to increase one's competitiveness. It means learning as much as possible, as soon as possible, about one's external environment including one's industry in general and relevant competitors.[2]
Key points:
Another definition of CI regards it as the organizational function responsible for the early identification of risks and opportunities in the market before they become obvious (""early signal analysis""). This definition focuses attention on the difference between dissemination of widely available factual information (such as market statistics, financial reports, newspaper clippings) performed by functions such as libraries and information centers, and competitive intelligence which is a perspective on developments and events aimed at yielding a competitive edge.[6]
The term CI is often viewed as synonymous with competitor analysis, but competitive intelligence is more than analyzing competitors; it embraces the entire environment and stakeholders: customers, competitors, distributors, technologies, and macroeconomic data.
CI literature is best exemplified by the bibliographies that were published in the Society of Competitive Intelligence Professionals' academic journal The Journal of Competitive Intelligence and Management.[7][8][9][10] Although elements of organizational intelligence collection have been a part of business for many years, the history of competitive intelligence arguably began in the U.S. in the 1970s, although the literature on the field pre-dates this time by at least several decades.[10] In 1980, Michael Porter published the study Competitive-Strategy: Techniques for Analyzing Industries and Competitors which is widely viewed as the foundation of modern competitive intelligence. This has since been extended most notably by the pair of Craig Fleisher and Babette Bensoussan, who through several popular books on competitive analysis have added 48 commonly applied competitive intelligence analysis techniques to the practitioner's tool box.[11][12] In 1985, Leonard Fuld published his best selling book dedicated to competitor intelligence.[13] However, the institutionalization of CI as a formal activity among American corporations can be traced to 1988, when Ben and Tamar Gilad published the first organizational model of a formal corporate CI function, which was then adopted widely by US companies.[14] The first professional certification program (CIP) was created in 1996 with the establishment of The Fuld-Gilad-Herring Academy of Competitive Intelligence in Cambridge, Massachusetts.
In 1986 the Society of Competitive Intelligence Professionals (SCIP) was founded in the United States and grew in the late 1990s to around 6,000 members worldwide, mainly in the United States and Canada, but with large numbers especially in the UK and Australia. Due to financial difficulties in 2009, the organization merged with Frost & Sullivan under the Frost & Sullivan Institute. SCIP has since been renamed ""Strategic & Competitive Intelligence Professionals"" to emphasise the strategic nature of the subject, and also to refocus the organisation's general approach, while keeping the existing SCIP brandname and logo. A number of efforts have been made to discuss the field's advances in post-secondary (university) education, covered by several authors including Blenkhorn & Fleisher,[15] Fleisher,[16] Fuld,[17] Prescott,[18] and McGonagle.[19] Although the general view would be that competitive intelligence concepts can be readily found and taught in many business schools around the globe, there are still relatively few dedicated academic programs, majors, or degrees in the field, a concern to academics in the field who would like to see it further researched.[16] These issues were widely discussed by over a dozen knowledgeable individuals in a special edition of the Competitive Intelligence Magazine that was dedicated to this topic.[20] In France, a Specialized Master in Economic Intelligence and Knowledge Management was created in 1995 within the CERAM Business School, now SKEMA Business School, in Paris, with the objective of delivering a full and professional training in Economic Intelligence. A Centre for Global Intelligence and Influence was created in September 2011 in the same School.
On the other hand, practitioners and companies regard professional accreditation as especially important in this field.[21] In 2011, SCIP recognized the Fuld-Gilad-Herring Academy of Competitive Intelligence's CIP certification process as its global, dual-level (CIP-I and CIP-II) certification program.
Global developments have also been uneven in competitive intelligence.[22] Several academic journals, particularly the Journal of Competitive Intelligence and Management in its third volume, provided coverage of the field's global development.[23] For example, in 1997 the École de guerre économique [fr] (School of economic warfare) was founded in Paris, France. It is the first European institution which teaches the tactics of economic warfare within a globalizing world. In Germany, competitive intelligence was unattended until the early 1990s. The term ""competitive intelligence"" first appeared in German literature in 1997. In 1995 a German SCIP chapter was founded, which is now second in terms of membership in Europe. In summer 2004 the Institute for Competitive Intelligence was founded, which provides a postgraduate certification program for Competitive Intelligence Professionals. Japan is currently the only country that officially maintains an economic intelligence agency (JETRO). It was founded by the Ministry of International Trade and Industry in 1958.
Accepting the importance of competitive intelligence, major multinational corporations, such as ExxonMobil, Procter & Gamble, and Johnson and Johnson, have created formal CI units.[citation needed] Importantly, organizations execute competitive intelligence activities not only as a safeguard to protect against market threats and changes, but also as a method for finding new opportunities and trends.[24]
Organizations use competitive intelligence to compare themselves to other organizations (""competitive benchmarking""), to identify risks and opportunities in their markets, and to pressure-test their plans against market response (business wargaming),[25] which enable them to make informed decisions. Most firms today realize the importance of knowing what their competitors are doing and how the industry is changing, and the information gathered allows organizations to understand their strengths and weaknesses.
One of the major activities involved in corporate competitive intelligence is use of ratio analysis, using key performance indicators (KPI). Organizations compare annual reports of their competitors on certain KPI and ratios, which are intrinsic to their industry. This helps them track their performance, vis-a-vis their competitors.
The actual importance of these categories of information to an organization depends on the contestability of its markets, the organizational culture, the personality and biases of its top decision makers, and the reporting structure of competitive intelligence within the company.
Strategic Intelligence (SI) focuses on the longer term, looking at issues affecting a company's competitiveness over the course of a couple of years. The actual time horizon for SI ultimately depends on the industry and how quickly it's changing. The general questions that SI answers are, ‘Where should we as a company be in X years?' and 'What are the strategic risks and opportunities facing us?' This type of intelligence work involves among others the identification of weak signals and application of methodology and process called Strategic Early Warning (SEW), first introduced by Gilad,[26][27][28] followed by Steven Shaker and Victor Richardson,[29] Alessandro Comai and Joaquin Tena,[30][31] and others. According to Gilad, 20% of the work of competitive intelligence practitioners should be dedicated to strategic early identification of weak signals within a SEW framework.
Tactical Intelligence: the focus is on providing information designed to improve shorter-term decisions, most often related with the intent of growing market share or revenues. Generally, it is the type of information that you would need to support the sales process in an organization. It investigates various aspects of a product/product line marketing:
With the right amount of information, organizations can avoid unpleasant surprises by anticipating competitors' moves and decreasing response time. Examples of competitive intelligence research is evident in daily newspapers, such as the Wall Street Journal, Business Week, and Fortune. Major airlines change hundreds of fares daily in response to competitors' tactics. They use information to plan their own marketing, pricing, and production strategies.
Resources, such as the Internet, have made gathering information on competitors easy. With a click of a button, analysts can discover future trends and market requirements. However competitive intelligence is much more than this, as the ultimate aim is to lead to competitive advantage. As the Internet is mostly public domain material, information gathered is less likely to result in insights that will be unique to the company. In fact there is a risk that information gathered from the Internet will be misinformation and mislead users, so competitive intelligence researchers are often wary of using such information.
As a result, although the Internet is viewed as a key source, most CI professionals should spend their time and budget gathering intelligence using primary research—networking with industry experts, from trade shows and conferences, from their own customers and suppliers, and so on. Where the Internet is used, it is to gather sources for primary research as well as information on what the company says about itself and its online presence (in the form of links to other companies, its strategy regarding search engines and online advertising, mentions in discussion forums and on blogs, etc.). Also, important are online subscription databases and news aggregation sources which have simplified the secondary source collection process. Social media sources are also becoming important—providing potential interviewee names, as well as opinions and attitudes, and sometimes breaking news (e.g., via Twitter).
Organizations must be careful not to spend too much time and effort on old competitors without realizing the existence of any new competitors. Knowing more about your competitors will allow your business to grow and succeed. The practice of competitive intelligence is growing every year, and most companies and business students now realize the importance of knowing their competitors.
According to Arjan Singh and Andrew Beurschgens in their 2006 article in the Competitive Intelligence Review, there are four stages of development of a competitive intelligence capability with a firm. It starts with ""stick fetching"", where a CI department is very reactive, up to ""world class"", where it is completely integrated in the decision-making process.
The technical advances in massive parallel processing offered by the Hadoop ""big data"" architecture has allowed the creation of multiple platforms for named-entity recognition such as the Apache Projects OpenNLP and Apache Stanbol. The former includes pre-trained statistical parsers that can discern elements key to establishing trends and evaluating competitive position and responding appropriately.[32] Public information mining from SEC.gov, Federal Contract Awards, social media (Twitter, Reddit, Facebook, and others), vendors, and competitor websites now permit real-time counterintelligence as a strategy for horizontal and vertical market expansion and product positioning. This occurs in an automated fashion on massive marketplaces such as Amazon.com and their classification and prediction of product associations and purchase probability.
Competitive intelligence has been influenced by national strategic intelligence. Although national intelligence was researched 50 years ago, competitive intelligence was introduced during the 1990s. Competitive-intelligence professionals can learn from national-intelligence experts, especially in the analysis of complex situations.[33] Competitive intelligence may be confused with (or seen to overlap) environmental scanning, business intelligence and market research.[34] Craig Fleisher[34] questions the appropriateness of the term, comparing it to business intelligence, competitor intelligence, knowledge management, market intelligence, marketing research and strategic intelligence.[35]
Fleisher[35][verification needed] suggests that business intelligence has two forms. Its narrow (contemporary) form is more focused on information technology and internal focus than CI, while its broader (historical) definition is more inclusive than CI. Knowledge management (KM), when improperly achieved, is seen as an information-technology driven organizational practice relying on data mining, corporate intranets and mapping organizational assets to make it accessible to organization members for decision-making. CI shares some aspects of KM; they are human-intelligence- and experience-based for a more-sophisticated qualitative analysis. km is essential for effective change. A key effective factor is a powerful, dedicated IT system executing the full intelligence cycle.[36]
Market intelligence (MI) is industry-targeted intelligence developed in real-time aspects of competitive events taking place among the four Ps of the marketing mix (pricing, place, promotion and product) in the product (or service) marketplace to better understand the market's attractiveness.[37] A time-based competitive tactic, MI is used by marketing and sales managers to respond to consumers more quickly in the marketplace. Fleisher suggests it is not distributed as widely as some forms of CI, which are also distributed to non-marketing decision-makers.[35][verification needed] Market intelligence has a shorter time horizon than other intelligence areas, and is measured in days, weeks, or (in slower-moving industries) months.
Market research is a tactical, method-driven field consisting of neutral, primary research of customer data (beliefs and perceptions) gathered in surveys or focus groups, and is analyzed with statistical-research techniques.[38] CI draws on a wider variety (primary and secondary) of sources from a wider range of stakeholders (suppliers, competitors, distributors, substitutes and media) to answer existing questions, raise new ones and guide action.[35][verification needed]
Ben Gilad and Jan Herring lay down a set of prerequisites defining CI, distinguishing it from other information-rich disciplines such as market research or business development. They show that a common body of knowledge and a unique set of tools (key intelligence topics, business war games and blindspots analysis) distinguish CI; while other sensory activities in a commercial firm focus on one segment of the market (customers, suppliers or acquisition targets), CI synthesizes data from all high-impact players (HIP).[21]
Gilad later focused his delineation of CI on the difference between information and intelligence. According to him, the common denominator among organizational sensory functions (whether they are called market research, business intelligence or market intelligence) is that they deliver information rather than intelligence. Intelligence, says Gilad, is a perspective on facts rather than the facts themselves. Unique among corporate functions, competitive intelligence has a perspective of risks and opportunities for a firm's performance; as such, it (not information activities) is part of an organization's risk-management activity.[39]
Ethics has been a long-held issue of discussion among CI practitioners.[34] The questions revolve around what is and is not allowable in terms of CI activity. A number of scholarly treatments have been generated on this topic, most prominently addressed through Society of Competitive Intelligence Professionals publications.[40] The book Competitive Intelligence Ethics: Navigating the Gray Zone provides nearly twenty separate views about ethics in CI, as well as another 10 codes used by various individuals or organizations.[40] Combining that with the over two dozen scholarly articles or studies found within the various CI bibliographic entries,[41][verification needed][9][10][42] it is clear that no shortage of study has gone into better classifying, understanding and addressing CI ethics.
Competitive information may be obtained from public or subscription sources, from networking with competitor staff or customers, disassembly of competitor products or from field research interviews. Competitive intelligence research is distinguishable from industrial espionage, as CI practitioners generally abide by local legal guidelines and ethical business norms.[43]
Outsourcing has become a big business for competitive intelligence professionals. There are many different companies in this field, including market research and consulting firms.[44]
"
960206221,2020-06-01 17:54:45,"Competitive intelligence (CI) is the systematic collection and analysis of information from multiple sources, and a coordinated CI program. [1] It is the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers in strategic decision making for an organization.
CI means understanding and learning what is happening in the world outside the business to increase one's competitiveness. It means learning as much as possible, as soon as possible, about one's external environment including one's industry in general and relevant competitors.[2]
Key points:
Another definition of CI regards it as the organizational function responsible for the early identification of risks and opportunities in the market before they become obvious (""early signal analysis""). This definition focuses attention on the difference between dissemination of widely available factual information (such as market statistics, financial reports, newspaper clippings) performed by functions such as libraries and information centers, and competitive intelligence which is a perspective on developments and events aimed at yielding a competitive edge.[6]
The term CI is often viewed as synonymous with competitor analysis, but competitive intelligence is more than analyzing competitors; it embraces the entire environment and stakeholders: customers, competitors, distributors, technologies, and macroeconomic data. It is also a tool for decision making.
CI literature is best exemplified by the bibliographies that were published in the Society of Competitive Intelligence Professionals' academic journal The Journal of Competitive Intelligence and Management.[7][8][9][10] Although elements of organizational intelligence collection have been a part of business for many years, the history of competitive intelligence arguably began in the U.S. in the 1970s, although the literature on the field pre-dates this time by at least several decades.[10] In 1980, Michael Porter published the study Competitive-Strategy: Techniques for Analyzing Industries and Competitors which is widely viewed as the foundation of modern competitive intelligence. This has since been extended most notably by the pair of Craig Fleisher and Babette Bensoussan, who through several popular books on competitive analysis have added 48 commonly applied competitive intelligence analysis techniques to the practitioner's tool box.[11][12] In 1985, Leonard Fuld published his best selling book dedicated to competitor intelligence.[13] However, the institutionalization of CI as a formal activity among American corporations can be traced to 1988, when Ben and Tamar Gilad published the first organizational model of a formal corporate CI function, which was then adopted widely by US companies.[14] The first professional certification program (CIP) was created in 1996 with the establishment of The Fuld-Gilad-Herring Academy of Competitive Intelligence in Cambridge, Massachusetts.
In 1986 the Society of Competitive Intelligence Professionals (SCIP) was founded in the United States and grew in the late 1990s to around 6,000 members worldwide, mainly in the United States and Canada, but with large numbers especially in the UK and Australia. Due to financial difficulties in 2009, the organization merged with Frost & Sullivan under the Frost & Sullivan Institute. SCIP has since been renamed ""Strategic & Competitive Intelligence Professionals"" to emphasise the strategic nature of the subject, and also to refocus the organisation's general approach, while keeping the existing SCIP brandname and logo. A number of efforts have been made to discuss the field's advances in post-secondary (university) education, covered by several authors including Blenkhorn & Fleisher,[15] Fleisher,[16] Fuld,[17] Prescott,[18] and McGonagle.[19] Although the general view would be that competitive intelligence concepts can be readily found and taught in many business schools around the globe, there are still relatively few dedicated academic programs, majors, or degrees in the field, a concern to academics in the field who would like to see it further researched.[16] These issues were widely discussed by over a dozen knowledgeable individuals in a special edition of the Competitive Intelligence Magazine that was dedicated to this topic.[20] In France, a Specialized Master in Economic Intelligence and Knowledge Management was created in 1995 within the CERAM Business School, now SKEMA Business School, in Paris, with the objective of delivering a full and professional training in Economic Intelligence. A Centre for Global Intelligence and Influence was created in September 2011 in the same School.
On the other hand, practitioners and companies regard professional accreditation as especially important in this field.[21] In 2011, SCIP recognized the Fuld-Gilad-Herring Academy of Competitive Intelligence's CIP certification process as its global, dual-level (CIP-I and CIP-II) certification program.
Global developments have also been uneven in competitive intelligence.[22] Several academic journals, particularly the Journal of Competitive Intelligence and Management in its third volume, provided coverage of the field's global development.[23] For example, in 1997 the École de guerre économique [fr] (School of economic warfare) was founded in Paris, France. It is the first European institution which teaches the tactics of economic warfare within a globalizing world. In Germany, competitive intelligence was unattended until the early 1990s. The term ""competitive intelligence"" first appeared in German literature in 1997. In 1995 a German SCIP chapter was founded, which is now second in terms of membership in Europe. In summer 2004 the Institute for Competitive Intelligence was founded, which provides a postgraduate certification program for Competitive Intelligence Professionals. Japan is currently the only country that officially maintains an economic intelligence agency (JETRO). It was founded by the Ministry of International Trade and Industry in 1958.
Accepting the importance of competitive intelligence, major multinational corporations, such as ExxonMobil, Procter & Gamble, and Johnson and Johnson, have created formal CI units.[citation needed] Importantly, organizations execute competitive intelligence activities not only as a safeguard to protect against market threats and changes, but also as a method for finding new opportunities and trends.[24]
Organizations use competitive intelligence to compare themselves to other organizations (""competitive benchmarking""), to identify risks and opportunities in their markets, and to pressure-test their plans against market response (business wargaming),[25] which enable them to make informed decisions. Most firms today realize the importance of knowing what their competitors are doing and how the industry is changing, and the information gathered allows organizations to understand their strengths and weaknesses.
One of the major activities involved in corporate competitive intelligence is use of ratio analysis, using key performance indicators (KPI). Organizations compare annual reports of their competitors on certain KPI and ratios, which are intrinsic to their industry. This helps them track their performance, vis-a-vis their competitors.
The actual importance of these categories of information to an organization depends on the contestability of its markets, the organizational culture, the personality and biases of its top decision makers, and the reporting structure of competitive intelligence within the company.
Strategic Intelligence (SI) focuses on the longer term, looking at issues affecting a company's competitiveness over the course of a couple of years. The actual time horizon for SI ultimately depends on the industry and how quickly it's changing. The general questions that SI answers are, ‘Where should we as a company be in X years?' and 'What are the strategic risks and opportunities facing us?' This type of intelligence work involves among others the identification of weak signals and application of methodology and process called Strategic Early Warning (SEW), first introduced by Gilad,[26][27][28] followed by Steven Shaker and Victor Richardson,[29] Alessandro Comai and Joaquin Tena,[30][31] and others. According to Gilad, 20% of the work of competitive intelligence practitioners should be dedicated to strategic early identification of weak signals within a SEW framework.
Tactical Intelligence: the focus is on providing information designed to improve shorter-term decisions, most often related with the intent of growing market share or revenues. Generally, it is the type of information that you would need to support the sales process in an organization. It investigates various aspects of a product/product line marketing:
With the right amount of information, organizations can avoid unpleasant surprises by anticipating competitors' moves and decreasing response time. Examples of competitive intelligence research is evident in daily newspapers, such as the Wall Street Journal, Business Week, and Fortune. Major airlines change hundreds of fares daily in response to competitors' tactics. They use information to plan their own marketing, pricing, and production strategies.
Resources, such as the Internet, have made gathering information on competitors easy. With a click of a button, analysts can discover future trends and market requirements. However competitive intelligence is much more than this, as the ultimate aim is to lead to competitive advantage. As the Internet is mostly public domain material, information gathered is less likely to result in insights that will be unique to the company. In fact there is a risk that information gathered from the Internet will be misinformation and mislead users, so competitive intelligence researchers are often wary of using such information.
As a result, although the Internet is viewed as a key source, most CI professionals should spend their time and budget gathering intelligence using primary research—networking with industry experts, from trade shows and conferences, from their own customers and suppliers, and so on. Where the Internet is used, it is to gather sources for primary research as well as information on what the company says about itself and its online presence (in the form of links to other companies, its strategy regarding search engines and online advertising, mentions in discussion forums and on blogs, etc.). Also, important are online subscription databases and news aggregation sources which have simplified the secondary source collection process. Social media sources are also becoming important—providing potential interviewee names, as well as opinions and attitudes, and sometimes breaking news (e.g., via Twitter).
Organizations must be careful not to spend too much time and effort on old competitors without realizing the existence of any new competitors. Knowing more about your competitors will allow your business to grow and succeed. The practice of competitive intelligence is growing every year, and most companies and business students now realize the importance of knowing their competitors.
According to Arjan Singh and Andrew Beurschgens in their 2006 article in the Competitive Intelligence Review, there are four stages of development of a competitive intelligence capability with a firm. It starts with ""stick fetching"", where a CI department is very reactive, up to ""world class"", where it is completely integrated in the decision-making process.
The technical advances in massive parallel processing offered by the Hadoop ""big data"" architecture has allowed the creation of multiple platforms for named-entity recognition such as the Apache Projects OpenNLP and Apache Stanbol. The former includes pre-trained statistical parsers that can discern elements key to establishing trends and evaluating competitive position and responding appropriately.[32] Public information mining from SEC.gov, Federal Contract Awards, social media (Twitter, Reddit, Facebook, and others), vendors, and competitor websites now permit real-time counterintelligence as a strategy for horizontal and vertical market expansion and product positioning. This occurs in an automated fashion on massive marketplaces such as Amazon.com and their classification and prediction of product associations and purchase probability.
Competitive intelligence has been influenced by national strategic intelligence. Although national intelligence was researched 50 years ago, competitive intelligence was introduced during the 1990s. Competitive-intelligence professionals can learn from national-intelligence experts, especially in the analysis of complex situations.[33] Competitive intelligence may be confused with (or seen to overlap) environmental scanning, business intelligence and market research.[34] Craig Fleisher[34] questions the appropriateness of the term, comparing it to business intelligence, competitor intelligence, knowledge management, market intelligence, marketing research and strategic intelligence.[35]
Fleisher[35][verification needed] suggests that business intelligence has two forms. Its narrow (contemporary) form is more focused on information technology and internal focus than CI, while its broader (historical) definition is more inclusive than CI. Knowledge management (KM), when improperly achieved, is seen as an information-technology driven organizational practice relying on data mining, corporate intranets and mapping organizational assets to make it accessible to organization members for decision-making. CI shares some aspects of KM; they are human-intelligence- and experience-based for a more-sophisticated qualitative analysis. km is essential for effective change. A key effective factor is a powerful, dedicated IT system executing the full intelligence cycle.[36]
Market intelligence (MI) is industry-targeted intelligence developed in real-time aspects of competitive events taking place among the four Ps of the marketing mix (pricing, place, promotion and product) in the product (or service) marketplace to better understand the market's attractiveness.[37] A time-based competitive tactic, MI is used by marketing and sales managers to respond to consumers more quickly in the marketplace. Fleisher suggests it is not distributed as widely as some forms of CI, which are also distributed to non-marketing decision-makers.[35][verification needed] Market intelligence has a shorter time horizon than other intelligence areas, and is measured in days, weeks, or (in slower-moving industries) months.
Market research is a tactical, method-driven field consisting of neutral, primary research of customer data (beliefs and perceptions) gathered in surveys or focus groups, and is analyzed with statistical-research techniques.[38] CI draws on a wider variety (primary and secondary) of sources from a wider range of stakeholders (suppliers, competitors, distributors, substitutes and media) to answer existing questions, raise new ones and guide action.[35][verification needed]
Ben Gilad and Jan Herring lay down a set of prerequisites defining CI, distinguishing it from other information-rich disciplines such as market research or business development. They show that a common body of knowledge and a unique set of tools (key intelligence topics, business war games and blindspots analysis) distinguish CI; while other sensory activities in a commercial firm focus on one segment of the market (customers, suppliers or acquisition targets), CI synthesizes data from all high-impact players (HIP).[21]
Gilad later focused his delineation of CI on the difference between information and intelligence. According to him, the common denominator among organizational sensory functions (whether they are called market research, business intelligence or market intelligence) is that they deliver information rather than intelligence. Intelligence, says Gilad, is a perspective on facts rather than the facts themselves. Unique among corporate functions, competitive intelligence has a perspective of risks and opportunities for a firm's performance; as such, it (not information activities) is part of an organization's risk-management activity.[39]
Ethics has been a long-held issue of discussion among CI practitioners.[34] The questions revolve around what is and is not allowable in terms of CI activity. A number of scholarly treatments have been generated on this topic, most prominently addressed through Society of Competitive Intelligence Professionals publications.[40] The book Competitive Intelligence Ethics: Navigating the Gray Zone provides nearly twenty separate views about ethics in CI, as well as another 10 codes used by various individuals or organizations.[40] Combining that with the over two dozen scholarly articles or studies found within the various CI bibliographic entries,[41][verification needed][9][10][42] it is clear that no shortage of study has gone into better classifying, understanding and addressing CI ethics.
Competitive information may be obtained from public or subscription sources, from networking with competitor staff or customers, disassembly of competitor products or from field research interviews. Competitive intelligence research is distinguishable from industrial espionage, as CI practitioners generally abide by local legal guidelines and ethical business norms.[43]
Outsourcing has become a big business for competitive intelligence professionals. There are many different companies in this field, including market research and consulting firms.[44]
"
961538323,2020-06-09 02:08:55,"Competitive intelligence (CI) is the systematic collection and analysis of information from multiple sources, and a coordinated CI program. [1] It is the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers in strategic decision making for an organization.
CI means understanding and learning what is happening in the world outside the business to increase one's competitiveness. It means learning as much as possible, as soon as possible, about one's external environment including one's industry in general and relevant competitors.[2]
Key points:
Another definition of CI regards it as the organizational function responsible for the early identification of risks and opportunities in the market before they become obvious (""early signal analysis""). This definition focuses attention on the difference between dissemination of widely available factual information (such as market statistics, financial reports, newspaper clippings) performed by functions such as libraries and information centers, and competitive intelligence which is a perspective on developments and events aimed at yielding a competitive edge.[6]
The term CI is often viewed as synonymous with competitor analysis, but competitive intelligence is more than analyzing competitors; it embraces the entire environment and stakeholders: customers, competitors, distributors, technologies, and macroeconomic data. It is also a tool for decision making.
CI literature is best exemplified by the bibliographies that were published in the Society of Competitive Intelligence Professionals' academic journal The Journal of Competitive Intelligence and Management.[7][8][9][10] Although elements of organizational intelligence collection have been a part of business for many years, the history of competitive intelligence arguably began in the U.S. in the 1970s, although the literature on the field pre-dates this time by at least several decades.[10] In 1980, Michael Porter published the study Competitive-Strategy: Techniques for Analyzing Industries and Competitors which is widely viewed as the foundation of modern competitive intelligence. This has since been extended most notably by the pair of Craig Fleisher and Babette Bensoussan, who through several popular books on competitive analysis have added 48 commonly applied competitive intelligence analysis techniques to the practitioner's tool box.[11][12] In 1985, Leonard Fuld published his best selling book dedicated to competitor intelligence.[13] However, the institutionalization of CI as a formal activity among American corporations can be traced to 1988, when Ben and Tamar Gilad published the first organizational model of a formal corporate CI function, which was then adopted widely by US companies.[14] The first professional certification program (CIP) was created in 1996 with the establishment of The Fuld-Gilad-Herring Academy of Competitive Intelligence in Cambridge, Massachusetts.
In 1986 the Society of Competitive Intelligence Professionals (SCIP) was founded in the United States and grew in the late 1990s to around 6,000 members worldwide, mainly in the United States and Canada, but with large numbers especially in the UK and Australia. Due to financial difficulties in 2009, the organization merged with Frost & Sullivan under the Frost & Sullivan Institute. SCIP has since been renamed ""Strategic & Competitive Intelligence Professionals"" to emphasise the strategic nature of the subject, and also to refocus the organisation's general approach, while keeping the existing SCIP brandname and logo. A number of efforts have been made to discuss the field's advances in post-secondary (university) education, covered by several authors including Blenkhorn & Fleisher,[15] Fleisher,[16] Fuld,[17] Prescott,[18] and McGonagle.[19] Although the general view would be that competitive intelligence concepts can be readily found and taught in many business schools around the globe, there are still relatively few dedicated academic programs, majors, or degrees in the field, a concern to academics in the field who would like to see it further researched.[16] These issues were widely discussed by over a dozen knowledgeable individuals in a special edition of the Competitive Intelligence Magazine that was dedicated to this topic.[20] In France, a Specialized Master in Economic Intelligence and Knowledge Management was created in 1995 within the CERAM Business School, now SKEMA Business School, in Paris, with the objective of delivering a full and professional training in Economic Intelligence. A Centre for Global Intelligence and Influence was created in September 2011 in the same School.
On the other hand, practitioners and companies regard professional accreditation as especially important in this field.[21] In 2011, SCIP recognized the Fuld-Gilad-Herring Academy of Competitive Intelligence's CIP certification process as its global, dual-level (CIP-I and CIP-II) certification program.
Global developments have also been uneven in competitive intelligence.[22] Several academic journals, particularly the Journal of Competitive Intelligence and Management in its third volume, provided coverage of the field's global development.[23] For example, in 1997 the École de guerre économique [fr] (School of economic warfare) was founded in Paris, France. It is the first European institution which teaches the tactics of economic warfare within a globalizing world. In Germany, competitive intelligence was unattended until the early 1990s. The term ""competitive intelligence"" first appeared in German literature in 1997. In 1995 a German SCIP chapter was founded, which is now second in terms of membership in Europe. In summer 2004 the Institute for Competitive Intelligence was founded, which provides a postgraduate certification program for Competitive Intelligence Professionals. Japan is currently the only country that officially maintains an economic intelligence agency (JETRO). It was founded by the Ministry of International Trade and Industry in 1958.
Accepting the importance of competitive intelligence, major multinational corporations, such as ExxonMobil, Procter & Gamble, and Johnson and Johnson, have created formal CI units.[citation needed] Importantly, organizations execute competitive intelligence activities not only as a safeguard to protect against market threats and changes, but also as a method for finding new opportunities and trends.[24]
Organizations use competitive intelligence to compare themselves to other organizations (""competitive benchmarking""), to identify risks and opportunities in their markets, and to pressure-test their plans against market response (business wargaming),[25] which enable them to make informed decisions. Most firms today realize the importance of knowing what their competitors are doing and how the industry is changing, and the information gathered allows organizations to understand their strengths and weaknesses.
One of the major activities involved in corporate competitive intelligence is use of ratio analysis, using key performance indicators (KPI). Organizations compare annual reports of their competitors on certain KPI and ratios, which are intrinsic to their industry. This helps them track their performance, vis-a-vis their competitors.
The actual importance of these categories of information to an organization depends on the contestability of its markets, the organizational culture, the personality and biases of its top decision makers, and the reporting structure of competitive intelligence within the company.
Strategic Intelligence (SI) focuses on the longer term, looking at issues affecting a company's competitiveness over the course of a couple of years. The actual time horizon for SI ultimately depends on the industry and how quickly it's changing. The general questions that SI answers are, ‘Where should we as a company be in X years?' and 'What are the strategic risks and opportunities facing us?' This type of intelligence work involves among others the identification of weak signals and application of methodology and process called Strategic Early Warning (SEW), first introduced by Gilad,[26][27][28] followed by Steven Shaker and Victor Richardson,[29] Alessandro Comai and Joaquin Tena,[30][31] and others. According to Gilad, 20% of the work of competitive intelligence practitioners should be dedicated to strategic early identification of weak signals within a SEW framework.
Tactical Intelligence: the focus is on providing information designed to improve shorter-term decisions, most often related with the intent of growing market share or revenues. Generally, it is the type of information that you would need to support the sales process in an organization. It investigates various aspects of a product/product line marketing:
With the right amount of information, organizations can avoid unpleasant surprises by anticipating competitors' moves and decreasing response time. Examples of competitive intelligence research is evident in daily newspapers, such as the Wall Street Journal, Business Week, and Fortune. Major airlines change hundreds of fares daily in response to competitors' tactics. They use information to plan their own marketing, pricing, and production strategies.
Resources, such as the Internet, have made gathering information on competitors easy. With a click of a button, analysts can discover future trends and market requirements. However competitive intelligence is much more than this, as the ultimate aim is to lead to competitive advantage. As the Internet is mostly public domain material, information gathered is less likely to result in insights that will be unique to the company. In fact there is a risk that information gathered from the Internet will be misinformation and mislead users, so competitive intelligence researchers are often wary of using such information.
As a result, although the Internet is viewed as a key source, most CI professionals should spend their time and budget gathering intelligence using primary research—networking with industry experts, from trade shows and conferences, from their own customers and suppliers, and so on. Where the Internet is used, it is to gather sources for primary research as well as information on what the company says about itself and its online presence (in the form of links to other companies, its strategy regarding search engines and online advertising, mentions in discussion forums and on blogs, etc.). Also, important are online subscription databases and news aggregation sources which have simplified the secondary source collection process. Social media sources are also becoming important—providing potential interviewee names, as well as opinions and attitudes, and sometimes breaking news (e.g., via Twitter).
Organizations must be careful not to spend too much time and effort on old competitors without realizing the existence of any new competitors. Knowing more about your competitors will allow your business to grow and succeed. The practice of competitive intelligence is growing every year, and most companies and business students now realize the importance of knowing their competitors.
According to Arjan Singh and Andrew Beurschgens in their 2006 article in the Competitive Intelligence Review, there are four stages of development of a competitive intelligence capability with a firm. It starts with ""stick fetching"", where a CI department is very reactive, up to ""world class"", where it is completely integrated in the decision-making process.
The technical advances in massive parallel processing offered by the Hadoop ""big data"" architecture has allowed the creation of multiple platforms for named-entity recognition such as the Apache Projects OpenNLP and Apache Stanbol. The former includes pre-trained statistical parsers that can discern elements key to establishing trends and evaluating competitive position and responding appropriately.[32] Public information mining from SEC.gov, Federal Contract Awards, social media (Twitter, Reddit, Facebook, and others), vendors, and competitor websites now permit real-time counterintelligence as a strategy for horizontal and vertical market expansion and product positioning. This occurs in an automated fashion on massive marketplaces such as Amazon.com and their classification and prediction of product associations and purchase probability.
Most recently, an entirely new industry has emerged — a number of tech companies created products that simplify or automate the way companies conduct competitive intelligence. Most of these products specialize on a single dimension of competitive intelligence in order to provide better service — for instance, they might focus on financial metrics, advertising strategies, customer reviews, employee sentiment, or social media mentions [33].
Competitive intelligence has been influenced by national strategic intelligence. Although national intelligence was researched 50 years ago, competitive intelligence was introduced during the 1990s. Competitive-intelligence professionals can learn from national-intelligence experts, especially in the analysis of complex situations.[34] Competitive intelligence may be confused with (or seen to overlap) environmental scanning, business intelligence and market research.[35] Craig Fleisher[35] questions the appropriateness of the term, comparing it to business intelligence, competitor intelligence, knowledge management, market intelligence, marketing research and strategic intelligence.[36]
Fleisher[36][verification needed] suggests that business intelligence has two forms. Its narrow (contemporary) form is more focused on information technology and internal focus than CI, while its broader (historical) definition is more inclusive than CI. Knowledge management (KM), when improperly achieved, is seen as an information-technology driven organizational practice relying on data mining, corporate intranets and mapping organizational assets to make it accessible to organization members for decision-making. CI shares some aspects of KM; they are human-intelligence- and experience-based for a more-sophisticated qualitative analysis. km is essential for effective change. A key effective factor is a powerful, dedicated IT system executing the full intelligence cycle.[37]
Market intelligence (MI) is industry-targeted intelligence developed in real-time aspects of competitive events taking place among the four Ps of the marketing mix (pricing, place, promotion and product) in the product (or service) marketplace to better understand the market's attractiveness.[38] A time-based competitive tactic, MI is used by marketing and sales managers to respond to consumers more quickly in the marketplace. Fleisher suggests it is not distributed as widely as some forms of CI, which are also distributed to non-marketing decision-makers.[36][verification needed] Market intelligence has a shorter time horizon than other intelligence areas, and is measured in days, weeks, or (in slower-moving industries) months.
Market research is a tactical, method-driven field consisting of neutral, primary research of customer data (beliefs and perceptions) gathered in surveys or focus groups, and is analyzed with statistical-research techniques.[39] CI draws on a wider variety (primary and secondary) of sources from a wider range of stakeholders (suppliers, competitors, distributors, substitutes and media) to answer existing questions, raise new ones and guide action.[36][verification needed]
Ben Gilad and Jan Herring lay down a set of prerequisites defining CI, distinguishing it from other information-rich disciplines such as market research or business development. They show that a common body of knowledge and a unique set of tools (key intelligence topics, business war games and blindspots analysis) distinguish CI; while other sensory activities in a commercial firm focus on one segment of the market (customers, suppliers or acquisition targets), CI synthesizes data from all high-impact players (HIP).[21]
Gilad later focused his delineation of CI on the difference between information and intelligence. According to him, the common denominator among organizational sensory functions (whether they are called market research, business intelligence or market intelligence) is that they deliver information rather than intelligence. Intelligence, says Gilad, is a perspective on facts rather than the facts themselves. Unique among corporate functions, competitive intelligence has a perspective of risks and opportunities for a firm's performance; as such, it (not information activities) is part of an organization's risk-management activity.[40]
Ethics has been a long-held issue of discussion among CI practitioners.[35] The questions revolve around what is and is not allowable in terms of CI activity. A number of scholarly treatments have been generated on this topic, most prominently addressed through Society of Competitive Intelligence Professionals publications.[41] The book Competitive Intelligence Ethics: Navigating the Gray Zone provides nearly twenty separate views about ethics in CI, as well as another 10 codes used by various individuals or organizations.[41] Combining that with the over two dozen scholarly articles or studies found within the various CI bibliographic entries,[42][verification needed][9][10][43] it is clear that no shortage of study has gone into better classifying, understanding and addressing CI ethics.
Competitive information may be obtained from public or subscription sources, from networking with competitor staff or customers, disassembly of competitor products or from field research interviews. Competitive intelligence research is distinguishable from industrial espionage, as CI practitioners generally abide by local legal guidelines and ethical business norms.[44]
Outsourcing has become a big business for competitive intelligence professionals. There are many different companies in this field, including market research and consulting firms.[45]
"
961543112,2020-06-09 02:48:57,"Competitive intelligence (CI) is the systematic collection and analysis of information from multiple sources, and a coordinated CI program. [1] It is the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers in strategic decision making for an organization.
CI means understanding and learning what is happening in the world outside the business to increase one's competitiveness. It means learning as much as possible, as soon as possible, about one's external environment including one's industry in general and relevant competitors.[2]
Key points:
Another definition of CI regards it as the organizational function responsible for the early identification of risks and opportunities in the market before they become obvious (""early signal analysis""). This definition focuses attention on the difference between dissemination of widely available factual information (such as market statistics, financial reports, newspaper clippings) performed by functions such as libraries and information centers, and competitive intelligence which is a perspective on developments and events aimed at yielding a competitive edge.[6]
The term CI is often viewed as synonymous with competitor analysis, but competitive intelligence is more than analyzing competitors; it embraces the entire environment and stakeholders: customers, competitors, distributors, technologies, and macroeconomic data. It is also a tool for decision making.
CI literature is best exemplified by the bibliographies that were published in the Society of Competitive Intelligence Professionals' academic journal The Journal of Competitive Intelligence and Management.[7][8][9][10] Although elements of organizational intelligence collection have been a part of business for many years, the history of competitive intelligence arguably began in the U.S. in the 1970s, although the literature on the field pre-dates this time by at least several decades.[10] In 1980, Michael Porter published the study Competitive-Strategy: Techniques for Analyzing Industries and Competitors which is widely viewed as the foundation of modern competitive intelligence. This has since been extended most notably by the pair of Craig Fleisher and Babette Bensoussan, who through several popular books on competitive analysis have added 48 commonly applied competitive intelligence analysis techniques to the practitioner's tool box.[11][12] In 1985, Leonard Fuld published his best selling book dedicated to competitor intelligence.[13] However, the institutionalization of CI as a formal activity among American corporations can be traced to 1988, when Ben and Tamar Gilad published the first organizational model of a formal corporate CI function, which was then adopted widely by US companies.[14] The first professional certification program (CIP) was created in 1996 with the establishment of The Fuld-Gilad-Herring Academy of Competitive Intelligence in Cambridge, Massachusetts.
In 1986 the Society of Competitive Intelligence Professionals (SCIP) was founded in the United States and grew in the late 1990s to around 6,000 members worldwide, mainly in the United States and Canada, but with large numbers especially in the UK and Australia. Due to financial difficulties in 2009, the organization merged with Frost & Sullivan under the Frost & Sullivan Institute. SCIP has since been renamed ""Strategic & Competitive Intelligence Professionals"" to emphasise the strategic nature of the subject, and also to refocus the organisation's general approach, while keeping the existing SCIP brandname and logo. A number of efforts have been made to discuss the field's advances in post-secondary (university) education, covered by several authors including Blenkhorn & Fleisher,[15] Fleisher,[16] Fuld,[17] Prescott,[18] and McGonagle.[19] Although the general view would be that competitive intelligence concepts can be readily found and taught in many business schools around the globe, there are still relatively few dedicated academic programs, majors, or degrees in the field, a concern to academics in the field who would like to see it further researched.[16] These issues were widely discussed by over a dozen knowledgeable individuals in a special edition of the Competitive Intelligence Magazine that was dedicated to this topic.[20] In France, a Specialized Master in Economic Intelligence and Knowledge Management was created in 1995 within the CERAM Business School, now SKEMA Business School, in Paris, with the objective of delivering a full and professional training in Economic Intelligence. A Centre for Global Intelligence and Influence was created in September 2011 in the same School.
On the other hand, practitioners and companies regard professional accreditation as especially important in this field.[21] In 2011, SCIP recognized the Fuld-Gilad-Herring Academy of Competitive Intelligence's CIP certification process as its global, dual-level (CIP-I and CIP-II) certification program.
Global developments have also been uneven in competitive intelligence.[22] Several academic journals, particularly the Journal of Competitive Intelligence and Management in its third volume, provided coverage of the field's global development.[23] For example, in 1997 the École de guerre économique [fr] (School of economic warfare) was founded in Paris, France. It is the first European institution which teaches the tactics of economic warfare within a globalizing world. In Germany, competitive intelligence was unattended until the early 1990s. The term ""competitive intelligence"" first appeared in German literature in 1997. In 1995 a German SCIP chapter was founded, which is now second in terms of membership in Europe. In summer 2004 the Institute for Competitive Intelligence was founded, which provides a postgraduate certification program for Competitive Intelligence Professionals. Japan is currently the only country that officially maintains an economic intelligence agency (JETRO). It was founded by the Ministry of International Trade and Industry in 1958.
Accepting the importance of competitive intelligence, major multinational corporations, such as ExxonMobil, Procter & Gamble, and Johnson and Johnson, have created formal CI units.[citation needed] Importantly, organizations execute competitive intelligence activities not only as a safeguard to protect against market threats and changes, but also as a method for finding new opportunities and trends.[24]
Organizations use competitive intelligence to compare themselves to other organizations (""competitive benchmarking""), to identify risks and opportunities in their markets, and to pressure-test their plans against market response (business wargaming),[25] which enable them to make informed decisions. Most firms today realize the importance of knowing what their competitors are doing and how the industry is changing, and the information gathered allows organizations to understand their strengths and weaknesses.
One of the major activities involved in corporate competitive intelligence is use of ratio analysis, using key performance indicators (KPI). Organizations compare annual reports of their competitors on certain KPI and ratios, which are intrinsic to their industry. This helps them track their performance, vis-a-vis their competitors.
The actual importance of these categories of information to an organization depends on the contestability of its markets, the organizational culture, the personality and biases of its top decision makers, and the reporting structure of competitive intelligence within the company.
Strategic Intelligence (SI) focuses on the longer term, looking at issues affecting a company's competitiveness over the course of a couple of years. The actual time horizon for SI ultimately depends on the industry and how quickly it's changing. The general questions that SI answers are, ‘Where should we as a company be in X years?' and 'What are the strategic risks and opportunities facing us?' This type of intelligence work involves among others the identification of weak signals and application of methodology and process called Strategic Early Warning (SEW), first introduced by Gilad,[26][27][28] followed by Steven Shaker and Victor Richardson,[29] Alessandro Comai and Joaquin Tena,[30][31] and others. According to Gilad, 20% of the work of competitive intelligence practitioners should be dedicated to strategic early identification of weak signals within a SEW framework.
Tactical Intelligence: the focus is on providing information designed to improve shorter-term decisions, most often related with the intent of growing market share or revenues. Generally, it is the type of information that you would need to support the sales process in an organization. It investigates various aspects of a product/product line marketing:
With the right amount of information, organizations can avoid unpleasant surprises by anticipating competitors' moves and decreasing response time. Examples of competitive intelligence research is evident in daily newspapers, such as the Wall Street Journal, Business Week, and Fortune. Major airlines change hundreds of fares daily in response to competitors' tactics. They use information to plan their own marketing, pricing, and production strategies.
Resources, such as the Internet, have made gathering information on competitors easy. With a click of a button, analysts can discover future trends and market requirements. However competitive intelligence is much more than this, as the ultimate aim is to lead to competitive advantage. As the Internet is mostly public domain material, information gathered is less likely to result in insights that will be unique to the company. In fact there is a risk that information gathered from the Internet will be misinformation and mislead users, so competitive intelligence researchers are often wary of using such information.
As a result, although the Internet is viewed as a key source, most CI professionals should spend their time and budget gathering intelligence using primary research—networking with industry experts, from trade shows and conferences, from their own customers and suppliers, and so on. Where the Internet is used, it is to gather sources for primary research as well as information on what the company says about itself and its online presence (in the form of links to other companies, its strategy regarding search engines and online advertising, mentions in discussion forums and on blogs, etc.). Also, important are online subscription databases and news aggregation sources which have simplified the secondary source collection process. Social media sources are also becoming important—providing potential interviewee names, as well as opinions and attitudes, and sometimes breaking news (e.g., via Twitter).
Organizations must be careful not to spend too much time and effort on old competitors without realizing the existence of any new competitors. Knowing more about your competitors will allow your business to grow and succeed. The practice of competitive intelligence is growing every year, and most companies and business students now realize the importance of knowing their competitors.
According to Arjan Singh and Andrew Beurschgens in their 2006 article in the Competitive Intelligence Review, there are four stages of development of a competitive intelligence capability with a firm. It starts with ""stick fetching"", where a CI department is very reactive, up to ""world class"", where it is completely integrated in the decision-making process.
The technical advances in massive parallel processing offered by the Hadoop ""big data"" architecture has allowed the creation of multiple platforms for named-entity recognition such as the Apache Projects OpenNLP and Apache Stanbol. The former includes pre-trained statistical parsers that can discern elements key to establishing trends and evaluating competitive position and responding appropriately.[32] Public information mining from SEC.gov, Federal Contract Awards, social media (Twitter, Reddit, Facebook, and others), vendors, and competitor websites now permit real-time counterintelligence as a strategy for horizontal and vertical market expansion and product positioning. This occurs in an automated fashion on massive marketplaces such as Amazon.com and their classification and prediction of product associations and purchase probability.
Competitive intelligence has been influenced by national strategic intelligence. Although national intelligence was researched 50 years ago, competitive intelligence was introduced during the 1990s. Competitive-intelligence professionals can learn from national-intelligence experts, especially in the analysis of complex situations.[33] Competitive intelligence may be confused with (or seen to overlap) environmental scanning, business intelligence and market research.[34] Craig Fleisher[34] questions the appropriateness of the term, comparing it to business intelligence, competitor intelligence, knowledge management, market intelligence, marketing research and strategic intelligence.[35]
Fleisher[35][verification needed] suggests that business intelligence has two forms. Its narrow (contemporary) form is more focused on information technology and internal focus than CI, while its broader (historical) definition is more inclusive than CI. Knowledge management (KM), when improperly achieved, is seen as an information-technology driven organizational practice relying on data mining, corporate intranets and mapping organizational assets to make it accessible to organization members for decision-making. CI shares some aspects of KM; they are human-intelligence- and experience-based for a more-sophisticated qualitative analysis. km is essential for effective change. A key effective factor is a powerful, dedicated IT system executing the full intelligence cycle.[36]
Market intelligence (MI) is industry-targeted intelligence developed in real-time aspects of competitive events taking place among the four Ps of the marketing mix (pricing, place, promotion and product) in the product (or service) marketplace to better understand the market's attractiveness.[37] A time-based competitive tactic, MI is used by marketing and sales managers to respond to consumers more quickly in the marketplace. Fleisher suggests it is not distributed as widely as some forms of CI, which are also distributed to non-marketing decision-makers.[35][verification needed] Market intelligence has a shorter time horizon than other intelligence areas, and is measured in days, weeks, or (in slower-moving industries) months.
Market research is a tactical, method-driven field consisting of neutral, primary research of customer data (beliefs and perceptions) gathered in surveys or focus groups, and is analyzed with statistical-research techniques.[38] CI draws on a wider variety (primary and secondary) of sources from a wider range of stakeholders (suppliers, competitors, distributors, substitutes and media) to answer existing questions, raise new ones and guide action.[35][verification needed]
Ben Gilad and Jan Herring lay down a set of prerequisites defining CI, distinguishing it from other information-rich disciplines such as market research or business development. They show that a common body of knowledge and a unique set of tools (key intelligence topics, business war games and blindspots analysis) distinguish CI; while other sensory activities in a commercial firm focus on one segment of the market (customers, suppliers or acquisition targets), CI synthesizes data from all high-impact players (HIP).[21]
Gilad later focused his delineation of CI on the difference between information and intelligence. According to him, the common denominator among organizational sensory functions (whether they are called market research, business intelligence or market intelligence) is that they deliver information rather than intelligence. Intelligence, says Gilad, is a perspective on facts rather than the facts themselves. Unique among corporate functions, competitive intelligence has a perspective of risks and opportunities for a firm's performance; as such, it (not information activities) is part of an organization's risk-management activity.[39]
Ethics has been a long-held issue of discussion among CI practitioners.[34] The questions revolve around what is and is not allowable in terms of CI activity. A number of scholarly treatments have been generated on this topic, most prominently addressed through Society of Competitive Intelligence Professionals publications.[40] The book Competitive Intelligence Ethics: Navigating the Gray Zone provides nearly twenty separate views about ethics in CI, as well as another 10 codes used by various individuals or organizations.[40] Combining that with the over two dozen scholarly articles or studies found within the various CI bibliographic entries,[41][verification needed][9][10][42] it is clear that no shortage of study has gone into better classifying, understanding and addressing CI ethics.
Competitive information may be obtained from public or subscription sources, from networking with competitor staff or customers, disassembly of competitor products or from field research interviews. Competitive intelligence research is distinguishable from industrial espionage, as CI practitioners generally abide by local legal guidelines and ethical business norms.[43]
Outsourcing has become a big business for competitive intelligence professionals. There are many different companies in this field, including market research and consulting firms.[44]
"
969358269,2020-07-24 22:49:57,"Competitive intelligence (CI) is the systematic collection and analysis of information from multiple sources, and a coordinated CI program.[1] It is the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers in strategic decision making for an organization.
CI means understanding and learning what is happening in the world outside the business to increase one's competitiveness. It means learning as much as possible, as soon as possible, about one's external environment including one's industry in general and relevant competitors.[2]
Key points:
Another definition of CI regards it as the organizational function responsible for the early identification of risks and opportunities in the market before they become obvious (""early signal analysis""). This definition focuses attention on the difference between dissemination of widely available factual information (such as market statistics, financial reports, newspaper clippings) performed by functions such as libraries and information centers, and competitive intelligence which is a perspective on developments and events aimed at yielding a competitive edge.[6]
The term CI is often viewed as synonymous with competitor analysis, but competitive intelligence is more than analyzing competitors; it embraces the entire environment and stakeholders: customers, competitors, distributors, technologies, and macroeconomic data. It is also a tool for decision making.
CI literature is best exemplified by the bibliographies that were published in the Society of Competitive Intelligence Professionals' academic journal The Journal of Competitive Intelligence and Management.[7][8][9][10] Although elements of organizational intelligence collection have been a part of business for many years, the history of competitive intelligence arguably began in the U.S. in the 1970s, although the literature on the field pre-dates this time by at least several decades.[10] In 1980, Michael Porter published the study Competitive-Strategy: Techniques for Analyzing Industries and Competitors which is widely viewed as the foundation of modern competitive intelligence. This has since been extended most notably by the pair of Craig Fleisher and Babette Bensoussan, who through several popular books on competitive analysis have added 48 commonly applied competitive intelligence analysis techniques to the practitioner's tool box.[11][12] In 1985, Leonard Fuld published his best selling book dedicated to competitor intelligence.[13] However, the institutionalization of CI as a formal activity among American corporations can be traced to 1988, when Ben and Tamar Gilad published the first organizational model of a formal corporate CI function, which was then adopted widely by US companies.[14] The first professional certification program (CIP) was created in 1996 with the establishment of The Fuld-Gilad-Herring Academy of Competitive Intelligence in Cambridge, Massachusetts.
In 1986 the Society of Competitive Intelligence Professionals (SCIP) was founded in the United States and grew in the late 1990s to around 6,000 members worldwide, mainly in the United States and Canada, but with large numbers especially in the UK and Australia. Due to financial difficulties in 2009, the organization merged with Frost & Sullivan under the Frost & Sullivan Institute. SCIP has since been renamed ""Strategic & Competitive Intelligence Professionals"" to emphasise the strategic nature of the subject, and also to refocus the organisation's general approach, while keeping the existing SCIP brandname and logo. A number of efforts have been made to discuss the field's advances in post-secondary (university) education, covered by several authors including Blenkhorn & Fleisher,[15] Fleisher,[16] Fuld,[17] Prescott,[18] and McGonagle.[19] Although the general view would be that competitive intelligence concepts can be readily found and taught in many business schools around the globe, there are still relatively few dedicated academic programs, majors, or degrees in the field, a concern to academics in the field who would like to see it further researched.[16] These issues were widely discussed by over a dozen knowledgeable individuals in a special edition of the Competitive Intelligence Magazine that was dedicated to this topic.[20] In France, a Specialized Master in Economic Intelligence and Knowledge Management was created in 1995 within the CERAM Business School, now SKEMA Business School, in Paris, with the objective of delivering a full and professional training in Economic Intelligence. A Centre for Global Intelligence and Influence was created in September 2011 in the same School.
On the other hand, practitioners and companies regard professional accreditation as especially important in this field.[21] In 2011, SCIP recognized the Fuld-Gilad-Herring Academy of Competitive Intelligence's CIP certification process as its global, dual-level (CIP-I and CIP-II) certification program.
Global developments have also been uneven in competitive intelligence.[22] Several academic journals, particularly the Journal of Competitive Intelligence and Management in its third volume, provided coverage of the field's global development.[23] For example, in 1997 the École de guerre économique [fr] (School of economic warfare) was founded in Paris, France. It is the first European institution which teaches the tactics of economic warfare within a globalizing world. In Germany, competitive intelligence was unattended until the early 1990s. The term ""competitive intelligence"" first appeared in German literature in 1997. In 1995 a German SCIP chapter was founded, which is now second in terms of membership in Europe. In summer 2004 the Institute for Competitive Intelligence was founded, which provides a postgraduate certification program for Competitive Intelligence Professionals. Japan is currently the only country that officially maintains an economic intelligence agency (JETRO). It was founded by the Ministry of International Trade and Industry in 1958.
Accepting the importance of competitive intelligence, major multinational corporations, such as ExxonMobil, Procter & Gamble, and Johnson and Johnson, have created formal CI units.[citation needed] Importantly, organizations execute competitive intelligence activities not only as a safeguard to protect against market threats and changes, but also as a method for finding new opportunities and trends.[24]
Organizations use competitive intelligence to compare themselves to other organizations (""competitive benchmarking""), to identify risks and opportunities in their markets, and to pressure-test their plans against market response (business wargaming),[25] which enable them to make informed decisions. Most firms today realize the importance of knowing what their competitors are doing and how the industry is changing, and the information gathered allows organizations to understand their strengths and weaknesses.
One of the major activities involved in corporate competitive intelligence is use of ratio analysis, using key performance indicators (KPI). Organizations compare annual reports of their competitors on certain KPI and ratios, which are intrinsic to their industry. This helps them track their performance, vis-a-vis their competitors.
The actual importance of these categories of information to an organization depends on the contestability of its markets, the organizational culture, the personality and biases of its top decision makers, and the reporting structure of competitive intelligence within the company.
Strategic Intelligence (SI) focuses on the longer term, looking at issues affecting a company's competitiveness over the course of a couple of years. The actual time horizon for SI ultimately depends on the industry and how quickly it's changing. The general questions that SI answers are, ‘Where should we as a company be in X years?' and 'What are the strategic risks and opportunities facing us?' This type of intelligence work involves among others the identification of weak signals and application of methodology and process called Strategic Early Warning (SEW), first introduced by Gilad,[26][27][28] followed by Steven Shaker and Victor Richardson,[29] Alessandro Comai and Joaquin Tena,[30][31] and others. According to Gilad, 20% of the work of competitive intelligence practitioners should be dedicated to strategic early identification of weak signals within a SEW framework.
Tactical Intelligence: the focus is on providing information designed to improve shorter-term decisions, most often related with the intent of growing market share or revenues. Generally, it is the type of information that you would need to support the sales process in an organization. It investigates various aspects of a product/product line marketing:
With the right amount of information, organizations can avoid unpleasant surprises by anticipating competitors' moves and decreasing response time. Examples of competitive intelligence research is evident in daily newspapers, such as the Wall Street Journal, Business Week, and Fortune. Major airlines change hundreds of fares daily in response to competitors' tactics. They use information to plan their own marketing, pricing, and production strategies.
Resources, such as the Internet, have made gathering information on competitors easy. With a click of a button, analysts can discover future trends and market requirements. However competitive intelligence is much more than this, as the ultimate aim is to lead to competitive advantage. As the Internet is mostly public domain material, information gathered is less likely to result in insights that will be unique to the company. In fact there is a risk that information gathered from the Internet will be misinformation and mislead users, so competitive intelligence researchers are often wary of using such information.
As a result, although the Internet is viewed as a key source, most CI professionals should spend their time and budget gathering intelligence using primary research—networking with industry experts, from trade shows and conferences, from their own customers and suppliers, and so on. Where the Internet is used, it is to gather sources for primary research as well as information on what the company says about itself and its online presence (in the form of links to other companies, its strategy regarding search engines and online advertising, mentions in discussion forums and on blogs, etc.). Also, important are online subscription databases and news aggregation sources which have simplified the secondary source collection process. Social media sources are also becoming important—providing potential interviewee names, as well as opinions and attitudes, and sometimes breaking news (e.g., via Twitter).
Organizations must be careful not to spend too much time and effort on old competitors without realizing the existence of any new competitors. Knowing more about your competitors will allow your business to grow and succeed. The practice of competitive intelligence is growing every year, and most companies and business students now realize the importance of knowing their competitors.
According to Arjan Singh and Andrew Beurschgens in their 2006 article in the Competitive Intelligence Review, there are four stages of development of a competitive intelligence capability with a firm. It starts with ""stick fetching"", where a CI department is very reactive, up to ""world class"", where it is completely integrated in the decision-making process.
The technical advances in massive parallel processing offered by the Hadoop ""big data"" architecture has allowed the creation of multiple platforms for named-entity recognition such as the Apache Projects OpenNLP and Apache Stanbol. The former includes pre-trained statistical parsers that can discern elements key to establishing trends and evaluating competitive position and responding appropriately.[32] Public information mining from SEC.gov, Federal Contract Awards, social media (Twitter, Reddit, Facebook, and others), vendors, and competitor websites now permit real-time counterintelligence as a strategy for horizontal and vertical market expansion and product positioning. This occurs in an automated fashion on massive marketplaces such as Amazon.com and their classification and prediction of product associations and purchase probability.
Competitive intelligence has been influenced by national strategic intelligence. Although national intelligence was researched 50 years ago, competitive intelligence was introduced during the 1990s. Competitive-intelligence professionals can learn from national-intelligence experts, especially in the analysis of complex situations.[33] Competitive intelligence may be confused with (or seen to overlap) environmental scanning, business intelligence and market research.[34] Craig Fleisher[34] questions the appropriateness of the term, comparing it to business intelligence, competitor intelligence, knowledge management, market intelligence, marketing research and strategic intelligence.[35]
Fleisher[35][verification needed] suggests that business intelligence has two forms. Its narrow (contemporary) form is more focused on information technology and internal focus than CI, while its broader (historical) definition is more inclusive than CI. Knowledge management (KM), when improperly achieved, is seen as an information-technology driven organizational practice relying on data mining, corporate intranets and mapping organizational assets to make it accessible to organization members for decision-making. CI shares some aspects of KM; they are human-intelligence- and experience-based for a more-sophisticated qualitative analysis. km is essential for effective change. A key effective factor is a powerful, dedicated IT system executing the full intelligence cycle.[36]
Market intelligence (MI) is industry-targeted intelligence developed in real-time aspects of competitive events taking place among the four Ps of the marketing mix (pricing, place, promotion and product) in the product (or service) marketplace to better understand the market's attractiveness.[37] A time-based competitive tactic, MI is used by marketing and sales managers to respond to consumers more quickly in the marketplace. Fleisher suggests it is not distributed as widely as some forms of CI, which are also distributed to non-marketing decision-makers.[35][verification needed] Market intelligence has a shorter time horizon than other intelligence areas, and is measured in days, weeks, or (in slower-moving industries) months.
Market research is a tactical, method-driven field consisting of neutral, primary research of customer data (beliefs and perceptions) gathered in surveys or focus groups, and is analyzed with statistical-research techniques.[38] CI draws on a wider variety (primary and secondary) of sources from a wider range of stakeholders (suppliers, competitors, distributors, substitutes and media) to answer existing questions, raise new ones and guide action.[35][verification needed]
Ben Gilad and Jan Herring lay down a set of prerequisites defining CI, distinguishing it from other information-rich disciplines such as market research or business development. They show that a common body of knowledge and a unique set of tools (key intelligence topics, business war games and blindspots analysis) distinguish CI; while other sensory activities in a commercial firm focus on one segment of the market (customers, suppliers or acquisition targets), CI synthesizes data from all high-impact players (HIP).[21]
Gilad later focused his delineation of CI on the difference between information and intelligence. According to him, the common denominator among organizational sensory functions (whether they are called market research, business intelligence or market intelligence) is that they deliver information rather than intelligence. Intelligence, says Gilad, is a perspective on facts rather than the facts themselves. Unique among corporate functions, competitive intelligence has a perspective of risks and opportunities for a firm's performance; as such, it (not information activities) is part of an organization's risk-management activity.[39]
Ethics has been a long-held issue of discussion among CI practitioners.[34] The questions revolve around what is and is not allowable in terms of CI activity. A number of scholarly treatments have been generated on this topic, most prominently addressed through Society of Competitive Intelligence Professionals publications.[40] The book Competitive Intelligence Ethics: Navigating the Gray Zone provides nearly twenty separate views about ethics in CI, as well as another 10 codes used by various individuals or organizations.[40] Combining that with the over two dozen scholarly articles or studies found within the various CI bibliographic entries,[41][verification needed][9][10][42] it is clear that no shortage of study has gone into better classifying, understanding and addressing CI ethics.
Competitive information may be obtained from public or subscription sources, from networking with competitor staff or customers, disassembly of competitor products or from field research interviews. Competitive intelligence research is distinguishable from industrial espionage, as CI practitioners generally abide by local legal guidelines and ethical business norms.[43]
Outsourcing has become a big business for competitive intelligence professionals. There are many different companies in this field, including market research and consulting firms.[44]
"
986349135,2020-10-31 08:48:03,"Competitive intelligence (CI) is the systematic collection and analysis of information from multiple sources, and a coordinated CI program.[1] It is the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers in strategic decision making for an organization.
CI means understanding and learning what is happening in the world outside the business to increase one's competitiveness. It means learning as much as possible, as soon as possible, about one's external environment including one's industry in general and relevant competitors.[2]
Key points:
Another definition of CI regards it as the organizational function responsible for the early identification of risks and opportunities in the market before they become obvious (""early signal analysis""). This definition focuses attention on the difference between dissemination of widely available factual information (such as market statistics, financial reports, newspaper clippings) performed by functions such as libraries and information centers, and competitive intelligence which is a perspective on developments and events aimed at yielding a competitive edge.[6]
The term CI is often viewed as synonymous with competitor analysis, but competitive intelligence is more than analyzing competitors; it embraces the entire environment and stakeholders: customers, competitors, distributors, technologies, and macroeconomic data. It is also a tool for decision making.
CI literature is best exemplified by the bibliographies that were published in the Society of Competitive Intelligence Professionals' academic journal The Journal of Competitive Intelligence and Management.[7][8][9][10] Although elements of organizational intelligence collection have been a part of business for many years, the history of competitive intelligence arguably began in the U.S. in the 1970s, although the literature on the field pre-dates this time by at least several decades.[10] In 1980, Michael Porter published the study Competitive-Strategy: Techniques for Analyzing Industries and Competitors which is widely viewed as the foundation of modern competitive intelligence. This has since been extended most notably by the pair of Craig Fleisher and Babette Bensoussan, who through several popular books on competitive analysis have added 48 commonly applied competitive intelligence analysis techniques to the practitioner's tool box.[11][12] In 1985, Leonard Fuld published his best selling book dedicated to competitor intelligence.[13] However, the institutionalization of CI as a formal activity among American corporations can be traced to 1988, when Ben and Tamar Gilad published the first organizational model of a formal corporate CI function, which was then adopted widely by US companies.[14] The first professional certification program (CIP) was created in 1996 with the establishment of The Fuld-Gilad-Herring Academy of Competitive Intelligence in Cambridge, Massachusetts.
In 1986 the Society of Competitive Intelligence Professionals (SCIP) was founded in the United States and grew in the late 1990s to around 6,000 members worldwide, mainly in the United States and Canada, but with large numbers especially in the UK and Australia. Due to financial difficulties in 2009, the organization merged with Frost & Sullivan under the Frost & Sullivan Institute. SCIP has since been renamed ""Strategic & Competitive Intelligence Professionals"" to emphasise the strategic nature of the subject, and also to refocus the organisation's general approach, while keeping the existing SCIP brandname and logo. A number of efforts have been made to discuss the field's advances in post-secondary (university) education, covered by several authors including Blenkhorn & Fleisher,[15] Fleisher,[16] Fuld,[17] Prescott,[18] and McGonagle.[19] Although the general view would be that competitive intelligence concepts can be readily found and taught in many business schools around the globe, there are still relatively few dedicated academic programs, majors, or degrees in the field, a concern to academics in the field who would like to see it further researched.[16] These issues were widely discussed by over a dozen knowledgeable individuals in a special edition of the Competitive Intelligence Magazine that was dedicated to this topic.[20] In France, a Specialized Master in Economic Intelligence and Knowledge Management was created in 1995 within the CERAM Business School, now SKEMA Business School, in Paris, with the objective of delivering a full and professional training in Economic Intelligence. A Centre for Global Intelligence and Influence was created in September 2011 in the same School.
On the other hand, practitioners and companies regard professional accreditation as especially important in this field.[21] In 2011, SCIP recognized the Fuld-Gilad-Herring Academy of Competitive Intelligence's CIP certification process as its global, dual-level (CIP-I and CIP-II) certification program.
Global developments have also been uneven in competitive intelligence.[22] Several academic journals, particularly the Journal of Competitive Intelligence and Management in its third volume, provided coverage of the field's global development.[23] For example, in 1997 the École de guerre économique [fr] (School of economic warfare) was founded in Paris, France. It is the first European institution which teaches the tactics of economic warfare within a globalizing world. In Germany, competitive intelligence was unattended until the early 1990s. The term ""competitive intelligence"" first appeared in German literature in 1997. In 1995 a German SCIP chapter was founded, which is now second in terms of membership in Europe. In summer 2004 the Institute for Competitive Intelligence was founded, which provides a postgraduate certification program for Competitive Intelligence Professionals. Japan is currently the only country that officially maintains an economic intelligence agency (JETRO). It was founded by the Ministry of International Trade and Industry in 1958.
Accepting the importance of competitive intelligence, major multinational corporations, such as ExxonMobil, Procter & Gamble, and Johnson and Johnson, have created formal CI units.[citation needed] Importantly, organizations execute competitive intelligence activities not only as a safeguard to protect against market threats and changes, but also as a method for finding new opportunities and trends.[24]
Organizations use competitive intelligence to compare themselves to other organizations (""competitive benchmarking""), to identify risks and opportunities in their markets, and to pressure-test their plans against market response (business wargaming),[25] which enable them to make informed decisions. Most firms today realize the importance of knowing what their competitors are doing and how the industry is changing, and the information gathered allows organizations to understand their strengths and weaknesses.
One of the major activities involved in corporate competitive intelligence is use of ratio analysis, using key performance indicators (KPI). Organizations compare annual reports of their competitors on certain KPI and ratios, which are intrinsic to their industry. This helps them track their performance, vis-a-vis their competitors.
The actual importance of these categories of information to an organization depends on the contestability of its markets, the organizational culture, the personality and biases of its top decision makers, and the reporting structure of competitive intelligence within the company.
Strategic Intelligence (SI) focuses on the longer term, looking at issues affecting a company's competitiveness over the course of a couple of years. The actual time horizon for SI ultimately depends on the industry and how quickly it's changing. The general questions that SI answers are, ‘Where should we as a company be in X years?' and 'What are the strategic risks and opportunities facing us?' This type of intelligence work involves among others the identification of weak signals and application of methodology and process called Strategic Early Warning (SEW), first introduced by Gilad,[26][27][28] followed by Steven Shaker and Victor Richardson,[29] Alessandro Comai and Joaquin Tena,[30][31] and others. According to Gilad, 20% of the work of competitive intelligence practitioners should be dedicated to strategic early identification of weak signals within a SEW framework.
Tactical Intelligence: the focus is on providing information designed to improve shorter-term decisions, most often related with the intent of growing market share or revenues. Generally, it is the type of information that you would need to support the sales process in an organization. It investigates various aspects of a product/product line marketing:
With the right amount of information, organizations can avoid unpleasant surprises by anticipating competitors' moves and decreasing response time. Examples of competitive intelligence research is evident in daily newspapers, such as The Wall Street Journal, Business Week, and Fortune. Major airlines change hundreds of fares daily in response to competitors' tactics. They use information to plan their own marketing, pricing, and production strategies.
Resources, such as the Internet, have made gathering information on competitors easy. With a click of a button, analysts can discover future trends and market requirements. However competitive intelligence is much more than this, as the ultimate aim is to lead to competitive advantage. As the Internet is mostly public domain material, information gathered is less likely to result in insights that will be unique to the company. In fact there is a risk that information gathered from the Internet will be misinformation and mislead users, so competitive intelligence researchers are often wary of using such information.
As a result, although the Internet is viewed as a key source, most CI professionals should spend their time and budget gathering intelligence using primary research—networking with industry experts, from trade shows and conferences, from their own customers and suppliers, and so on. Where the Internet is used, it is to gather sources for primary research as well as information on what the company says about itself and its online presence (in the form of links to other companies, its strategy regarding search engines and online advertising, mentions in discussion forums and on blogs, etc.). Also, important are online subscription databases and news aggregation sources which have simplified the secondary source collection process. Social media sources are also becoming important—providing potential interviewee names, as well as opinions and attitudes, and sometimes breaking news (e.g., via Twitter).
Organizations must be careful not to spend too much time and effort on old competitors without realizing the existence of any new competitors. Knowing more about your competitors will allow your business to grow and succeed. The practice of competitive intelligence is growing every year, and most companies and business students now realize the importance of knowing their competitors.
According to Arjan Singh and Andrew Beurschgens in their 2006 article in the Competitive Intelligence Review, there are four stages of development of a competitive intelligence capability with a firm. It starts with ""stick fetching"", where a CI department is very reactive, up to ""world class"", where it is completely integrated in the decision-making process.
The technical advances in massive parallel processing offered by the Hadoop ""big data"" architecture has allowed the creation of multiple platforms for named-entity recognition such as the Apache Projects OpenNLP and Apache Stanbol. The former includes pre-trained statistical parsers that can discern elements key to establishing trends and evaluating competitive position and responding appropriately.[32] Public information mining from SEC.gov, Federal Contract Awards, social media (Twitter, Reddit, Facebook, and others), vendors, and competitor websites now permit real-time counterintelligence as a strategy for horizontal and vertical market expansion and product positioning. This occurs in an automated fashion on massive marketplaces such as Amazon.com and their classification and prediction of product associations and purchase probability.
Competitive intelligence has been influenced by national strategic intelligence. Although national intelligence was researched 50 years ago, competitive intelligence was introduced during the 1990s. Competitive-intelligence professionals can learn from national-intelligence experts, especially in the analysis of complex situations.[33] Competitive intelligence may be confused with (or seen to overlap) environmental scanning, business intelligence and market research.[34] Craig Fleisher[34] questions the appropriateness of the term, comparing it to business intelligence, competitor intelligence, knowledge management, market intelligence, marketing research and strategic intelligence.[35]
Fleisher[35][verification needed] suggests that business intelligence has two forms. Its narrow (contemporary) form is more focused on information technology and internal focus than CI, while its broader (historical) definition is more inclusive than CI. Knowledge management (KM), when improperly achieved, is seen as an information-technology driven organizational practice relying on data mining, corporate intranets and mapping organizational assets to make it accessible to organization members for decision-making. CI shares some aspects of KM; they are human-intelligence- and experience-based for a more-sophisticated qualitative analysis. km is essential for effective change. A key effective factor is a powerful, dedicated IT system executing the full intelligence cycle.[36]
Market intelligence (MI) is industry-targeted intelligence developed in real-time aspects of competitive events taking place among the four Ps of the marketing mix (pricing, place, promotion and product) in the product (or service) marketplace to better understand the market's attractiveness.[37] A time-based competitive tactic, MI is used by marketing and sales managers to respond to consumers more quickly in the marketplace. Fleisher suggests it is not distributed as widely as some forms of CI, which are also distributed to non-marketing decision-makers.[35][verification needed] Market intelligence has a shorter time horizon than other intelligence areas, and is measured in days, weeks, or (in slower-moving industries) months.
Market research is a tactical, method-driven field consisting of neutral, primary research of customer data (beliefs and perceptions) gathered in surveys or focus groups, and is analyzed with statistical-research techniques.[38] CI draws on a wider variety (primary and secondary) of sources from a wider range of stakeholders (suppliers, competitors, distributors, substitutes and media) to answer existing questions, raise new ones and guide action.[35][verification needed]
Ben Gilad and Jan Herring lay down a set of prerequisites defining CI, distinguishing it from other information-rich disciplines such as market research or business development. They show that a common body of knowledge and a unique set of tools (key intelligence topics, business war games and blindspots analysis) distinguish CI; while other sensory activities in a commercial firm focus on one segment of the market (customers, suppliers or acquisition targets), CI synthesizes data from all high-impact players (HIP).[21]
Gilad later focused his delineation of CI on the difference between information and intelligence. According to him, the common denominator among organizational sensory functions (whether they are called market research, business intelligence or market intelligence) is that they deliver information rather than intelligence. Intelligence, says Gilad, is a perspective on facts rather than the facts themselves. Unique among corporate functions, competitive intelligence has a perspective of risks and opportunities for a firm's performance; as such, it (not information activities) is part of an organization's risk-management activity.[39]
Ethics has been a long-held issue of discussion among CI practitioners.[34] The questions revolve around what is and is not allowable in terms of CI activity. A number of scholarly treatments have been generated on this topic, most prominently addressed through Society of Competitive Intelligence Professionals publications.[40] The book Competitive Intelligence Ethics: Navigating the Gray Zone provides nearly twenty separate views about ethics in CI, as well as another 10 codes used by various individuals or organizations.[40] Combining that with the over two dozen scholarly articles or studies found within the various CI bibliographic entries,[41][verification needed][9][10][42] it is clear that no shortage of study has gone into better classifying, understanding and addressing CI ethics.
Competitive information may be obtained from public or subscription sources, from networking with competitor staff or customers, disassembly of competitor products or from field research interviews. Competitive intelligence research is distinguishable from industrial espionage, as CI practitioners generally abide by local legal guidelines and ethical business norms.[43]
Outsourcing has become a big business for competitive intelligence professionals. There are many different companies in this field, including market research and consulting firms.[44]
"
988509148,2020-11-13 16:44:41,"Competitive intelligence (CI) is the systematic collection and analysis of information from multiple sources, and a coordinated CI program.[1] It is the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers in strategic decision making for an organization.
CI means understanding and learning what is happening in the world outside the business to increase one's competitiveness. It means learning as much as possible, as soon as possible, about one's external environment including one's industry in general and relevant competitors.[2]
Key points:
Another definition of CI regards it as the organizational function responsible for the early identification of risks and opportunities in the market before they become obvious (""early signal analysis""). This definition focuses attention on the difference between dissemination of widely available factual information (such as market statistics, financial reports, newspaper clippings) performed by functions such as libraries and information centers, and competitive intelligence which is a perspective on developments and events aimed at yielding a competitive edge.[6]
The term CI is often viewed as synonymous with competitor analysis, but competitive intelligence is more than analyzing competitors; it embraces the entire environment and stakeholders: customers, competitors, distributors, technologies, and macroeconomic data. It is also a tool for decision making.
CI literature is best exemplified by the bibliographies that were published in the Society of Competitive Intelligence Professionals' academic journal The Journal of Competitive Intelligence and Management.[7][8][9][10] Although elements of organizational intelligence collection have been a part of business for many years, the history of competitive intelligence arguably began in the U.S. in the 1970s, although the literature on the field pre-dates this time by at least several decades.[10] In 1980, Michael Porter published the study Competitive-Strategy: Techniques for Analyzing Industries and Competitors which is widely viewed as the foundation of modern competitive intelligence. This has since been extended most notably by the pair of Craig Fleisher and Babette Bensoussan, who through several popular books on competitive analysis have added 48 commonly applied competitive intelligence analysis techniques to the practitioner's tool box.[11][12] In 1985, Leonard Fuld published his best selling book dedicated to competitor intelligence.[13] However, the institutionalization of CI as a formal activity among American corporations can be traced to 1988, when Ben and Tamar Gilad published the first organizational model of a formal corporate CI function, which was then adopted widely by US companies.[14] The first professional certification program (CIP) was created in 1996 with the establishment of The Fuld-Gilad-Herring Academy of Competitive Intelligence in Cambridge, Massachusetts.
In 1986 the Society of Competitive Intelligence Professionals (SCIP) was founded in the United States and grew in the late 1990s to around 6,000 members worldwide, mainly in the United States and Canada, but with large numbers especially in the UK and Australia. Due to financial difficulties in 2009, the organization merged with Frost & Sullivan under the Frost & Sullivan Institute. SCIP has since been renamed ""Strategic & Competitive Intelligence Professionals"" to emphasise the strategic nature of the subject, and also to refocus the organisation's general approach, while keeping the existing SCIP brandname and logo. A number of efforts have been made to discuss the field's advances in post-secondary (university) education, covered by several authors including Blenkhorn & Fleisher,[15] Fleisher,[16] Fuld,[17] Prescott,[18] and McGonagle.[19] Although the general view would be that competitive intelligence concepts can be readily found and taught in many business schools around the globe, there are still relatively few dedicated academic programs, majors, or degrees in the field, a concern to academics in the field who would like to see it further researched.[16] These issues were widely discussed by over a dozen knowledgeable individuals in a special edition of the Competitive Intelligence Magazine that was dedicated to this topic.[20] In France, a Specialized Master in Economic Intelligence and Knowledge Management was created in 1995 within the CERAM Business School, now SKEMA Business School, in Paris, with the objective of delivering a full and professional training in Economic Intelligence. A Centre for Global Intelligence and Influence was created in September 2011 in the same School.
On the other hand, practitioners and companies regard professional accreditation as especially important in this field.[21] In 2011, SCIP recognized the Fuld-Gilad-Herring Academy of Competitive Intelligence's CIP certification process as its global, dual-level (CIP-I and CIP-II) certification program.
Global developments have also been uneven in competitive intelligence.[22] Several academic journals, particularly the Journal of Competitive Intelligence and Management in its third volume, provided coverage of the field's global development.[23] For example, in 1997 the École de guerre économique [fr] (School of economic warfare) was founded in Paris, France. It is the first European institution which teaches the tactics of economic warfare within a globalizing world. In Germany, competitive intelligence was unattended until the early 1990s. The term ""competitive intelligence"" first appeared in German literature in 1997. In 1995 a German SCIP chapter was founded, which is now second in terms of membership in Europe. In summer 2004 the Institute for Competitive Intelligence was founded, which provides a postgraduate certification program for Competitive Intelligence Professionals. Japan is currently the only country that officially maintains an economic intelligence agency (JETRO). It was founded by the Ministry of International Trade and Industry in 1958.
Accepting the importance of competitive intelligence, major multinational corporations, such as ExxonMobil, Procter & Gamble, and Johnson and Johnson, have created formal CI units.[citation needed] Importantly, organizations execute competitive intelligence activities not only as a safeguard to protect against market threats and changes, but also as a method for finding new opportunities and trends.[24]
Organizations use competitive intelligence to compare themselves to other organizations (""competitive benchmarking""), to identify risks and opportunities in their markets, and to pressure-test their plans against market response (business wargaming),[25] which enable them to make informed decisions. Most firms today realize the importance of knowing what their competitors are doing and how the industry is changing, and the information gathered allows organizations to understand their strengths and weaknesses.
One of the major activities involved in corporate competitive intelligence is use of ratio analysis, using key performance indicators (KPI). Organizations compare annual reports of their competitors on certain KPI and ratios, which are intrinsic to their industry. This helps them track their performance, vis-a-vis their competitors.
The actual importance of these categories of information to an organization depends on the contestability of its markets, the organizational culture, the personality and biases of its top decision makers, and the reporting structure of competitive intelligence within the company.
Strategic Intelligence (SI) focuses on the longer term, looking at issues affecting a company's competitiveness over the course of a couple of years. The actual time horizon for SI ultimately depends on the industry and how quickly it's changing. The general questions that SI answers are, ‘Where should we as a company be in X years?' and 'What are the strategic risks and opportunities facing us?' This type of intelligence work involves among others the identification of weak signals and application of methodology and process called Strategic Early Warning (SEW), first introduced by Gilad,[26][27][28] followed by Steven Shaker and Victor Richardson,[29] Alessandro Comai and Joaquin Tena,[30][31] and others. According to Gilad, 20% of the work of competitive intelligence practitioners should be dedicated to strategic early identification of weak signals within a SEW framework.
Tactical Intelligence: the focus is on providing information designed to improve shorter-term decisions, most often related with the intent of growing market share or revenues. Generally, it is the type of information that you would need to support the sales process in an organization. It investigates various aspects of a product/product line marketing:
With the right amount of information, organizations can avoid unpleasant surprises by anticipating competitors' moves and decreasing response time. Examples of competitive intelligence research is evident in daily newspapers, such as The Wall Street Journal, Business Week, and Fortune. Major airlines change hundreds of fares daily in response to competitors' tactics. They use information to plan their own marketing, pricing, and production strategies.
Resources, such as the Internet, have made gathering information on competitors easy. With a click of a button, analysts can discover future trends and market requirements. However competitive intelligence is much more than this, as the ultimate aim is to lead to competitive advantage. As the Internet is mostly public domain material, information gathered is less likely to result in insights that will be unique to the company. In fact there is a risk that information gathered from the Internet will be misinformation and mislead users, so competitive intelligence researchers are often wary of using such information.
As a result, although the Internet is viewed as a key source, most CI professionals should spend their time and budget gathering intelligence using primary research—networking with industry experts, from trade shows and conferences, from their own customers and suppliers, and so on. Where the Internet is used, it is to gather sources for primary research as well as information on what the company says about itself and its online presence (in the form of links to other companies, its strategy regarding search engines and online advertising, mentions in discussion forums and on blogs, etc.). Also, important are online subscription databases and news aggregation sources which have simplified the secondary source collection process. Social media sources are also becoming important—providing potential interviewee names, as well as opinions and attitudes, and sometimes breaking news (e.g., via Twitter).
Organizations must be careful not to spend too much time and effort on old competitors without realizing the existence of any new competitors. Knowing more about your competitors will allow your business to grow and succeed. The practice of competitive intelligence is growing every year, and most companies and business students now realize the importance of knowing their competitors.
The technical advances in massive parallel processing offered by the Hadoop ""big data"" architecture has allowed the creation of multiple platforms for named-entity recognition such as the Apache Projects OpenNLP and Apache Stanbol. The former includes pre-trained statistical parsers that can discern elements key to establishing trends and evaluating competitive position and responding appropriately.[32] Public information mining from SEC.gov, Federal Contract Awards, social media (Twitter, Reddit, Facebook, and others), vendors, and competitor websites now permit real-time counterintelligence as a strategy for horizontal and vertical market expansion and product positioning. This occurs in an automated fashion on massive marketplaces such as Amazon.com and their classification and prediction of product associations and purchase probability.
A new industry emerged of tech companies with tools that simplify and automate the way companies conduct competitive intelligence. With technology responsible for scraping billions of pieces of data and pulling it into a central platform, this new trend of competitive intelligence tools has effectively reshaped how competitor analysis is performed and intelligence gathered. [33].
Competitive intelligence has been influenced by national strategic intelligence. Although national intelligence was researched 50 years ago, competitive intelligence was introduced during the 1990s. Competitive-intelligence professionals can learn from national-intelligence experts, especially in the analysis of complex situations.[34] Competitive intelligence may be confused with (or seen to overlap) environmental scanning, business intelligence and market research.[35] Craig Fleisher[35] questions the appropriateness of the term, comparing it to business intelligence, competitor intelligence, knowledge management, market intelligence, marketing research and strategic intelligence.[36]
Fleisher[36][verification needed] suggests that business intelligence has two forms. Its narrow (contemporary) form is more focused on information technology and internal focus than CI, while its broader (historical) definition is more inclusive than CI. Knowledge management (KM), when improperly achieved, is seen as an information-technology driven organizational practice relying on data mining, corporate intranets and mapping organizational assets to make it accessible to organization members for decision-making. CI shares some aspects of KM; they are human-intelligence- and experience-based for a more-sophisticated qualitative analysis. km is essential for effective change. A key effective factor is a powerful, dedicated IT system executing the full intelligence cycle.[37]
Market intelligence (MI) is industry-targeted intelligence developed in real-time aspects of competitive events taking place among the four Ps of the marketing mix (pricing, place, promotion and product) in the product (or service) marketplace to better understand the market's attractiveness.[38] A time-based competitive tactic, MI is used by marketing and sales managers to respond to consumers more quickly in the marketplace. Fleisher suggests it is not distributed as widely as some forms of CI, which are also distributed to non-marketing decision-makers.[36][verification needed] Market intelligence has a shorter time horizon than other intelligence areas, and is measured in days, weeks, or (in slower-moving industries) months.
Market research is a tactical, method-driven field consisting of neutral, primary research of customer data (beliefs and perceptions) gathered in surveys or focus groups, and is analyzed with statistical-research techniques.[39] CI draws on a wider variety (primary and secondary) of sources from a wider range of stakeholders (suppliers, competitors, distributors, substitutes and media) to answer existing questions, raise new ones and guide action.[36][verification needed]
Ben Gilad and Jan Herring lay down a set of prerequisites defining CI, distinguishing it from other information-rich disciplines such as market research or business development. They show that a common body of knowledge and a unique set of tools (key intelligence topics, business war games and blindspots analysis) distinguish CI; while other sensory activities in a commercial firm focus on one segment of the market (customers, suppliers or acquisition targets), CI synthesizes data from all high-impact players (HIP).[21]
Gilad later focused his delineation of CI on the difference between information and intelligence. According to him, the common denominator among organizational sensory functions (whether they are called market research, business intelligence or market intelligence) is that they deliver information rather than intelligence. Intelligence, says Gilad, is a perspective on facts rather than the facts themselves. Unique among corporate functions, competitive intelligence has a perspective of risks and opportunities for a firm's performance; as such, it (not information activities) is part of an organization's risk-management activity.[40]
Ethics has been a long-held issue of discussion among CI practitioners.[35] The questions revolve around what is and is not allowable in terms of CI activity. A number of scholarly treatments have been generated on this topic, most prominently addressed through Society of Competitive Intelligence Professionals publications.[41] The book Competitive Intelligence Ethics: Navigating the Gray Zone provides nearly twenty separate views about ethics in CI, as well as another 10 codes used by various individuals or organizations.[41] Combining that with the over two dozen scholarly articles or studies found within the various CI bibliographic entries,[42][verification needed][9][10][43] it is clear that no shortage of study has gone into better classifying, understanding and addressing CI ethics.
Competitive information may be obtained from public or subscription sources, from networking with competitor staff or customers, disassembly of competitor products or from field research interviews. Competitive intelligence research is distinguishable from industrial espionage, as CI practitioners generally abide by local legal guidelines and ethical business norms.[44]
Outsourcing has become a big business for competitive intelligence professionals. There are many different companies in this field, including market research and consulting firms.[45]
"
988638385,2020-11-14 10:54:44,"Competitive intelligence (CI) is the systematic collection and analysis of information from multiple sources, and a coordinated CI program.[1] It is the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers in strategic decision making for an organization.
CI means understanding and learning what is happening in the world outside the business to increase one's competitiveness. It means learning as much as possible, as soon as possible, about one's external environment including one's industry in general and relevant competitors.[2]
Key points:
Another definition of CI regards it as the organizational function responsible for the early identification of risks and opportunities in the market before they become obvious (""early signal analysis""). This definition focuses attention on the difference between dissemination of widely available factual information (such as market statistics, financial reports, newspaper clippings) performed by functions such as libraries and information centers, and competitive intelligence which is a perspective on developments and events aimed at yielding a competitive edge.[6]
The term CI is often viewed as synonymous with competitor analysis, but competitive intelligence is more than analyzing competitors; it embraces the entire environment and stakeholders: customers, competitors, distributors, technologies, and macroeconomic data. It is also a tool for decision making.
CI literature is best exemplified by the bibliographies that were published in the Society of Competitive Intelligence Professionals' academic journal The Journal of Competitive Intelligence and Management.[7][8][9][10] Although elements of organizational intelligence collection have been a part of business for many years, the history of competitive intelligence arguably began in the U.S. in the 1970s, although the literature on the field pre-dates this time by at least several decades.[10] In 1980, Michael Porter published the study Competitive-Strategy: Techniques for Analyzing Industries and Competitors which is widely viewed as the foundation of modern competitive intelligence. This has since been extended most notably by the pair of Craig Fleisher and Babette Bensoussan, who through several popular books on competitive analysis have added 48 commonly applied competitive intelligence analysis techniques to the practitioner's tool box.[11][12] In 1985, Leonard Fuld published his best selling book dedicated to competitor intelligence.[13] However, the institutionalization of CI as a formal activity among American corporations can be traced to 1988, when Ben and Tamar Gilad published the first organizational model of a formal corporate CI function, which was then adopted widely by US companies.[14] The first professional certification program (CIP) was created in 1996 with the establishment of The Fuld-Gilad-Herring Academy of Competitive Intelligence in Cambridge, Massachusetts.
In 1986 the Society of Competitive Intelligence Professionals (SCIP) was founded in the United States and grew in the late 1990s to around 6,000 members worldwide, mainly in the United States and Canada, but with large numbers especially in the UK and Australia. Due to financial difficulties in 2009, the organization merged with Frost & Sullivan under the Frost & Sullivan Institute. SCIP has since been renamed ""Strategic & Competitive Intelligence Professionals"" to emphasise the strategic nature of the subject, and also to refocus the organisation's general approach, while keeping the existing SCIP brandname and logo. A number of efforts have been made to discuss the field's advances in post-secondary (university) education, covered by several authors including Blenkhorn & Fleisher,[15] Fleisher,[16] Fuld,[17] Prescott,[18] and McGonagle.[19] Although the general view would be that competitive intelligence concepts can be readily found and taught in many business schools around the globe, there are still relatively few dedicated academic programs, majors, or degrees in the field, a concern to academics in the field who would like to see it further researched.[16] These issues were widely discussed by over a dozen knowledgeable individuals in a special edition of the Competitive Intelligence Magazine that was dedicated to this topic.[20] In France, a Specialized Master in Economic Intelligence and Knowledge Management was created in 1995 within the CERAM Business School, now SKEMA Business School, in Paris, with the objective of delivering a full and professional training in Economic Intelligence. A Centre for Global Intelligence and Influence was created in September 2011 in the same School.
On the other hand, practitioners and companies regard professional accreditation as especially important in this field.[21] In 2011, SCIP recognized the Fuld-Gilad-Herring Academy of Competitive Intelligence's CIP certification process as its global, dual-level (CIP-I and CIP-II) certification program.
Global developments have also been uneven in competitive intelligence.[22] Several academic journals, particularly the Journal of Competitive Intelligence and Management in its third volume, provided coverage of the field's global development.[23] For example, in 1997 the École de guerre économique [fr] (School of economic warfare) was founded in Paris, France. It is the first European institution which teaches the tactics of economic warfare within a globalizing world. In Germany, competitive intelligence was unattended until the early 1990s. The term ""competitive intelligence"" first appeared in German literature in 1997. In 1995 a German SCIP chapter was founded, which is now second in terms of membership in Europe. In summer 2004 the Institute for Competitive Intelligence was founded, which provides a postgraduate certification program for Competitive Intelligence Professionals. Japan is currently the only country that officially maintains an economic intelligence agency (JETRO). It was founded by the Ministry of International Trade and Industry in 1958.
Accepting the importance of competitive intelligence, major multinational corporations, such as ExxonMobil, Procter & Gamble, and Johnson and Johnson, have created formal CI units.[citation needed] Importantly, organizations execute competitive intelligence activities not only as a safeguard to protect against market threats and changes, but also as a method for finding new opportunities and trends.[24]
Organizations use competitive intelligence to compare themselves to other organizations (""competitive benchmarking""), to identify risks and opportunities in their markets, and to pressure-test their plans against market response (business wargaming),[25] which enable them to make informed decisions. Most firms today realize the importance of knowing what their competitors are doing and how the industry is changing, and the information gathered allows organizations to understand their strengths and weaknesses.
One of the major activities involved in corporate competitive intelligence is use of ratio analysis, using key performance indicators (KPI). Organizations compare annual reports of their competitors on certain KPI and ratios, which are intrinsic to their industry. This helps them track their performance, vis-a-vis their competitors.
The actual importance of these categories of information to an organization depends on the contestability of its markets, the organizational culture, the personality and biases of its top decision makers, and the reporting structure of competitive intelligence within the company.
Strategic Intelligence (SI) focuses on the longer term, looking at issues affecting a company's competitiveness over the course of a couple of years. The actual time horizon for SI ultimately depends on the industry and how quickly it's changing. The general questions that SI answers are, ‘Where should we as a company be in X years?' and 'What are the strategic risks and opportunities facing us?' This type of intelligence work involves among others the identification of weak signals and application of methodology and process called Strategic Early Warning (SEW), first introduced by Gilad,[26][27][28] followed by Steven Shaker and Victor Richardson,[29] Alessandro Comai and Joaquin Tena,[30][31] and others. According to Gilad, 20% of the work of competitive intelligence practitioners should be dedicated to strategic early identification of weak signals within a SEW framework.
Tactical Intelligence: the focus is on providing information designed to improve shorter-term decisions, most often related with the intent of growing market share or revenues. Generally, it is the type of information that you would need to support the sales process in an organization. It investigates various aspects of a product/product line marketing:
With the right amount of information, organizations can avoid unpleasant surprises by anticipating competitors' moves and decreasing response time. Examples of competitive intelligence research is evident in daily newspapers, such as The Wall Street Journal, Business Week, and Fortune. Major airlines change hundreds of fares daily in response to competitors' tactics. They use information to plan their own marketing, pricing, and production strategies.
Resources, such as the Internet, have made gathering information on competitors easy. With a click of a button, analysts can discover future trends and market requirements. However competitive intelligence is much more than this, as the ultimate aim is to lead to competitive advantage. As the Internet is mostly public domain material, information gathered is less likely to result in insights that will be unique to the company. In fact there is a risk that information gathered from the Internet will be misinformation and mislead users, so competitive intelligence researchers are often wary of using such information.
As a result, although the Internet is viewed as a key source, most CI professionals should spend their time and budget gathering intelligence using primary research—networking with industry experts, from trade shows and conferences, from their own customers and suppliers, and so on. Where the Internet is used, it is to gather sources for primary research as well as information on what the company says about itself and its online presence (in the form of links to other companies, its strategy regarding search engines and online advertising, mentions in discussion forums and on blogs, etc.). Also, important are online subscription databases and news aggregation sources which have simplified the secondary source collection process. Social media sources are also becoming important—providing potential interviewee names, as well as opinions and attitudes, and sometimes breaking news (e.g., via Twitter).
Organizations must be careful not to spend too much time and effort on old competitors without realizing the existence of any new competitors. Knowing more about your competitors will allow your business to grow and succeed. The practice of competitive intelligence is growing every year, and most companies and business students now realize the importance of knowing their competitors.
The technical advances in massive parallel processing offered by the Hadoop ""big data"" architecture has allowed the creation of multiple platforms for named-entity recognition such as the Apache Projects OpenNLP and Apache Stanbol. The former includes pre-trained statistical parsers that can discern elements key to establishing trends and evaluating competitive position and responding appropriately.[32] Public information mining from SEC.gov, Federal Contract Awards, social media (Twitter, Reddit, Facebook, and others), vendors, and competitor websites now permit real-time counterintelligence as a strategy for horizontal and vertical market expansion and product positioning. This occurs in an automated fashion on massive marketplaces such as Amazon.com and their classification and prediction of product associations and purchase probability.
A new industry emerged of tech companies with tools that simplify and automate the way companies conduct competitive intelligence. With technology responsible for scraping billions of pieces of data and pulling it into a central platform, this new trend of competitive intelligence tools has effectively reshaped how competitor analysis is performed and intelligence gathered.[33]
Competitive intelligence has been influenced by national strategic intelligence. Although national intelligence was researched 50 years ago, competitive intelligence was introduced during the 1990s. Competitive-intelligence professionals can learn from national-intelligence experts, especially in the analysis of complex situations.[34] Competitive intelligence may be confused with (or seen to overlap) environmental scanning, business intelligence and market research.[35] Craig Fleisher[35] questions the appropriateness of the term, comparing it to business intelligence, competitor intelligence, knowledge management, market intelligence, marketing research and strategic intelligence.[36]
Fleisher[36][verification needed] suggests that business intelligence has two forms. Its narrow (contemporary) form is more focused on information technology and internal focus than CI, while its broader (historical) definition is more inclusive than CI. Knowledge management (KM), when improperly achieved, is seen as an information-technology driven organizational practice relying on data mining, corporate intranets and mapping organizational assets to make it accessible to organization members for decision-making. CI shares some aspects of KM; they are human-intelligence- and experience-based for a more-sophisticated qualitative analysis. km is essential for effective change. A key effective factor is a powerful, dedicated IT system executing the full intelligence cycle.[37]
Market intelligence (MI) is industry-targeted intelligence developed in real-time aspects of competitive events taking place among the four Ps of the marketing mix (pricing, place, promotion and product) in the product (or service) marketplace to better understand the market's attractiveness.[38] A time-based competitive tactic, MI is used by marketing and sales managers to respond to consumers more quickly in the marketplace. Fleisher suggests it is not distributed as widely as some forms of CI, which are also distributed to non-marketing decision-makers.[36][verification needed] Market intelligence has a shorter time horizon than other intelligence areas, and is measured in days, weeks, or (in slower-moving industries) months.
Market research is a tactical, method-driven field consisting of neutral, primary research of customer data (beliefs and perceptions) gathered in surveys or focus groups, and is analyzed with statistical-research techniques.[39] CI draws on a wider variety (primary and secondary) of sources from a wider range of stakeholders (suppliers, competitors, distributors, substitutes and media) to answer existing questions, raise new ones and guide action.[36][verification needed]
Ben Gilad and Jan Herring lay down a set of prerequisites defining CI, distinguishing it from other information-rich disciplines such as market research or business development. They show that a common body of knowledge and a unique set of tools (key intelligence topics, business war games and blindspots analysis) distinguish CI; while other sensory activities in a commercial firm focus on one segment of the market (customers, suppliers or acquisition targets), CI synthesizes data from all high-impact players (HIP).[21]
Gilad later focused his delineation of CI on the difference between information and intelligence. According to him, the common denominator among organizational sensory functions (whether they are called market research, business intelligence or market intelligence) is that they deliver information rather than intelligence. Intelligence, says Gilad, is a perspective on facts rather than the facts themselves. Unique among corporate functions, competitive intelligence has a perspective of risks and opportunities for a firm's performance; as such, it (not information activities) is part of an organization's risk-management activity.[40]
Ethics has been a long-held issue of discussion among CI practitioners.[35] The questions revolve around what is and is not allowable in terms of CI activity. A number of scholarly treatments have been generated on this topic, most prominently addressed through Society of Competitive Intelligence Professionals publications.[41] The book Competitive Intelligence Ethics: Navigating the Gray Zone provides nearly twenty separate views about ethics in CI, as well as another 10 codes used by various individuals or organizations.[41] Combining that with the over two dozen scholarly articles or studies found within the various CI bibliographic entries,[42][verification needed][9][10][43] it is clear that no shortage of study has gone into better classifying, understanding and addressing CI ethics.
Competitive information may be obtained from public or subscription sources, from networking with competitor staff or customers, disassembly of competitor products or from field research interviews. Competitive intelligence research is distinguishable from industrial espionage, as CI practitioners generally abide by local legal guidelines and ethical business norms.[44]
Outsourcing has become a big business for competitive intelligence professionals. There are many different companies in this field, including market research and consulting firms.[45]
"
990711368,2020-11-26 02:20:06,"Competitive intelligence (CI) is the systematic collection and analysis of information from multiple sources, and a coordinated CI program.[1] It is the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers in strategic decision making for an organization.
CI means understanding and learning what is happening in the world outside the business to increase one's competitiveness. It means learning as much as possible, as soon as possible, about one's external environment including one's industry in general and relevant competitors.[2]
Key points:
Another definition of CI regards it as the organizational function responsible for the early identification of risks and opportunities in the market before they become obvious (""early signal analysis""). This definition focuses attention on the difference between dissemination of widely available factual information (such as market statistics, financial reports, newspaper clippings) performed by functions such as libraries and information centers, and competitive intelligence which is a perspective on developments and events aimed at yielding a competitive edge.[6]
The term CI is often viewed as synonymous with competitor analysis, but competitive intelligence is more than analyzing competitors; it embraces the entire environment and stakeholders: customers, competitors, distributors, technologies, and macroeconomic data. It is also a tool for decision making.
CI literature is best exemplified by the bibliographies that were published in the Society of Competitive Intelligence Professionals' academic journal The Journal of Competitive Intelligence and Management.[7][8][9][10] Although elements of organizational intelligence collection have been a part of business for many years, the history of competitive intelligence arguably began in the U.S. in the 1970s, although the literature on the field pre-dates this time by at least several decades.[10] In 1980, Michael Porter published the study Competitive-Strategy: Techniques for Analyzing Industries and Competitors which is widely viewed as the foundation of modern competitive intelligence. This has since been extended most notably by the pair of Craig Fleisher and Babette Bensoussan, who through several popular books on competitive analysis have added 48 commonly applied competitive intelligence analysis techniques to the practitioner's tool box.[11][12] In 1985, Leonard Fuld published his best selling book dedicated to competitor intelligence.[13] However, the institutionalization of CI as a formal activity among American corporations can be traced to 1988, when Ben and Tamar Gilad published the first organizational model of a formal corporate CI function, which was then adopted widely by US companies.[14] The first professional certification program (CIP) was created in 1996 with the establishment of The Fuld-Gilad-Herring Academy of Competitive Intelligence in Cambridge, Massachusetts.
In 1986 the Society of Competitive Intelligence Professionals (SCIP) was founded in the United States and grew in the late 1990s to around 6,000 members worldwide, mainly in the United States and Canada, but with large numbers especially in the UK and Australia. Due to financial difficulties in 2009, the organization merged with Frost & Sullivan under the Frost & Sullivan Institute. SCIP has since been renamed ""Strategic & Competitive Intelligence Professionals"" to emphasise the strategic nature of the subject, and also to refocus the organisation's general approach, while keeping the existing SCIP brandname and logo. A number of efforts have been made to discuss the field's advances in post-secondary (university) education, covered by several authors including Blenkhorn & Fleisher,[15] Fleisher,[16] Fuld,[17] Prescott,[18] and McGonagle.[19] Although the general view would be that competitive intelligence concepts can be readily found and taught in many business schools around the globe, there are still relatively few dedicated academic programs, majors, or degrees in the field, a concern to academics in the field who would like to see it further researched.[16] These issues were widely discussed by over a dozen knowledgeable individuals in a special edition of the Competitive Intelligence Magazine that was dedicated to this topic.[20] In France, a Specialized Master in Economic Intelligence and Knowledge Management was created in 1995 within the CERAM Business School, now SKEMA Business School, in Paris, with the objective of delivering a full and professional training in Economic Intelligence. A Centre for Global Intelligence and Influence was created in September 2011 in the same School.
On the other hand, practitioners and companies regard professional accreditation as especially important in this field.[21] In 2011, SCIP recognized the Fuld-Gilad-Herring Academy of Competitive Intelligence's CIP certification process as its global, dual-level (CIP-I and CIP-II) certification program.
Global developments have also been uneven in competitive intelligence.[22] Several academic journals, particularly the Journal of Competitive Intelligence and Management in its third volume, provided coverage of the field's global development.[23] For example, in 1997 the École de guerre économique [fr] (School of economic warfare) was founded in Paris, France. It is the first European institution which teaches the tactics of economic warfare within a globalizing world. In Germany, competitive intelligence was unattended until the early 1990s. The term ""competitive intelligence"" first appeared in German literature in 1997. In 1995 a German SCIP chapter was founded, which is now second in terms of membership in Europe. In summer 2004 the Institute for Competitive Intelligence was founded, which provides a postgraduate certification program for Competitive Intelligence Professionals. Japan is currently the only country that officially maintains an economic intelligence agency (JETRO). It was founded by the Ministry of International Trade and Industry in 1958.
Accepting the importance of competitive intelligence, major multinational corporations, such as ExxonMobil, Procter & Gamble, and Johnson and Johnson, have created formal CI units.[citation needed] Importantly, organizations execute competitive intelligence activities not only as a safeguard to protect against market threats and changes, but also as a method for finding new opportunities and trends.[24]
Organizations use competitive intelligence to compare themselves to other organizations (""competitive benchmarking""), to identify risks and opportunities in their markets, and to pressure-test their plans against market response (business wargaming),[25] which enable them to make informed decisions. Most firms today realize the importance of knowing what their competitors are doing and how the industry is changing, and the information gathered allows organizations to understand their strengths and weaknesses.
One of the major activities involved in corporate competitive intelligence is use of ratio analysis, using key performance indicators (KPI). Organizations compare annual reports of their competitors on certain KPI and ratios, which are intrinsic to their industry. This helps them track their performance, vis-a-vis their competitors.
The actual importance of these categories of information to an organization depends on the contestability of its markets, the organizational culture, the personality and biases of its top decision makers, and the reporting structure of competitive intelligence within the company.
Strategic Intelligence (SI) focuses on the longer term, looking at issues affecting a company's competitiveness over the course of a couple of years. The actual time horizon for SI ultimately depends on the industry and how quickly it's changing. The general questions that SI answers are, ‘Where should we as a company be in X years?' and 'What are the strategic risks and opportunities facing us?' This type of intelligence work involves among others the identification of weak signals and application of methodology and process called Strategic Early Warning (SEW), first introduced by Gilad,[26][27][28] followed by Steven Shaker and Victor Richardson,[29] Alessandro Comai and Joaquin Tena,[30][31] and others. According to Gilad, 20% of the work of competitive intelligence practitioners should be dedicated to strategic early identification of weak signals within a SEW framework.
Tactical Intelligence: the focus is on providing information designed to improve shorter-term decisions, most often related with the intent of growing market share or revenues. Generally, it is the type of information that you would need to support the sales process in an organization. It investigates various aspects of a product/product line marketing:
With the right amount of information, organizations can avoid unpleasant surprises by anticipating competitors' moves and decreasing response time. Examples of competitive intelligence research is evident in daily newspapers, such as The Wall Street Journal, Business Week, and Fortune. Major airlines change hundreds of fares daily in response to competitors' tactics. They use information to plan their own marketing, pricing, and production strategies.
Resources, such as the Internet, have made gathering information on competitors easy. With a click of a button, analysts can discover future trends and market requirements. However competitive intelligence is much more than this, as the ultimate aim is to lead to competitive advantage. As the Internet is mostly public domain material, information gathered is less likely to result in insights that will be unique to the company. In fact there is a risk that information gathered from the Internet will be misinformation and mislead users, so competitive intelligence researchers are often wary of using such information.
As a result, although the Internet is viewed as a key source, most CI professionals should spend their time and budget gathering intelligence using primary research—networking with industry experts, from trade shows and conferences, from their own customers and suppliers, and so on. Where the Internet is used, it is to gather sources for primary research as well as information on what the company says about itself and its online presence (in the form of links to other companies, its strategy regarding search engines and online advertising, mentions in discussion forums and on blogs, etc.). Also, important are online subscription databases and news aggregation sources which have simplified the secondary source collection process. Social media sources are also becoming important—providing potential interviewee names, as well as opinions and attitudes, and sometimes breaking news (e.g., via Twitter).
Organizations must be careful not to spend too much time and effort on old competitors without realizing the existence of any new competitors. Knowing more about your competitors will allow your business to grow and succeed. The practice of competitive intelligence is growing every year, and most companies and business students now realize the importance of knowing their competitors.
The technical advances in massive parallel processing offered by the Hadoop ""big data"" architecture has allowed the creation of multiple platforms for named-entity recognition such as the Apache Projects OpenNLP and Apache Stanbol. The former includes pre-trained statistical parsers that can discern elements key to establishing trends and evaluating competitive position and responding appropriately.[32] Public information mining from SEC.gov, Federal Contract Awards, social media (Twitter, Reddit, Facebook, and others), vendors, and competitor websites now permit real-time counterintelligence as a strategy for horizontal and vertical market expansion and product positioning. This occurs in an automated fashion on massive marketplaces such as Amazon.com and their classification and prediction of product associations and purchase probability.
A new industry emerged of tech companies with tools that simplify and automate the way companies conduct competitive intelligence. With technology responsible for scraping billions of pieces of data and pulling it into a central platform, this new trend of competitive intelligence tools has effectively reshaped how competitor analysis is performed and intelligence gathered.[33]
Competitive intelligence has been influenced by national strategic intelligence. Although national intelligence was researched 50 years ago, competitive intelligence was introduced during the 1990s. Competitive-intelligence professionals can learn from national-intelligence experts, especially in the analysis of complex situations.[34] Competitive intelligence may be confused with (or seen to overlap) environmental scanning, business intelligence and market research.[35] Craig Fleisher[35] questions the appropriateness of the term, comparing it to business intelligence, competitor intelligence, knowledge management, market intelligence, marketing research and strategic intelligence.[36]
Fleisher[36][verification needed] suggests that business intelligence has two forms. Its narrow (contemporary) form is more focused on information technology and internal focus than CI, while its broader (historical) definition is more inclusive than CI. Knowledge management (KM), when improperly achieved, is seen as an information-technology driven organizational practice relying on data mining, corporate intranets and mapping organizational assets to make it accessible to organization members for decision-making. CI shares some aspects of KM; they are human-intelligence- and experience-based for a more-sophisticated qualitative analysis. km is essential for effective change. A key effective factor is a powerful, dedicated IT system executing the full intelligence cycle.[37]
Market intelligence (MI) is industry-targeted intelligence developed in real-time aspects of competitive events taking place among the four Ps of the marketing mix (pricing, place, promotion and product) in the product (or service) marketplace to better understand the market's attractiveness.[38] A time-based competitive tactic, MI is used by marketing and sales managers to respond to consumers more quickly in the marketplace. Fleisher suggests it is not distributed as widely as some forms of CI, which are also distributed to non-marketing decision-makers.[36][verification needed] Market intelligence has a shorter time horizon than other intelligence areas, and is measured in days, weeks, or (in slower-moving industries) months.
Market research is a tactical, method-driven field consisting of neutral, primary research of customer data (beliefs and perceptions) gathered in surveys or focus groups, and is analyzed with statistical-research techniques.[39] CI draws on a wider variety (primary and secondary) of sources from a wider range of stakeholders (suppliers, competitors, distributors, substitutes and media) to answer existing questions, raise new ones and guide action.[36][verification needed]
Ben Gilad and Jan Herring lay down a set of prerequisites defining CI, distinguishing it from other information-rich disciplines such as market research or business development. They show that a common body of knowledge and a unique set of tools (key intelligence topics, business war games and blindspots analysis) distinguish CI; while other sensory activities in a commercial firm focus on one segment of the market (customers, suppliers or acquisition targets), CI synthesizes data from all high-impact players (HIP).[21]
Gilad later focused his delineation of CI on the difference between information and intelligence. According to him, the common denominator among organizational sensory functions (whether they are called market research, business intelligence or market intelligence) is that they deliver information rather than intelligence. Intelligence, says Gilad, is a perspective on facts rather than the facts themselves. Unique among corporate functions, competitive intelligence has a perspective of risks and opportunities for a firm's performance; as such, it (not information activities) is part of an organization's risk-management activity.[40]
Ethics has been a long-held issue of discussion among CI practitioners.[35] The questions revolve around what is and is not allowable in terms of CI activity. A number of scholarly treatments have been generated on this topic, most prominently addressed through Society of Competitive Intelligence Professionals publications.[41] The book Competitive Intelligence Ethics: Navigating the Gray Zone provides nearly twenty separate views about ethics in CI, as well as another 10 codes used by various individuals or organizations.[41] Combining that with the over two dozen scholarly articles or studies found within the various CI bibliographic entries,[42][verification needed][9][10][43] it is clear that no shortage of study has gone into better classifying, understanding and addressing CI ethics.
Competitive information may be obtained from public or subscription sources, from networking with competitor staff or customers, disassembly of competitor products or from field research interviews. Competitive intelligence research is distinguishable from industrial espionage, as CI practitioners generally abide by local legal guidelines and ethical business norms.[44]
Outsourcing has become a big business for competitive intelligence professionals. There are many different companies in this field, including market research and consulting firms.[45]
"
1001971697,2021-01-22 05:38:17,"Competitive intelligence (CI) is the systematic collection and analysis of information from multiple sources, and a coordinated CI program.[1] It is the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers in strategic decision making for an organization.
CI means understanding and learning what is happening in the world outside the business to increase one's competitiveness. It means learning as much as possible, as soon as possible, about one's external environment including one's industry in general and relevant competitors.[2]
Key points:
Another definition of CI regards it as the organizational function responsible for the early identification of risks and opportunities in the market before they become obvious (""early signal analysis""). This definition focuses attention on the difference between dissemination of widely available factual information (such as market statistics, financial reports, newspaper clippings) performed by functions such as libraries and information centers, and competitive intelligence which is a perspective on developments and events aimed at yielding a competitive edge.[6]
The term CI is often viewed as synonymous with competitor analysis, but competitive intelligence is more than analyzing competitors; it embraces the entire environment and stakeholders: customers, competitors, distributors, technologies, and macroeconomic data. It is also a tool for decision making.
CI literature is best exemplified by the bibliographies that were published in the Society of Competitive Intelligence Professionals' academic journal The Journal of Competitive Intelligence and Management.[7][8][9][10] Although elements of organizational intelligence collection have been a part of business for many years, the history of competitive intelligence arguably began in the U.S. in the 1970s, although the literature on the field pre-dates this time by at least several decades.[10] In 1980, Michael Porter published the study Competitive-Strategy: Techniques for Analyzing Industries and Competitors which is widely viewed as the foundation of modern competitive intelligence. This has since been extended most notably by the pair of Craig Fleisher and Babette Bensoussan, who through several popular books on competitive analysis have added 48 commonly applied competitive intelligence analysis techniques to the practitioner's tool box.[11][12] In 1985, Leonard Fuld published his best selling book dedicated to competitor intelligence.[13] However, the institutionalization of CI as a formal activity among American corporations can be traced to 1988, when Ben and Tamar Gilad published the first organizational model of a formal corporate CI function, which was then adopted widely by US companies.[14] The first professional certification program (CIP) was created in 1996 with the establishment of The Fuld-Gilad-Herring Academy of Competitive Intelligence in Cambridge, Massachusetts.
In 1986 the Society of Competitive Intelligence Professionals (SCIP) was founded in the United States and grew in the late 1990s to around 6,000 members worldwide, mainly in the United States and Canada, but with large numbers especially in the UK and Australia. Due to financial difficulties in 2009, the organization merged with Frost & Sullivan under the Frost & Sullivan Institute. SCIP has since been renamed ""Strategic & Competitive Intelligence Professionals"" to emphasise the strategic nature of the subject, and also to refocus the organisation's general approach, while keeping the existing SCIP brandname and logo. A number of efforts have been made to discuss the field's advances in post-secondary (university) education, covered by several authors including Blenkhorn & Fleisher,[15] Fleisher,[16] Fuld,[17] Prescott,[18] and McGonagle.[19] Although the general view would be that competitive intelligence concepts can be readily found and taught in many business schools around the globe, there are still relatively few dedicated academic programs, majors, or degrees in the field, a concern to academics in the field who would like to see it further researched.[16] These issues were widely discussed by over a dozen knowledgeable individuals in a special edition of the Competitive Intelligence Magazine that was dedicated to this topic.[20] In France, a Specialized Master in Economic Intelligence and Knowledge Management was created in 1995 within the CERAM Business School, now SKEMA Business School, in Paris, with the objective of delivering a full and professional training in Economic Intelligence. A Centre for Global Intelligence and Influence was created in September 2011 in the same School.
On the other hand, practitioners and companies regard professional accreditation as especially important in this field.[21] In 2011, SCIP recognized the Fuld-Gilad-Herring Academy of Competitive Intelligence's CIP certification process as its global, dual-level (CIP-I and CIP-II) certification program.
Global developments have also been uneven in competitive intelligence.[22] Several academic journals, particularly the Journal of Competitive Intelligence and Management in its third volume, provided coverage of the field's global development.[23] For example, in 1997 the École de guerre économique [fr] (School of economic warfare) was founded in Paris, France. It is the first European institution which teaches the tactics of economic warfare within a globalizing world. In Germany, competitive intelligence was unattended until the early 1990s. The term ""competitive intelligence"" first appeared in German literature in 1997. In 1995 a German SCIP chapter was founded, which is now second in terms of membership in Europe. In summer 2004 the Institute for Competitive Intelligence was founded, which provides a postgraduate certification program for Competitive Intelligence Professionals. Japan is currently the only country that officially maintains an economic intelligence agency (JETRO). It was founded by the Ministry of International Trade and Industry in 1958.
Accepting the importance of competitive intelligence, major multinational corporations, such as ExxonMobil, Procter & Gamble, and Johnson and Johnson, have created formal CI units.[citation needed] Importantly, organizations execute competitive intelligence activities not only as a safeguard to protect against market threats and changes, but also as a method for finding new opportunities and trends.[24]
Organizations use competitive intelligence to compare themselves to other organizations (""competitive benchmarking""), to identify risks and opportunities in their markets, and to pressure-test their plans against market response (business wargaming),[25] which enable them to make informed decisions. Most firms today realize the importance of knowing what their competitors are doing and how the industry is changing, and the information gathered allows organizations to understand their strengths and weaknesses.
One of the major activities involved in corporate competitive intelligence is use of ratio analysis, using key performance indicators (KPI). Organizations compare annual reports of their competitors on certain KPI and ratios, which are intrinsic to their industry. This helps them track their performance, vis-a-vis their competitors.
The actual importance of these categories of information to an organization depends on the contestability of its markets, the organizational culture, the personality and biases of its top decision makers, and the reporting structure of competitive intelligence within the company.
Strategic Intelligence (SI) focuses on the longer term, looking at issues affecting a company's competitiveness over the course of a couple of years. The actual time horizon for SI ultimately depends on the industry and how quickly it's changing. The general questions that SI answers are, ‘Where should we as a company be in X years?' and 'What are the strategic risks and opportunities facing us?' This type of intelligence work involves among others the identification of weak signals and application of methodology and process called Strategic Early Warning (SEW), first introduced by Gilad,[26][27][28] followed by Steven Shaker and Victor Richardson,[29] Alessandro Comai and Joaquin Tena,[30][31] and others. According to Gilad, 20% of the work of competitive intelligence practitioners should be dedicated to strategic early identification of weak signals within a SEW framework.
Tactical Intelligence: the focus is on providing information designed to improve shorter-term decisions, most often related with the intent of growing market share or revenues. Generally, it is the type of information that you would need to support the sales process in an organization. It investigates various aspects of a product/product line marketing:
With the right amount of information, organizations can avoid unpleasant surprises by anticipating competitors' moves and decreasing response time. Examples of competitive intelligence research is evident in daily newspapers, such as The Wall Street Journal, Business Week, and Fortune. Major airlines change hundreds of fares daily in response to competitors' tactics. They use information to plan their own marketing, pricing, and production strategies.
Resources, such as the Internet, have made gathering information on competitors easy. With a click of a button, analysts can discover future trends and market requirements. However competitive intelligence is much more than this, as the ultimate aim is to lead to competitive advantage. As the Internet is mostly public domain material, information gathered is less likely to result in insights that will be unique to the company. In fact there is a risk that information gathered from the Internet will be misinformation and mislead users, so competitive intelligence researchers are often wary of using such information.
As a result, although the Internet is viewed as a key source, most CI professionals should spend their time and budget gathering intelligence using primary research—networking with industry experts, from trade shows and conferences, from their own customers and suppliers, and so on. Where the Internet is used, it is to gather sources for primary research as well as information on what the company says about itself and its online presence (in the form of links to other companies, its strategy regarding search engines and online advertising, mentions in discussion forums and on blogs, etc.). Also, important are online subscription databases and news aggregation sources which have simplified the secondary source collection process. Social media sources are also becoming important—providing potential interviewee names, as well as opinions and attitudes, and sometimes breaking news (e.g., via Twitter).
Organizations must be careful not to spend too much time and effort on old competitors without realizing the existence of any new competitors. Knowing more about your competitors will allow your business to grow and succeed. The practice of competitive intelligence is growing every year, and most companies and business students now realize the importance of knowing their competitors.
The technical advances in massive parallel processing offered by the Hadoop ""big data"" architecture has allowed the creation of multiple platforms for named-entity recognition such as the Apache Projects OpenNLP and Apache Stanbol. The former includes pre-trained statistical parsers that can discern elements key to establishing trends and evaluating competitive position and responding appropriately.[32] Public information mining from SEC.gov, Federal Contract Awards, social media (Twitter, Reddit, Facebook, and others), vendors, and competitor websites now permit real-time counterintelligence as a strategy for horizontal and vertical market expansion and product positioning. This occurs in an automated fashion on massive marketplaces such as Amazon.com and their classification and prediction of product associations and purchase probability.
A new industry emerged of tech companies with tools that simplify and automate the way companies conduct competitive intelligence. With technology responsible for scraping billions of pieces of data and pulling it into a central platform, this new trend of competitive intelligence tools has effectively reshaped how competitor analysis is performed and intelligence gathered.[33]
Competitive intelligence has been influenced by national strategic intelligence. Although national intelligence was researched 50 years ago, competitive intelligence was introduced during the 1990s. Competitive-intelligence professionals can learn from national-intelligence experts, especially in the analysis of complex situations.[34] Competitive intelligence may be confused with (or seen to overlap) environmental scanning, business intelligence and market research.[35] Craig Fleisher[35] questions the appropriateness of the term, comparing it to business intelligence, competitor intelligence, knowledge management, market intelligence, marketing research and strategic intelligence.[36]
Fleisher[36][verification needed] suggests that business intelligence has two forms. Its narrow (contemporary) form is more focused on information technology and internal focus than CI, while its broader (historical) definition is more inclusive than CI. Knowledge management (KM), when improperly achieved, is seen as an information-technology driven organizational practice relying on data mining, corporate intranets and mapping organizational assets to make it accessible to organization members for decision-making. CI shares some aspects of KM; they are human-intelligence- and experience-based for a more-sophisticated qualitative analysis. km is essential for effective change. A key effective factor is a powerful, dedicated IT system executing the full intelligence cycle.[37]
Market intelligence (MI) is industry-targeted intelligence developed in real-time aspects of competitive events taking place among the four Ps of the marketing mix (pricing, place, promotion and product) in the product (or service) marketplace to better understand the market's attractiveness.[38] A time-based competitive tactic, MI is used by marketing and sales managers to respond to consumers more quickly in the marketplace. Fleisher suggests it is not distributed as widely as some forms of CI, which are also distributed to non-marketing decision-makers.[36][verification needed] Market intelligence has a shorter time horizon than other intelligence areas, and is measured in days, weeks, or (in slower-moving industries) months.
Market research is a tactical, method-driven field consisting of neutral, primary research of customer data (beliefs and perceptions) gathered in surveys or focus groups, and is analyzed with statistical-research techniques.[39] CI draws on a wider variety (primary and secondary) of sources from a wider range of stakeholders (suppliers, competitors, distributors, substitutes and media) to answer existing questions, raise new ones and guide action.[36][verification needed]
Ben Gilad and Jan Herring lay down a set of prerequisites defining CI, distinguishing it from other information-rich disciplines such as market research or business development. They show that a common body of knowledge and a unique set of tools (key intelligence topics, business war games and blindspots analysis) distinguish CI; while other sensory activities in a commercial firm focus on one segment of the market (customers, suppliers or acquisition targets), CI synthesizes data from all high-impact players (HIP).[21]
Gilad later focused his delineation of CI on the difference between information and intelligence. According to him, the common denominator among organizational sensory functions (whether they are called market research, business intelligence or market intelligence) is that they deliver information rather than intelligence. Intelligence, says Gilad, is a perspective on facts rather than the facts themselves. Unique among corporate functions, competitive intelligence has a perspective of risks and opportunities for a firm's performance; as such, it (not information activities) is part of an organization's risk-management activity.[40]
Ethics has been a long-held issue of discussion among CI practitioners.[35] The questions revolve around what is and is not allowable in terms of CI activity. A number of scholarly treatments have been generated on this topic, most prominently addressed through Society of Competitive Intelligence Professionals publications.[41] The book Competitive Intelligence Ethics: Navigating the Gray Zone provides nearly twenty separate views about ethics in CI, as well as another 10 codes used by various individuals or organizations.[41] Combining that with the over two dozen scholarly articles or studies found within the various CI bibliographic entries,[42][verification needed][9][10][43] it is clear that no shortage of study has gone into better classifying, understanding and addressing CI ethics.
Competitive information may be obtained from public or subscription sources, from networking with competitor staff or customers, disassembly of competitor products or from field research interviews. Competitive intelligence research is distinguishable from industrial espionage, as CI practitioners generally abide by local legal guidelines and ethical business norms.[44]
Outsourcing has become a big business for competitive intelligence professionals. There are many different companies in this field, including market research and consulting firms.[45]
"
1024350367,2021-05-21 15:48:27,"Competitive intelligence (CI) is the systematic collection and analysis of information from multiple sources, and a coordinated CI program.[1] It is the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers in strategic decision making for an organization.
CI means understanding and learning what is happening in the world outside the business to increase one's competitiveness. It means learning as much as possible, as soon as possible, about one's external environment including one's industry in general and relevant competitors.[2]
Key points:
Another definition of CI regards it as the organizational function responsible for the early identification of risks and opportunities in the market before they become obvious (""early signal analysis""). This definition focuses attention on the difference between dissemination of widely available factual information (such as market statistics, financial reports, newspaper clippings) performed by functions such as libraries and information centers, and competitive intelligence which is a perspective on developments and events aimed at yielding a competitive edge.[6]
The term CI is often viewed as synonymous with competitor analysis, but competitive intelligence is more than analyzing competitors; it embraces the entire environment and stakeholders: customers, competitors, distributors, technologies, and macroeconomic data. It is also a tool for decision making.
CI literature is best exemplified by the bibliographies that were published in the Society of Competitive Intelligence Professionals' academic journal The Journal of Competitive Intelligence and Management.[7][8][9][10] Although elements of organizational intelligence collection have been a part of business for many years, the history of competitive intelligence arguably began in the U.S. in the 1970s, although the literature on the field pre-dates this time by at least several decades.[10] In 1980, Michael Porter published the study Competitive-Strategy: Techniques for Analyzing Industries and Competitors which is widely viewed as the foundation of modern competitive intelligence. This has since been extended most notably by the pair of Craig Fleisher and Babette Bensoussan, who through several popular books on competitive analysis have added 48 commonly applied competitive intelligence analysis techniques to the practitioner's tool box.[11][12] In 1985, Leonard Fuld published his best selling book dedicated to competitor intelligence.[13] However, the institutionalization of CI as a formal activity among American corporations can be traced to 1988, when Ben and Tamar Gilad published the first organizational model of a formal corporate CI function, which was then adopted widely by US companies.[14] The first professional certification program (CIP) was created in 1996 with the establishment of The Fuld-Gilad-Herring Academy of Competitive Intelligence in Cambridge, Massachusetts.
In 1986 the Society of Competitive Intelligence Professionals (SCIP) was founded in the United States and grew in the late 1990s to around 6,000 members worldwide, mainly in the United States and Canada, but with large numbers especially in the UK and Australia. Due to financial difficulties in 2009, the organization merged with Frost & Sullivan under the Frost & Sullivan Institute. SCIP has since been renamed ""Strategic & Competitive Intelligence Professionals"" to emphasise the strategic nature of the subject, and also to refocus the organisation's general approach, while keeping the existing SCIP brandname and logo. A number of efforts have been made to discuss the field's advances in post-secondary (university) education, covered by several authors including Blenkhorn & Fleisher,[15] Fleisher,[16] Fuld,[17] Prescott,[18] and McGonagle.[19] Although the general view would be that competitive intelligence concepts can be readily found and taught in many business schools around the globe, there are still relatively few dedicated academic programs, majors, or degrees in the field, a concern to academics in the field who would like to see it further researched.[16] These issues were widely discussed by over a dozen knowledgeable individuals in a special edition of the Competitive Intelligence Magazine that was dedicated to this topic.[20] In France, a Specialized Master in Economic Intelligence and Knowledge Management was created in 1995 within the CERAM Business School, now SKEMA Business School, in Paris, with the objective of delivering a full and professional training in Economic Intelligence. A Centre for Global Intelligence and Influence was created in September 2011 in the same School.
On the other hand, practitioners and companies regard professional accreditation as especially important in this field.[21] In 2011, SCIP recognized the Fuld-Gilad-Herring Academy of Competitive Intelligence's CIP certification process as its global, dual-level (CIP-I and CIP-II) certification program.
Global developments have also been uneven in competitive intelligence.[22] Several academic journals, particularly the Journal of Competitive Intelligence and Management in its third volume, provided coverage of the field's global development.[23] For example, in 1997 the École de guerre économique [fr] (School of economic warfare) was founded in Paris, France. It is the first European institution which teaches the tactics of economic warfare within a globalizing world. In Germany, competitive intelligence was unattended until the early 1990s. The term ""competitive intelligence"" first appeared in German literature in 1997. In 1995 a German SCIP chapter was founded, which is now second in terms of membership in Europe. In summer 2004 the Institute for Competitive Intelligence was founded, which provides a postgraduate certification program for Competitive Intelligence Professionals. Japan is currently the only country that officially maintains an economic intelligence agency (JETRO). It was founded by the Ministry of International Trade and Industry in 1958.
Accepting the importance of competitive intelligence, major multinational corporations, such as ExxonMobil, Procter & Gamble, and Johnson and Johnson, have created formal CI units.[citation needed] Importantly, organizations execute competitive intelligence activities not only as a safeguard to protect against market threats and changes, but also as a method for finding new opportunities and trends.[24]
Organizations use competitive intelligence to compare themselves to other organizations (""competitive benchmarking""), to identify risks and opportunities in their markets, and to pressure-test their plans against market response (business wargaming),[25] which enable them to make informed decisions. Most firms today realize the importance of knowing what their competitors are doing and how the industry is changing, and the information gathered allows organizations to understand their strengths and weaknesses.
One of the major activities involved in corporate competitive intelligence is use of ratio analysis, using key performance indicators (KPI). Organizations compare annual reports of their competitors on certain KPI and ratios, which are intrinsic to their industry. This helps them track their performance, vis-a-vis their competitors.
The actual importance of these categories of information to an organization depends on the contestability of its markets, the organizational culture, the personality and biases of its top decision makers, and the reporting structure of competitive intelligence within the company.
Strategic Intelligence (SI) focuses on the longer term, looking at issues affecting a company's competitiveness over the course of a couple of years. The actual time horizon for SI ultimately depends on the industry and how quickly it's changing. The general questions that SI answers are, ‘Where should we as a company be in X years?' and 'What are the strategic risks and opportunities facing us?' This type of intelligence work involves among others the identification of weak signals and application of methodology and process called Strategic Early Warning (SEW), first introduced by Gilad,[26][27][28] followed by Steven Shaker and Victor Richardson,[29] Alessandro Comai and Joaquin Tena,[30][31] and others. According to Gilad, 20% of the work of competitive intelligence practitioners should be dedicated to strategic early identification of weak signals within a SEW framework.
Tactical Intelligence: the focus is on providing information designed to improve shorter-term decisions, most often related with the intent of growing market share or revenues. Generally, it is the type of information that you would need to support the sales process in an organization. It investigates various aspects of a product/product line marketing:
With the right amount of information, organizations can avoid unpleasant surprises by anticipating competitors' moves and decreasing response time. Examples of competitive intelligence research is evident in daily newspapers, such as The Wall Street Journal, Business Week, and Fortune. Major airlines change hundreds of fares daily in response to competitors' tactics. They use information to plan their own marketing, pricing, and production strategies.
Resources, such as the Internet, have made gathering information on competitors easy. With a click of a button, analysts can discover future trends and market requirements. However competitive intelligence is much more than this, as the ultimate aim is to lead to competitive advantage. As the Internet is mostly public domain material, information gathered is less likely to result in insights that will be unique to the company. In fact there is a risk that information gathered from the Internet will be misinformation and mislead users, so competitive intelligence researchers are often wary of using such information.
As a result, although the Internet is viewed as a key source, most CI professionals should spend their time and budget gathering intelligence using primary research—networking with industry experts, from trade shows and conferences, from their own customers and suppliers, and so on. Where the Internet is used, it is to gather sources for primary research as well as information on what the company says about itself and its online presence (in the form of links to other companies, its strategy regarding search engines and online advertising, mentions in discussion forums and on blogs, etc.). Also, important are online subscription databases and news aggregation sources which have simplified the secondary source collection process. Social media sources are also becoming important—providing potential interviewee names, as well as opinions and attitudes, and sometimes breaking news (e.g., via Twitter).
Organizations must be careful not to spend too much time and effort on old competitors without realizing the existence of any new competitors. Knowing more about your competitors will allow your business to grow and succeed. The practice of competitive intelligence is growing every year, and most companies and business students now realize the importance of knowing their competitors.
The technical advances in massive parallel processing offered by the Hadoop ""big data"" architecture has allowed the creation of multiple platforms for named-entity recognition such as the Apache Projects OpenNLP and Apache Stanbol. The former includes pre-trained statistical parsers that can discern elements key to establishing trends and evaluating competitive position and responding appropriately.[32] Public information mining from SEC.gov, Federal Contract Awards, social media (Twitter, Reddit, Facebook, and others), vendors, and competitor websites now permit real-time counterintelligence as a strategy for horizontal and vertical market expansion and product positioning. This occurs in an automated fashion on massive marketplaces such as Amazon.com and their classification and prediction of product associations and purchase probability.
A new industry emerged of tech companies with tools that simplify and automate the way companies conduct competitive intelligence. With technology responsible for scraping billions of pieces of data and pulling it into a central platform, this new trend of competitive intelligence tools has effectively reshaped how competitor analysis is performed and intelligence gathered.[33]
Competitive intelligence has been influenced by national strategic intelligence. Although national intelligence was researched 50 years ago, competitive intelligence was introduced during the 1990s. Competitive-intelligence professionals can learn from national-intelligence experts, especially in the analysis of complex situations.[34] Competitive intelligence may be confused with (or seen to overlap) environmental scanning, business intelligence and market research.[35] Craig Fleisher[35] questions the appropriateness of the term, comparing it to business intelligence, competitor intelligence, knowledge management, market intelligence, marketing research and strategic intelligence.[36]
Fleisher[36][verification needed] suggests that business intelligence has two forms. Its narrow (contemporary) form is more focused on information technology and internal focus than CI, while its broader (historical) definition is more inclusive than CI. Knowledge management (KM), when improperly achieved, is seen as an information-technology driven organizational practice relying on data mining, corporate intranets and mapping organizational assets to make it accessible to organization members for decision-making. CI shares some aspects of KM; they are human-intelligence- and experience-based for a more-sophisticated qualitative analysis. km is essential for effective change. A key effective factor is a powerful, dedicated IT system executing the full intelligence cycle.[37]
Market intelligence (MI) is industry-targeted intelligence developed in real-time aspects of competitive events taking place among the four Ps of the marketing mix (pricing, place, promotion and product) in the product (or service) marketplace to better understand the market's attractiveness.[38] A time-based competitive tactic, MI is used by marketing and sales managers to respond to consumers more quickly in the marketplace. Fleisher suggests it is not distributed as widely as some forms of CI, which are also distributed to non-marketing decision-makers.[36][verification needed] Market intelligence has a shorter time horizon than other intelligence areas, and is measured in days, weeks, or (in slower-moving industries) months.
Market research is a tactical, method-driven field consisting of neutral, primary research of customer data (beliefs and perceptions) gathered in surveys or focus groups, and is analyzed with statistical-research techniques.[39] CI draws on a wider variety (primary and secondary) of sources from a wider range of stakeholders (suppliers, competitors, distributors, substitutes and media) to answer existing questions, raise new ones and guide action.[36][verification needed]
Ben Gilad and Jan Herring lay down a set of prerequisites defining CI, distinguishing it from other information-rich disciplines such as market research or business development. They show that a common body of knowledge and a unique set of tools (key intelligence topics, business war games and blindspots analysis) distinguish CI; while other sensory activities in a commercial firm focus on one segment of the market (customers, suppliers or acquisition targets), CI synthesizes data from all high-impact players (HIP).[21]
Gilad later focused his delineation of CI on the difference between information and intelligence. According to him, the common denominator among organizational sensory functions (whether they are called market research, business intelligence or market intelligence) is that they deliver information rather than intelligence. Intelligence, says Gilad, is a perspective on facts rather than the facts themselves. Unique among corporate functions, competitive intelligence has a perspective of risks and opportunities for a firm's performance; as such, it (not information activities) is part of an organization's risk-management activity.[40]
Ethics has been a long-held issue of discussion among CI practitioners.[35] The questions revolve around what is and is not allowable in terms of CI activity. A number of scholarly treatments have been generated on this topic, most prominently addressed through Society of Competitive Intelligence Professionals publications.[41] The book Competitive Intelligence Ethics: Navigating the Gray Zone provides nearly twenty separate views about ethics in CI, as well as another 10 codes used by various individuals or organizations.[41] Combining that with the over two dozen scholarly articles or studies found within the various CI bibliographic entries,[42][verification needed][9][10][43] it is clear that no shortage of study has gone into better classifying, understanding and addressing CI ethics.
Competitive information may be obtained from public or subscription sources, from networking with competitor staff or customers, disassembly of competitor products or from field research interviews. Competitive intelligence research is distinguishable from industrial espionage, as CI practitioners generally abide by local legal guidelines and ethical business norms.[44]
Outsourcing has become a big business for competitive intelligence professionals. There are many different companies in this field, including market research and consulting firms.[45]
"
1030196265,2021-06-24 13:29:51,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems[1].[2] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[2][3][4][5] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[6] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[2] 
The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[7] It also is a buzzword[8] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[9] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[10] Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[11]
The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[12][13] Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[14] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[15] Currently, the terms data mining and knowledge discovery are used interchangeably.
In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDD Newsletter SIGKDD Explorations.[16] The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.
The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[17] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.
The knowledge discovery in databases (KDD) process is commonly defined with the stages:
It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:
or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results in Validation.
Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[18] The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[19] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[20]
In 2020, a new scientific data mining process that is called MeKDDaM [21]. Unlike, CRISP-DM, which was designed originally for business applications, MeKDDaM is designed to support traceability, justifiability, and reproducibility which are essential aspects for scientific applications.
Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.
Data mining involves six common classes of tasks:[6]
Data mining can unintentionally be misused, and can then produce results that appear to be significant; but which do not actually predict future behavior and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.[22]
The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by data mining algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish ""spam"" from ""legitimate"" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.
If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.
The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[23][24] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[25] and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".[26]
Computer science conferences on data mining include:
Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases
There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[27]
Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.
While the term ""data mining"" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[28]
The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[29] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[30][31]
Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[32] This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[33][34][35]
It is recommended[according to whom?] to be aware of the following before data are collected:[32]
Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[32] However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[36]
The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.[37]
Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[38]
In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[39]
In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their ""informed consent"" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, ""'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.""[40] This underscores the necessity for data anonymity in data aggregation and mining practices.
U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.
Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[41] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. 
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[42] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[43]
US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.[44]
The following applications are available under free/open-source licenses. Public access to application source code is also available.
The following applications are available under proprietary licenses.
For more information about extracting information out of data (as opposed to analyzing data) , see:
"
1030210119,2021-06-24 15:16:03,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems[1].[2] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[2][3][4][5] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[6] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[2] 
The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[7] It also is a buzzword[8] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[9] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[10] Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[11]
The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[12][13] Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[14] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[15] Currently, the terms data mining and knowledge discovery are used interchangeably.
In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDD Newsletter SIGKDD Explorations.[16] The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.
The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[17] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.
The knowledge discovery in databases (KDD) process is commonly defined with the stages:
It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:
or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results in Validation.
Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[18] The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[19] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[20]
In 2020, a scientific data mining process model that is called MeKDDaM was publish [21]. Unlike, CRISP-DM, which was originally designed for business applications, MeKDDaM was designed to be consistent with both deductive and inductive cycle of knowledge and to support traceability, justifiability, and reproducibility of results which are essential for scientific data mining.
Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.
Data mining involves six common classes of tasks:[6]
Data mining can unintentionally be misused, and can then produce results that appear to be significant; but which do not actually predict future behavior and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.[22]
The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by data mining algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish ""spam"" from ""legitimate"" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.
If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.
The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[23][24] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[25] and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".[26]
Computer science conferences on data mining include:
Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases
There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[27]
Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.
While the term ""data mining"" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[28]
The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[29] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[30][31]
Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[32] This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[33][34][35]
It is recommended[according to whom?] to be aware of the following before data are collected:[32]
Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[32] However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[36]
The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.[37]
Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[38]
In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[39]
In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their ""informed consent"" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, ""'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.""[40] This underscores the necessity for data anonymity in data aggregation and mining practices.
U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.
Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[41] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. 
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[42] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[43]
US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.[44]
The following applications are available under free/open-source licenses. Public access to application source code is also available.
The following applications are available under proprietary licenses.
For more information about extracting information out of data (as opposed to analyzing data) , see:
"
1030210317,2021-06-24 15:17:49,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems[1].[2] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[2][3][4][5] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[6] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[2] 
The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[7] It also is a buzzword[8] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[9] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[10] Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[11]
The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[12][13] Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[14] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[15] Currently, the terms data mining and knowledge discovery are used interchangeably.
In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDD Newsletter SIGKDD Explorations.[16] The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.
The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[17] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.
The knowledge discovery in databases (KDD) process is commonly defined with the stages:
It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:
or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results in Validation.
Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[18] The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[19] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[20]
In 2020, a scientific data mining process model that is called MeKDDaM was publish [21]. Unlike, CRISP-DM, which was originally designed for business applications, MeKDDaM was designed to be consistent with the deductive and inductive cycle of knowledge and also to support traceability, justifiability, and reproducibility which are essential for scientific data mining.
Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.
Data mining involves six common classes of tasks:[6]
Data mining can unintentionally be misused, and can then produce results that appear to be significant; but which do not actually predict future behavior and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.[22]
The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by data mining algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish ""spam"" from ""legitimate"" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.
If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.
The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[23][24] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[25] and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".[26]
Computer science conferences on data mining include:
Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases
There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[27]
Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.
While the term ""data mining"" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[28]
The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[29] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[30][31]
Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[32] This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[33][34][35]
It is recommended[according to whom?] to be aware of the following before data are collected:[32]
Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[32] However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[36]
The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.[37]
Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[38]
In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[39]
In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their ""informed consent"" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, ""'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.""[40] This underscores the necessity for data anonymity in data aggregation and mining practices.
U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.
Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[41] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. 
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[42] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[43]
US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.[44]
The following applications are available under free/open-source licenses. Public access to application source code is also available.
The following applications are available under proprietary licenses.
For more information about extracting information out of data (as opposed to analyzing data) , see:
"
1030228934,2021-06-24 17:33:19,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems[1].[2] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[2][3][4][5] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[6] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[2] 
The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[7] It also is a buzzword[8] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[9] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[10] Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[11]
The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[12][13] Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[14] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[15] Currently, the terms data mining and knowledge discovery are used interchangeably.
In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDD Newsletter SIGKDD Explorations.[16] The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.
The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[17] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.
The knowledge discovery in databases (KDD) process is commonly defined with the stages:
It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:
or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results in Validation.
Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[18] The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[19] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[20]
In 2020, a scientific data mining process model that is called MeKDDaM was published [21]. Unlike, CRISP-DM, which was originally designed for business applications, MeKDDaM was designed to be consistent with the deductive and inductive cycle of knowledge and also to support traceability, justifiability, and reproducibility which are essential features in scientific data mining.
Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.
Data mining involves six common classes of tasks:[6]
Data mining can unintentionally be misused, and can then produce results that appear to be significant; but which do not actually predict future behavior and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.[22]
The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by data mining algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish ""spam"" from ""legitimate"" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.
If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.
The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[23][24] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[25] and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".[26]
Computer science conferences on data mining include:
Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases
There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[27]
Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.
While the term ""data mining"" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[28]
The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[29] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[30][31]
Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[32] This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[33][34][35]
It is recommended[according to whom?] to be aware of the following before data are collected:[32]
Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[32] However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[36]
The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.[37]
Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[38]
In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[39]
In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their ""informed consent"" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, ""'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.""[40] This underscores the necessity for data anonymity in data aggregation and mining practices.
U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.
Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[41] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. 
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[42] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[43]
US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.[44]
The following applications are available under free/open-source licenses. Public access to application source code is also available.
The following applications are available under proprietary licenses.
For more information about extracting information out of data (as opposed to analyzing data) , see:
"
1030259444,2021-06-24 21:09:16,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems[1].[2] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[2][3][4][5] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[6] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[2] 
The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[7] It also is a buzzword[8] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[9] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[10] Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[11]
The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[12][13] Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[14] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[15] Currently, the terms data mining and knowledge discovery are used interchangeably.
In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDD Newsletter SIGKDD Explorations.[16] The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.
The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[17] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.
The knowledge discovery in databases (KDD) process is commonly defined with the stages:
It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:
or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results in Validation.
Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[18] The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[19] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[20]
In 2020, a scientific data mining process (MeKDDaM) was published [21]. Unlike, CRISP-DM, which was originally designed for business applications, MeKDDaM was designed to be consistent with the scientific methodology and to be applied to data-driven and hypothesis-driven scientific applications.
Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.
Data mining involves six common classes of tasks:[6]
Data mining can unintentionally be misused, and can then produce results that appear to be significant; but which do not actually predict future behavior and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.[22]
The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by data mining algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish ""spam"" from ""legitimate"" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.
If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.
The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[23][24] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[25] and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".[26]
Computer science conferences on data mining include:
Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases
There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[27]
Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.
While the term ""data mining"" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[28]
The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[29] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[30][31]
Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[32] This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[33][34][35]
It is recommended[according to whom?] to be aware of the following before data are collected:[32]
Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[32] However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[36]
The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.[37]
Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[38]
In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[39]
In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their ""informed consent"" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, ""'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.""[40] This underscores the necessity for data anonymity in data aggregation and mining practices.
U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.
Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[41] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. 
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[42] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[43]
US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.[44]
The following applications are available under free/open-source licenses. Public access to application source code is also available.
The following applications are available under proprietary licenses.
For more information about extracting information out of data (as opposed to analyzing data) , see:
"
1030948996,2021-06-28 23:05:57,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] 
The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[8] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[9] Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[10]
The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[11][12] Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[13] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[14] Currently, the terms data mining and knowledge discovery are used interchangeably.
In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDD Newsletter SIGKDD Explorations.[15] The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.
The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[16] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.
The knowledge discovery in databases (KDD) process is commonly defined with the stages:
It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:
or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results in Validation.
Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[17] The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[18] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[19]
In 2020, a scientific data mining process (MeKDDaM) was published [20]. Unlike, CRISP-DM, which was originally designed for business applications, MeKDDaM was designed to be consistent with the scientific methodology and to be applied to data-driven and hypothesis-driven scientific applications.
Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.
Data mining involves six common classes of tasks:[5]
Data mining can unintentionally be misused, and can then produce results that appear to be significant; but which do not actually predict future behavior and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.[21]
The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by data mining algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish ""spam"" from ""legitimate"" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.
If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.
The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[22][23] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[24] and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".[25]
Computer science conferences on data mining include:
Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases
There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[26]
Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.
While the term ""data mining"" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[27]
The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[28] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[29][30]
Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[31] This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[32][33][34]
It is recommended[according to whom?] to be aware of the following before data are collected:[31]
Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[31] However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[35]
The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.[36]
Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[37]
In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[38]
In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their ""informed consent"" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, ""'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.""[39] This underscores the necessity for data anonymity in data aggregation and mining practices.
U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.
Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[40] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. 
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[41] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[42]
US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.[43]
The following applications are available under free/open-source licenses. Public access to application source code is also available.
The following applications are available under proprietary licenses.
For more information about extracting information out of data (as opposed to analyzing data) , see:
"
1030949136,2021-06-28 23:07:03,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] 
The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[8] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[9] Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[10]
The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[11][12] Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[13] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[14] Currently, the terms data mining and knowledge discovery are used interchangeably.
In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDD Newsletter SIGKDD Explorations.[15] The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.
The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[16] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.
The knowledge discovery in databases (KDD) process is commonly defined with the stages:
It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:
or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results in Validation.
Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[17] The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[18] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[19]
Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.
Data mining involves six common classes of tasks:[5]
Data mining can unintentionally be misused, and can then produce results that appear to be significant; but which do not actually predict future behavior and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.[20]
The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by data mining algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish ""spam"" from ""legitimate"" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.
If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.
The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[21][22] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[23] and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".[24]
Computer science conferences on data mining include:
Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases
There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[25]
Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.
While the term ""data mining"" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[26]
The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[27] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[28][29]
Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[30] This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[31][32][33]
It is recommended[according to whom?] to be aware of the following before data are collected:[30]
Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[30] However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[34]
The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.[35]
Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[36]
In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[37]
In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their ""informed consent"" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, ""'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.""[38] This underscores the necessity for data anonymity in data aggregation and mining practices.
U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.
Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[39] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. 
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[40] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[41]
US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.[42]
The following applications are available under free/open-source licenses. Public access to application source code is also available.
The following applications are available under proprietary licenses.
For more information about extracting information out of data (as opposed to analyzing data) , see:
"
1030950024,2021-06-28 23:15:12,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] 
The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[8] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[9] Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[10]
The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[11][12] Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[13] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[14] Currently, the terms data mining and knowledge discovery are used interchangeably.
In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDD Newsletter SIGKDD Explorations.[15] The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.
The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[16] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.
The knowledge discovery in databases (KDD) process is commonly defined with the stages:
It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:
or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation.
Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[17] The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[18] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[19]
Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.
Data mining involves six common classes of tasks:[5]
Data mining can unintentionally be misused, and can then produce results that appear to be significant; but which do not actually predict future behavior and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.[20]
The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by data mining algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish ""spam"" from ""legitimate"" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.
If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.
The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[21][22] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[23] and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".[24]
Computer science conferences on data mining include:
Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases
There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[25]
Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.
While the term ""data mining"" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[26]
The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[27] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[28][29]
Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[30] This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[31][32][33]
It is recommended[according to whom?] to be aware of the following before data are collected:[30]
Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[30] However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[34]
The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.[35]
Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[36]
In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[37]
In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their ""informed consent"" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, ""'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.""[38] This underscores the necessity for data anonymity in data aggregation and mining practices.
U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.
Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[39] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. 
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[40] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[41]
US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.[42]
The following applications are available under free/open-source licenses. Public access to application source code is also available.
The following applications are available under proprietary licenses.
For more information about extracting information out of data (as opposed to analyzing data) , see:
"
1031176840,2021-06-30 05:55:23,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] 
The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[8] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[9] Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[10]
The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[11][12] Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[13] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[14] Currently, the terms data mining and knowledge discovery are used interchangeably.
In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDD Newsletter SIGKDD Explorations.[15] The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.
The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[16] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.
The knowledge discovery in databases (KDD) process is commonly defined with the stages:
It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:
or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation.
Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[17] The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[18] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[19]
A scientific data mining process (MeKDDaM) was published in 2020 [20]. Unlike, CRISP-DM, which was originally designed for business applications, MeKDDaM was designed for scientific applications.
Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.
Data mining involves six common classes of tasks:[5]
Data mining can unintentionally be misused, and can then produce results that appear to be significant; but which do not actually predict future behavior and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.[21]
The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by data mining algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish ""spam"" from ""legitimate"" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.
If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.
The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[22][23] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[24] and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".[25]
Computer science conferences on data mining include:
Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases
There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[26]
Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.
While the term ""data mining"" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[27]
The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[28] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[29][30]
Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[31] This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[32][33][34]
It is recommended[according to whom?] to be aware of the following before data are collected:[31]
Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[31] However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[35]
The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.[36]
Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[37]
In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[38]
In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their ""informed consent"" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, ""'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.""[39] This underscores the necessity for data anonymity in data aggregation and mining practices.
U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.
Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[40] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. 
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[41] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[42]
US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.[43]
The following applications are available under free/open-source licenses. Public access to application source code is also available.
The following applications are available under proprietary licenses.
For more information about extracting information out of data (as opposed to analyzing data) , see:
"
1031203808,2021-06-30 09:47:00,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] 
The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[8] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[9] Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[10]
The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[11][12] Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[13] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[14] Currently, the terms data mining and knowledge discovery are used interchangeably.
In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDD Newsletter SIGKDD Explorations.[15] The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.
The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[16] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.
The knowledge discovery in databases (KDD) process is commonly defined with the stages:
It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:
or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation.
Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[17] The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[18] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[19]
A scientific data mining process (MeKDDaM) was published in 2020 [20]. Unlike, CRISP-DM, which was originally designed for business applications, MeKDDaM was designed for scientific applications.
Before using data mining algorithms, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.
Data mining involves six common classes of tasks:[5]
Data mining can unintentionally be misused, and can then produce results that appear to be significant; but which do not actually predict future behavior and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.[21]
The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by data mining algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish ""spam"" from ""legitimate"" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.
If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.
The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[22][23] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[24] and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".[25]
Computer science conferences on data mining include:
Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases
There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[26]
Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.
While the term ""data mining"" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[27]
The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[28] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[29][30]
Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[31] This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[32][33][34]
It is recommended[according to whom?] to be aware of the following before data are collected:[31]
Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[31] However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[35]
The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.[36]
Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[37]
In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[38]
In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their ""informed consent"" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, ""'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.""[39] This underscores the necessity for data anonymity in data aggregation and mining practices.
U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.
Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[40] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. 
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[41] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[42]
US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.[43]
The following applications are available under free/open-source licenses. Public access to application source code is also available.
The following applications are available under proprietary licenses.
For more information about extracting information out of data (as opposed to analyzing data) , see:
"
1031203902,2021-06-30 09:48:00,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] 
The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[8] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[9] Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[10]
The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[11][12] Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[13] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[14] Currently, the terms data mining and knowledge discovery are used interchangeably.
In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDD Newsletter SIGKDD Explorations.[15] The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.
The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[16] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.
The knowledge discovery in databases (KDD) process is commonly defined with the stages:
It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:
or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation.
Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[17] The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[18] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[19]
A scientific data mining process (MeKDDaM) was published in 2020 [20]. Unlike, CRISP-DM, which was originally designed for business applications, MeKDDaM was designed for scientific applications.
Before using data mining algorithms, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.
Data mining involves six common classes of tasks:[5]
Data mining can unintentionally be misused, and can then produce results that appear to be significant; but which do not actually predict future behavior and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.[21]
The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by data mining algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish ""spam"" from ""legitimate"" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.
If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.
The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[22][23] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[24] and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".[25]
Computer science conferences on data mining include:
Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases
There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[26]
Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.
While the term ""data mining"" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[27]
The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[28] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[29][30]
Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[31] This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[32][33][34]
It is recommended[according to whom?] to be aware of the following before data are collected:[31]
Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[31] However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[35]
The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.[36]
Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[37]
In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[38]
In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their ""informed consent"" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, ""'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.""[39] This underscores the necessity for data anonymity in data aggregation and mining practices.
U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.
Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[40] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. 
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[41] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[42]
US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.[43]
The following applications are available under free/open-source licenses. Public access to application source code is also available.
The following applications are available under proprietary licenses.
For more information about extracting information out of data (as opposed to analyzing data) , see:
"
1031203969,2021-06-30 09:48:41,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] 
The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[8] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[9] Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[10]
The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[11][12] Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[13] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[14] Currently, the terms data mining and knowledge discovery are used interchangeably.
In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDD Newsletter SIGKDD Explorations.[15] The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.
The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[16] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.
The knowledge discovery in databases (KDD) process is commonly defined with the stages:
It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:
or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation.
Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[17] The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[18] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[19]
A scientific data mining process (MeKDDaM) was published in 2020 [20]. Unlike, CRISP-DM, which was originally designed for business applications, MeKDDaM was designed for scientific applications.
Before using data mining algorithms, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.
Data mining involves six common classes of tasks:[5]
Data mining can unintentionally be misused, and can then produce results that appear to be significant; but which do not actually predict future behavior and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.[21]
The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by data mining algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish ""spam"" from ""legitimate"" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.
If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.
The Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[22][23] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[24] and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".[25]
Computer science conferences on data mining include:
Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases
There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[26]
Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.
While the term ""data mining"" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[27]
The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[28] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[29][30]
Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[31] This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[32][33][34]
It is recommended[according to whom?] to be aware of the following before data are collected:[31]
Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[31] However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[35]
The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.[36]
Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[37]
In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[38]
In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their ""informed consent"" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, ""'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.""[39] This underscores the necessity for data anonymity in data aggregation and mining practices.
U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.
Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[40] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. 
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[41] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[42]
US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.[43]
The following applications are available under free/open-source licenses. Public access to application source code is also available.
The following applications are available under proprietary licenses.
For more information about extracting information out of data (as opposed to analyzing data) , see:
"
1031204106,2021-06-30 09:49:54,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] 
The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[8] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[9] Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[10]
The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[11][12] Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[13] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[14] Currently, the terms data mining and knowledge discovery are used interchangeably.
In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDD Newsletter SIGKDD Explorations.[15] The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.
The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[16] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.
The knowledge discovery in databases (KDD) process is commonly defined with the stages:
It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:
or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation.
Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[17] The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[18] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[19]
A scientific data mining process (MeKDDaM) was published in 2020 [20]. Unlike, CRISP-DM, which was originally designed for business applications, MeKDDaM was designed for scientific applications.
Before using data mining algorithms, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.
Data mining involves six common classes of tasks:[5]
Data mining can unintentionally be misused, and can then produce results that appear to be significant; but which do not actually predict future behavior and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.[21]
The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by data mining algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish ""spam"" from ""legitimate"" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.
If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.
The Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[22][23] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[24] and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".[25]
Computer science conferences on data mining include:
Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases
There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[26]
Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.
While the term ""data mining"" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[27]
Data mining poses concerns regarding issues including privacy, legality, and ethics.[28] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[29][30]
Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[31] This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[32][33][34]
It is recommended[according to whom?] to be aware of the following before data are collected:[31]
Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[31] However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[35]
The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.[36]
Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[37]
In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[38]
In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their ""informed consent"" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, ""'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.""[39] This underscores the necessity for data anonymity in data aggregation and mining practices.
U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.
Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[40] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. 
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[41] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[42]
US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.[43]
The following applications are available under free/open-source licenses. Public access to application source code is also available.
The following applications are available under proprietary licenses.
For more information about extracting information out of data (as opposed to analyzing data) , see:
"
1031211876,2021-06-30 10:54:32,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] 
The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[8] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[9] Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[10]
The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[11][12] Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[13] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[14] Currently, the terms data mining and knowledge discovery are used interchangeably.
In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDD Newsletter SIGKDD Explorations.[15] The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.
The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[16] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.
The knowledge discovery in databases (KDD) process is commonly defined with the stages:
It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:
or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation.
Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[17] The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[18] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[19]
Before using data mining algorithms, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.
Data mining involves six common classes of tasks:[5]
Data mining can unintentionally be misused, and can then produce results that appear to be significant; but which do not actually predict future behavior and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.[20]
The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by data mining algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish ""spam"" from ""legitimate"" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.
If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.
The Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[21][22] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[23] and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".[24]
Computer science conferences on data mining include:
Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases
There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[25]
Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.
While the term ""data mining"" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[26]
Data mining poses concerns regarding issues including privacy, legality, and ethics.[27] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[28][29]
Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[30] This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[31][32][33]
It is recommended[according to whom?] to be aware of the following before data are collected:[30]
Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[30] However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[34]
The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.[35]
Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[36]
In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[37]
In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their ""informed consent"" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, ""'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.""[38] This underscores the necessity for data anonymity in data aggregation and mining practices.
U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.
Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[39] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. 
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[40] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[41]
US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.[42]
The following applications are available under free/open-source licenses. Public access to application source code is also available.
The following applications are available under proprietary licenses.
For more information about extracting information out of data (as opposed to analyzing data) , see:
"
1031490826,2021-07-01 22:52:13,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] 
The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[8] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[9] Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[10]
The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[11][12] Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[13] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[14] Currently, the terms data mining and knowledge discovery are used interchangeably.
In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDD Newsletter SIGKDD Explorations.[15] The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.
The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[16] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.
The knowledge discovery in databases (KDD) process is commonly defined with the stages:
It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:
or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results in Validation.
Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[17] The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[18] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[19]
In 2020, a new scientific data mining process that is called MeKDDaM [20]. Unlike, CRISP-DM, which was designed originally for business applications, MeKDDaM is designed to support traceability, justifiability, and reproducibility which are essential aspects for scientific applications.
Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.
Data mining involves six common classes of tasks:[5]
Data mining can unintentionally be misused, and can then produce results that appear to be significant; but which do not actually predict future behavior and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.[21]
The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by data mining algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish ""spam"" from ""legitimate"" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.
If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.
The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[22][23] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[24] and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".[25]
Computer science conferences on data mining include:
Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases
There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[26]
Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.
While the term ""data mining"" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[27]
The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[28] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[29][30]
Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[31] This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[32][33][34]
It is recommended[according to whom?] to be aware of the following before data are collected:[31]
Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[31] However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[35]
The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.[36]
Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[37]
In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[38]
In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their ""informed consent"" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, ""'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.""[39] This underscores the necessity for data anonymity in data aggregation and mining practices.
U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.
Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[40] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. 
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[41] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[42]
US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.[43]
The following applications are available under free/open-source licenses. Public access to application source code is also available.
The following applications are available under proprietary licenses.
For more information about extracting information out of data (as opposed to analyzing data) , see:
"
1031559165,2021-07-02 08:31:17,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] 
The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[8] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[9] Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[10]
The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[11][12] Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[13] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[14] Currently, the terms data mining and knowledge discovery are used interchangeably.
In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDD Newsletter SIGKDD Explorations.[15] The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.
The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[16] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.
The knowledge discovery in databases (KDD) process is commonly defined with the stages:
It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:
or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation.
Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[17] The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[18] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[19]
Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.
Data mining involves six common classes of tasks:[5]
Data mining can unintentionally be misused, and can then produce results that appear to be significant; but which do not actually predict future behavior and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.[20]
The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by data mining algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish ""spam"" from ""legitimate"" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.
If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.
The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[21][22] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[23] and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".[24]
Computer science conferences on data mining include:
Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases
There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[25]
Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.
While the term ""data mining"" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[26]
The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[27] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[28][29]
Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[30] This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[31][32][33]
It is recommended[according to whom?] to be aware of the following before data are collected:[30]
Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[30] However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[34]
The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.[35]
Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[36]
In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[37]
In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their ""informed consent"" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, ""'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.""[38] This underscores the necessity for data anonymity in data aggregation and mining practices.
U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.
Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[39] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. 
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[40] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[41]
US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.[42]
The following applications are available under free/open-source licenses. Public access to application source code is also available.
The following applications are available under proprietary licenses.
For more information about extracting information out of data (as opposed to analyzing data) , see:
"
1031559617,2021-07-02 08:35:25,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] 
The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[8] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[9] Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[10]
The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[11][12] Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[13] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[14] Currently, the terms data mining and knowledge discovery are used interchangeably.
In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDD Newsletter SIGKDD Explorations.[15] The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.
The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[16] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.
The knowledge discovery in databases (KDD) process is commonly defined with the stages:
It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:
or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation.
Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[17] The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[18] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[19]
Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.
Data mining involves six common classes of tasks:[5]
Data mining can unintentionally be misused, and can then produce results that appear to be significant; but which do not actually predict future behavior and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.[20]
The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by data mining algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish ""spam"" from ""legitimate"" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.
If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.
The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[21][22] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[23] and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".[24]
Computer science conferences on data mining include:
Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases
There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[25]
Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.
While the term ""data mining"" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[26]
The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[27] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[28][29]
Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[30] This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[31][32][33]
It is recommended[according to whom?] to be aware of the following before data are collected:[30]
Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[30] However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[34]
The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.[35]
Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[36]
In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[37]
In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their ""informed consent"" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, ""'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.""[38] This underscores the necessity for data anonymity in data aggregation and mining practices.
U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.
Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[39] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. 
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[40] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[41]
US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.[42]
The following applications are available under free/open-source licenses. Public access to application source code is also available.
The following applications are available under proprietary licenses.
For more information about extracting information out of data (as opposed to analyzing data) , see:
"
1026066166,2021-05-31 05:41:24,"Market intelligence (MI) is the information relevant to a company's market - trends, competitor and customer (existing, lost and targeted) monitoring, gathered and analyzed specifically for the purpose of accurate and confident decision-making in determining strategy in areas such as market opportunity, market penetration strategy, and market development.[1]
It is a subtype of competitive intelligence (CI), is data and information gathered by companies that provides continuous insight into market trends such as competitors and customers values and preferences. MI along with the marketing capabilities of an organisation provides a guideline into the allocation and implementation of resources and processes.[2] It is used for the purpose of continuously supplying strategic marketing planning for organisations to gauge marketing positions in order for companies to gain competitive advantage and best meet objectives.[3][4]
Market intelligence includes the process of gathering data from the company's external environment like online sources - news websites, company websites, secondary data sources, social media, RSS feeds, etc., whereas the business intelligence process is based primarily on internal recorded events – such as sales, shipments, and purchases. The purpose of incorporating Market Information or intelligence into the Business Intelligence process is to provide decision-makers with a more “complete picture” of ongoing corporate performance in a set of given market conditions.[5]
MI and its broader term, marketing intelligence, was first introduced in “Marketing Intelligence for Top Management” by Kelley,[6] to provide information that was analysed, reliable and consistent for an organisation to better create policies and make business decisions. There are many different views of MI as researchers coming from different professional and educational backgrounds add into its meaning. 
Following Kelley, in “How to Develop a Marketing Intelligence System”, R. Pinkerton[7] shows the proactiveness of organisations as marketing intelligence systems is applied whilst the technological revolution arises.[8]  Contributions to MI include professional organisations such as “Global Intelligence Alliance” and “the Society of Competitive Intelligence Professionals”. These organisations have contributed both empirical and theoretical research in an attempt to further define and understand MI. 
As research into MI comes from scholars and non-scholars of different backgrounds it has resulted in a fragmented state of research. This has led to MI being used interchangeably with other market terms such as competitive intelligence, business intelligence and strategic intelligence. MI to this current date continues to change to meet organisational requirements.
The implementation of MI varies depending on how organisations perceive it. MI is defined as being comprised of three main activities, these activities are Information Acquisition, the gathering of marketing information that is required for current and future customer needs, Information Analysis which is the intelligence gained from the information collected and Information Activation, which is using the intelligence to implement and develop marketing plans. 
Frameworks can be flexible, however the basis that organisations use to model the MI surrounds a four-step process, which are, collection, validation, processing and communication. Data mining techniques are used throughout the processes to aid in the gathering and analysing of data and information retrieved. MI is a continuous process that organisations need to keep track of to improve their strategic marketing planning. These processes target the three activities that MI is defined by. The model can be adjusted and adapted when required and can be implemented all at once or by sections.[9] 
Collection is the first step in the MI model, it involves the gathering of data and information of a particular market sector. Such data and information can be gathered from external sources, such as other organisations and their market strategies, research institutes and business reports[9]. 
Internal factors can include looking into current strategy processes and personal customer trends[9]. It is estimated that 70% to 80% of intelligence resides within organisations employees or, internal MI network, as they are the team who gains information’s when interacting with suppliers, customers and other industry contacts. To involve employees into an intelligence program to gain data and information the following considerations can be noted: developing a rewards program to promote participation, providing MI goals, requirements and a timeframe for information to be given in and creating a proper communication method to promote the intelligence program with employees such as using an e-mail system. 
A challenge that arises in the collection of data and information is the identification of relevant information, this is a result of enterprises not clearly defining a market sector. 
Validation is the second step in the MI model, it is the “normalisation and correction of data and information collected”.[9] The maintaining of good data quality is important as data and information is being retrieved from many different sources. Data and information obtained from sources can be dirty, meaning that it is incomplete, wrong, inappropriate, duplicated. This step will allow data and information to be adjusted and understandable to the organisation, furthermore it allows for consistency and compliance to be present.[9] If data quality is not maintained correctly it can lead to organisational losses with revenue and governmental regulation failures.
Data cleansing is a complex process that involves several stages in order to get good data quality for MI strategy use. Stages include defining the organisation’s level of data quality, detecting error from the data collected and then repairing the errors6, pg732. The five stages of data cleansing are data analysis to identify errors, eliminating the errors, checking to ensure elimination of error were done appropriately, refreshing the data in the data warehouse and finally replacing the dirty data with clean data.
Processing is the third step in the MI model. It involves the use of translating the clean data using organisational rules, modelling, logic and analysis to produce readable information, reports and spreadsheets that allows the organisation to gain specified knowledge.[9] The interpretation of data into readable information is difficult as it is complex, it requires proper technology and heavy commitment from a top organisational level to match data and information gained and align it to a marketing strategy.
Communication is the last step in the MI model. It involves the sharing, delivering and transmission of information gained from the processing step to figures in the organisation who will apply it accordingly to the market strategy[9]. As MI is a continuously changing, the communication of the MI strategy requires managers whom have expertise in the given market industry in order to determine the ongoing validity of the MI strategy and its implementation. In order to make the communication of the MI strategy as successful as possible, this process must be performed by every level of an organisation, also known as the intelligence organisation[4] .
Intelligence organisation refers to the “people and information resources who make the market intelligence process happen”.[4] The five elements of an intelligence organisation are, MI leadership who manages and leads the MI process, a MI team, a portfolio of external information sources that is set up by the MI team, internal MI network made up of MI users and the MI user’s personal information source network. An intelligence organisation element is made up of external and internal factors that allows for a continuous MI process.[4]
Market intelligence needs accurate market information that is gathered with curated Market and Competitive Intelligence tools and methods. To gather information companies can conduct surveys, interviews, visit and monitor competitors outlets or gather and buy data from different sources.
The gathering of MI data is different dependent on an organisation’s financial capabilities. Sources of data and information is separated into qualitative, quantitative, formal, informal, published and unpublished. With such sources being retrieved both internally and externally from the organisation.[8] It involves using search engines and corporate web sites to see competitor’s strategies, identifying business trends through reputable publications and existing customer clientele.[12] Organisations use different systems to gather MI, one system is that is used is Open-source Intelligence system.[13]
Traditional interviews and surveys can be done either in-house or by using specialist agencies. As more and more markets are digitalized the market intelligence space has seen a lot of new digital tools that companies can use. There are tools such as Google Consumer Survey that enable companies to ask web users a question through Google network of online publishers. There are also specialist sites companies can buy market intelligence information and market intelligence software like Contify, Uptime that companies can use to collect and monitor data from the internet.
Open-source intelligence is a predominant form of MI gathering that organisations employ. OSINT is defined as the “scanning, finding, gathering, exploitation, validation, analysis, and sharing with intelligence-seeking clients of publicly available print and digital/electronic data from unclassified, non-secret, and grey literature.[14] It is frequently used as its system is user friendly, its inexpensive and that it processes an abundant amount of raw materials that can be further processed.[15]
MI processes have been used in many organisation’s strategic market planning, however, there are still difficulties in what the hard and soft benefits in using a MI process for an organisation. The benefits of a successful MI process can be sectioned into three categories, better and faster decisions, time and cost savings and organisational learning and new ideas, however, overall, it can improve profitability and the competitiveness of an organisation.[4]
There are issues that arises in the process of acquiring MI data and information. These issues if not mitigated or resolved can lead to financial and reputational damages to the organisation. An issue that can arise is the unethical and illegal collection of data and information. Organisations can collect data for MI illegally or unethically. Industrial espionage is the gathering. An example of illegal MI collection practice is when British Airways breached the Data Protect Act 1984 through accessing Virgin’s confidential flight details [17][9]
A standard of conduct was developed by the non-for-profit organisation Society of Competitive Intelligence Professionals, creating a code of ethics that can be adhered to by organisations when collecting market intelligence, to prevent the illegal and unethical collection of data and information.[9]
"
1026068418,2021-05-31 06:03:57,"Market intelligence (MI) is the information relevant to a company's market - trends, competitor and customer (existing, lost and targeted) monitoring, gathered and analyzed specifically for the purpose of accurate and confident decision-making in determining strategy in areas such as market opportunity, market penetration strategy, and market development.[1]
It is a subtype of competitive intelligence (CI), is data and information gathered by companies that provides continuous insight into market trends such as competitors and customers values and preferences. MI along with the marketing capabilities of an organisation provides a guideline into the allocation and implementation of resources and processes.[2] It is used for the purpose of continuously supplying strategic marketing planning for organisations to gauge marketing positions in order for companies to gain competitive advantage and best meet objectives.[3][4]
Market intelligence includes the process of gathering data from the company's external environment like online sources - news websites, company websites, secondary data sources, social media, RSS feeds, etc., whereas the business intelligence process is based primarily on internal recorded events – such as sales, shipments, and purchases. The purpose of incorporating Market Information or intelligence into the Business Intelligence process is to provide decision-makers with a more “complete picture” of ongoing corporate performance in a set of given market conditions.[5]
MI and its broader term, marketing intelligence, was first introduced in “Marketing Intelligence for Top Management” by Kelley,[6] to provide information that was analysed, reliable and consistent for an organisation to better create policies and make business decisions. There are many different views of MI as researchers coming from different professional and educational backgrounds add into its meaning. 
Following Kelley, in “How to Develop a Marketing Intelligence System”, R. Pinkerton[7] shows the proactiveness of organisations as marketing intelligence systems is applied whilst the technological revolution arises.[8]  Contributions to MI include professional organisations such as “Global Intelligence Alliance” and “the Society of Competitive Intelligence Professionals”. These organisations have contributed both empirical and theoretical research in an attempt to further define and understand MI. 
As research into MI comes from scholars and non-scholars of different backgrounds it has resulted in a fragmented state of research. This has led to MI being used interchangeably with other market terms such as competitive intelligence, business intelligence and strategic intelligence. MI to this current date continues to change to meet organisational requirements.
The implementation of MI varies depending on how organisations perceive it. MI is defined as being comprised of three main activities, these activities are Information Acquisition, the gathering of marketing information that is required for current and future customer needs, Information Analysis which is the intelligence gained from the information collected and Information Activation, which is using the intelligence to implement and develop marketing plans. 
Frameworks can be flexible, however the basis that organisations use to model the MI surrounds a four-step process, which are, collection, validation, processing and communication. Data mining techniques are used throughout the processes to aid in the gathering and analysing of data and information retrieved. MI is a continuous process that organisations need to keep track of to improve their strategic marketing planning. These processes target the three activities that MI is defined by. The model can be adjusted and adapted when required and can be implemented all at once or by sections.[9] 
Collection is the first step in the MI model, it involves the gathering of data and information of a particular market sector. Such data and information can be gathered from external sources, such as other organisations and their market strategies, research institutes and business reports[9]. 
Internal factors can include looking into current strategy processes and personal customer trends[9]. It is estimated that 70% to 80% of intelligence resides within organisations employees or, internal MI network, as they are the team who gains information’s when interacting with suppliers, customers and other industry contacts. To involve employees into an intelligence program to gain data and information the following considerations can be noted: developing a rewards program to promote participation, providing MI goals, requirements and a timeframe for information to be given in and creating a proper communication method to promote the intelligence program with employees such as using an e-mail system. 
A challenge that arises in the collection of data and information is the identification of relevant information, this is a result of enterprises not clearly defining a market sector. 
Validation is the second step in the MI model, it is the “normalisation and correction of data and information collected”.[9] The maintaining of good data quality is important as data and information is being retrieved from many different sources. Data and information obtained from sources can be dirty, meaning that it is incomplete, wrong, inappropriate, duplicated. This step will allow data and information to be adjusted and understandable to the organisation, furthermore it allows for consistency and compliance to be present.[9] If data quality is not maintained correctly it can lead to organisational losses with revenue and governmental regulation failures.
Data cleansing is a complex process that involves several stages in order to get good data quality for MI strategy use. Stages include defining the organisation’s level of data quality, detecting error from the data collected and then repairing the errors6, pg732. The five stages of data cleansing are data analysis to identify errors, eliminating the errors, checking to ensure elimination of error were done appropriately, refreshing the data in the data warehouse and finally replacing the dirty data with clean data.
Processing is the third step in the MI model. It involves the use of translating the clean data using organisational rules, modelling, logic and analysis to produce readable information, reports and spreadsheets that allows the organisation to gain specified knowledge.[9] The interpretation of data into readable information is difficult as it is complex, it requires proper technology and heavy commitment from a top organisational level to match data and information gained and align it to a marketing strategy.
Communication is the last step in the MI model. It involves the sharing, delivering and transmission of information gained from the processing step to figures in the organisation who will apply it accordingly to the market strategy[9]. As MI is a continuously changing, the communication of the MI strategy requires managers whom have expertise in the given market industry in order to determine the ongoing validity of the MI strategy and its implementation. In order to make the communication of the MI strategy as successful as possible, this process must be performed by every level of an organisation, also known as the intelligence organisation[4] .
Intelligence organisation refers to the “people and information resources who make the market intelligence process happen”.[4] The five elements of an intelligence organisation are, MI leadership who manages and leads the MI process, a MI team, a portfolio of external information sources that is set up by the MI team, internal MI network made up of MI users and the MI user’s personal information source network. An intelligence organisation element is made up of external and internal factors that allows for a continuous MI process.[4]
Market intelligence needs accurate market information that is gathered with curated Market and Competitive Intelligence tools and methods. To gather information companies can conduct surveys, interviews, visit and monitor competitors outlets or gather and buy data from different sources.
The gathering of MI data is different dependent on an organisation’s financial capabilities. Sources of data and information is separated into qualitative, quantitative, formal, informal, published and unpublished. With such sources being retrieved both internally and externally from the organisation.[8] It involves using search engines and corporate web sites to see competitor’s strategies, identifying business trends through reputable publications and existing customer clientele.[12] Organisations use different systems to gather MI, one system is that is used is Open-source Intelligence system.[13]
Traditional interviews and surveys can be done either in-house or by using specialist agencies. As more and more markets are digitalized the market intelligence space has seen a lot of new digital tools that companies can use. There are tools such as Google Consumer Survey that enable companies to ask web users a question through Google network of online publishers. There are also specialist sites companies can buy market intelligence information and market intelligence software like Contify, Uptime that companies can use to collect and monitor data from the internet.
Open-source intelligence is a predominant form of MI gathering that organisations employ. OSINT is defined as the “scanning, finding, gathering, exploitation, validation, analysis, and sharing with intelligence-seeking clients of publicly available print and digital/electronic data from unclassified, non-secret, and grey literature.[14] It is frequently used as its system is user friendly, its inexpensive and that it processes an abundant amount of raw materials that can be further processed.[15]
MI processes have been used in many organisation’s strategic market planning, however, there are still difficulties in what the hard and soft benefits in using a MI process for an organisation. The benefits of a successful MI process can be sectioned into three categories, better and faster decisions, time and cost savings and organisational learning and new ideas, however, overall, it can improve profitability and the competitiveness of an organisation.[4]
There are issues that arises in the process of acquiring MI data and information. These issues if not mitigated or resolved can lead to financial and reputational damages to the organisation. An issue that can arise is the unethical and illegal collection of data and information. Organisations can collect data for MI illegally or unethically. Industrial espionage is the gathering. An example of illegal MI collection practice is when British Airways breached the Data Protect Act 1984 through accessing Virgin’s confidential flight details [17][9]
A standard of conduct was developed by the non-for-profit organisation Society of Competitive Intelligence Professionals, creating a code of ethics that can be adhered to by organisations when collecting market intelligence, to prevent the illegal and unethical collection of data and information.[9]
"
1026116621,2021-05-31 13:32:27,"Market intelligence (MI) is the information relevant to a company's market - trends, competitor and customer (existing, lost and targeted) monitoring, gathered and analyzed. It is a subtype of competitive intelligence (CI), is data and information gathered by companies that provides continuous insight into market trends such as competitors and customers values and preferences. As market trends are continuously changing, the definition and process of MI changes as well.
MI along with the marketing capabilities of an organisation provides a guideline into the allocation and implementation of resources and processes.[1] It is used for the purpose of continuously supplying strategic marketing planning for organisations to gauge marketing positions in order for companies to gain competitive advantage and best meet objectives.[2][3]
Organisations can develop MI frameworks and models that are suited to financial capabilities and desired market sectors but are mainly based on the four-step process of collection, validation, processing and communication of MI. The gathering of MI data is sorted into many different categories, including, but not limited to, qualitative, quantitative, formal, informal, published, and unpublished. MI data is gathered both internally and externally.
Benefits that MI can bring are that it provides customer, competitor and market insights allowing organisations to gain a competitive advantage in their marketing strategies. Issues that MI can bring is through acquiring data and information through illegal or unethical ways, which can lead to financial loss and government regulatory failures.
MI and its broader term, marketing intelligence, was first introduced in “Marketing Intelligence for Top Management” by Kelley,[4] to provide information that was analysed, reliable and consistent for an organisation to better create policies and make business decisions. There are many different views of MI as researchers coming from different professional and educational backgrounds add into its meaning. 
Following Kelley, in “How to Develop a Marketing Intelligence System”, R. Pinkerton[5] shows the proactiveness of organisations as marketing intelligence systems is applied whilst the technological revolution arises.[6]  Contributions to MI include professional organisations such as “Global Intelligence Alliance” and “the Society of Competitive Intelligence Professionals”. These organisations have contributed both empirical and theoretical research in an attempt to further define and understand MI. 
As research into MI comes from scholars and non-scholars of different backgrounds it has resulted in a fragmented state of research. This has led to MI being used interchangeably with other market terms such as competitive intelligence, business intelligence and strategic intelligence. MI to this current date continues to change to meet organisational requirements.
The implementation of MI varies depending on how organisations perceive it. MI is defined as being comprised of three main activities, these activities are Information Acquisition, the gathering of marketing information that is required for current and future customer needs, Information Analysis which is the intelligence gained from the information collected and Information Activation, which is using the intelligence to implement and develop marketing plans. 
Frameworks can be flexible, however the basis that organisations use to model the MI surrounds a four-step process, which are, collection, validation, processing and communication. Data mining techniques are used throughout the processes to aid in the gathering and analysing of data and information retrieved. MI is a continuous process that organisations need to keep track of to improve their strategic marketing planning. These processes target the three activities that MI is defined by. The model can be adjusted and adapted when required and can be implemented all at once or by sections.[7] 
Collection is the first step in the MI model, it involves the gathering of data and information of a particular market sector. Such data and information can be gathered from external sources, such as other organisations and their market strategies, research institutes and business reports[7]. 
Internal factors can include looking into current strategy processes and personal customer trends[7]. It is estimated that 70% to 80% of intelligence resides within organisations employees or, internal MI network, as they are the team who gains information’s when interacting with suppliers, customers and other industry contacts. To involve employees into an intelligence program to gain data and information the following considerations can be noted: developing a rewards program to promote participation, providing MI goals, requirements and a timeframe for information to be given in and creating a proper communication method to promote the intelligence program with employees such as using an e-mail system. 
A challenge that arises in the collection of data and information is the identification of relevant information, this is a result of enterprises not clearly defining a market sector. 
Validation is the second step in the MI model, it is the “normalisation and correction of data and information collected”.[7] The maintaining of good data quality is important as data and information is being retrieved from many different sources. Data and information obtained from sources can be dirty, meaning that it is incomplete, wrong, inappropriate, duplicated. This step will allow data and information to be adjusted and understandable to the organisation, furthermore it allows for consistency and compliance to be present.[7] If data quality is not maintained correctly it can lead to organisational losses with revenue and governmental regulation failures.
Data cleansing is a complex process that involves several stages in order to get good data quality for MI strategy use. Stages include defining the organisation’s level of data quality, detecting error from the data collected and then repairing the errors6, pg732. The five stages of data cleansing are data analysis to identify errors, eliminating the errors, checking to ensure elimination of error were done appropriately, refreshing the data in the data warehouse and finally replacing the dirty data with clean data.
Processing is the third step in the MI model. It involves the use of translating the clean data using organisational rules, modelling, logic and analysis to produce readable information, reports and spreadsheets that allows the organisation to gain specified knowledge.[7] The interpretation of data into readable information is difficult as it is complex, it requires proper technology and heavy commitment from a top organisational level to match data and information gained and align it to a marketing strategy.
Communication is the last step in the MI model. It involves the sharing, delivering and transmission of information gained from the processing step to figures in the organisation who will apply it accordingly to the market strategy[7]. As MI is a continuously changing, the communication of the MI strategy requires managers whom have expertise in the given market industry in order to determine the ongoing validity of the MI strategy and its implementation. In order to make the communication of the MI strategy as successful as possible, this process must be performed by every level of an organisation, also known as the intelligence organisation[3] .
Intelligence organisation refers to the “people and information resources who make the market intelligence process happen”.[3] The five elements of an intelligence organisation are, MI leadership who manages and leads the MI process, a MI team, a portfolio of external information sources that is set up by the MI team, internal MI network made up of MI users and the MI user’s personal information source network. An intelligence organisation element is made up of external and internal factors that allows for a continuous MI process.[3]
The gathering of MI data is different dependent on an organisation’s financial capabilities. Sources of data and information is separated into qualitative, quantitative, formal, informal, published and unpublished. With such sources being retrieved both internally and externally from the organisation.[6] It involves using search engines and corporate web sites to see competitor’s strategies, identifying business trends through reputable publications and existing customer clientele.[10] Organisations use different systems to gather MI, one system is that is used is Open-source Intelligence system.[11]
Sources of internal intelligence gathering is include but is not limited to, gathering data from customers, manufacturers, through research and development (R&D), employees, also known as Salesforce, physical evidence, sales quotes, sales records, trade shows and new hires11,pg8. These data sources were ranked by organisations on a scale measuring five for being very important to one being not important. It was founded that customers and manufacturers and R&D are the most important to organisations with one hundred percent of organisations ranking these data sources with the number four or higher11,pg7,. It shows that in the process of collection and gathering MI data and information, these data sources brought the most value to organisations.
Sources of external intelligence gathering is included but is not limited to, gathering data from client meetings, dealers/distributors, customers, business associates, market research projects, suppliers, on line services, periodicals and government publications11,pg8. These data sources were compared on the same scale as internal intelligence gathering sources, with results showing that intelligence gathered through client meetings being the most important to organisations, with one hundred percent of organisations ranking this data source with the number four or higher11,pg7. 
Marketing information systems allow for organisations to continuously acquire, generate, and maintain external and internal information12,pg4. They are systems that make use of artificial intelligence (AI) technology to aid in the planning of strategic and tactical marketing strategy of MI but also share marketing expertise13,pg4.
Open-source intelligence is a predominant form of MI gathering that organisations employ. OSINT is defined as the “scanning, finding, gathering, exploitation, validation, analysis, and sharing with intelligence-seeking clients of publicly available print and digital/electronic data from unclassified, non-secret, and grey literature.[12] It is frequently used as its system is user friendly, its inexpensive and that it processes an abundant amount of raw materials that can be further processed.[13]
Using MI can bring to organisations both benefits and issues depending on how MI is acquired, maintained, and implemented. Benefits that MI can bring includes but is not limited to gaining competitive advantage in their marketing strategies. Issues that MI can bring can include but is not limited to, financial losses and government regulatory failures.
MI processes have been used in many organisation’s strategic market planning, however, there are still difficulties in what the hard and soft benefits in using a MI process for an organisation. The benefits of a successful MI process can be sectioned into three categories, better and faster decisions, time and cost savings and organisational learning and new ideas, however, overall, it can improve profitability and the competitiveness of an organisation.[3]
There are issues that arises in the process of acquiring MI data and information. These issues if not mitigated or resolved can lead to financial and reputational damages to the organisation. An issue that can arise is the unethical and illegal collection of data and information. Organisations can collect data for MI illegally or unethically. Industrial espionage is the gathering. An example of illegal MI collection practice is when British Airways breached the Data Protect Act 1984 through accessing Virgin’s confidential flight details [15][7]
A standard of conduct was developed by the non-for-profit organisation Society of Competitive Intelligence Professionals, creating a code of ethics that can be adhered to by organisations when collecting market intelligence, to prevent the illegal and unethical collection of data and information.[7]
Dirty data that is collected needs to be cleansed to maintain good data quality. Challenges that arise in data cleansing is that there is a large volume of data being received leading to organisations being faced with many risks of failure to detect bad data being processed through3,pg734. If data quality is not managed properly, it can result in financial losses, inefficient implementation of MI strategies and failure to comply with government regulations14,pg80. A reason for financial loss is due operational costs, as there is an increase in resources and time spent to identify and fix the bad data14,pg81.
"
1026118966,2021-05-31 13:52:49,"Market intelligence (MI) is the information relevant to a company's market - trends, competitor and customer (existing, lost and targeted) monitoring, gathered and analyzed. It is a subtype of competitive intelligence (CI), is data and information gathered by companies that provides continuous insight into market trends such as competitors and customers values and preferences. 
MI along with the marketing capabilities of an organisation provides a guideline into the allocation and implementation of resources and processes.[1] It is used for the purpose of continuously supplying strategic marketing planning for organisations to gauge marketing positions in order for companies to gain competitive advantage and best meet objectives.[2][3]
Organisations can develop MI frameworks and models that are suited to financial capabilities and desired market sectors but are mainly based on the four-step process of collection, validation, processing and communication of MI. The gathering of MI data is sorted into many different categories, including, but not limited to, qualitative, quantitative, formal, informal, published, and unpublished. MI data is gathered both internally and externally.
Benefits that MI can bring are that it provides customer, competitor and market insights allowing organisations to gain a competitive advantage in their marketing strategies. Issues that MI can bring is through acquiring data and information through illegal or unethical ways, which can lead to financial loss and government regulatory failures.
MI and its broader term, marketing intelligence, was first introduced in “Marketing Intelligence for Top Management” by Kelley,[4] to provide information that was analysed, reliable and consistent for an organisation to better create policies and make business decisions.  
Following Kelley, in “How to Develop a Marketing Intelligence System”, R. Pinkerton[5] shows the proactiveness of organisations as marketing intelligence systems is applied whilst the technological revolution arises.[6]  Contributions to MI include professional organisations such as “Global Intelligence Alliance” and “the Society of Competitive Intelligence Professionals” (SCIP). These organisations have contributed both empirical and theoretical research in an attempt to further define and understand MI[7].  
As research into MI comes from scholars and non-scholars of different backgrounds it has resulted in a fragmented state of research. This has led to MI being used interchangeably with other market terms such as competitive intelligence, business intelligence and strategic intelligence[8]. MI to this current date continues to change to meet organisational requirements. 
The implementation of MI varies depending on how organisations perceive it. MI is defined as being comprised of three main activities, these activities are Information Acquisition, the gathering of marketing information that is required for current and future customer needs, Information Analysis which is the intelligence gained from the information collected and Information Activation, which is using the intelligence to implement and develop marketing plans[9]. 
Frameworks can be flexible, however the basis that organisations use to model the MI surrounds a four-step process, which are, collection, validation, processing and communication. Data mining techniques are used throughout the processes to aid in the gathering and analysing of data and information retrieved[10].MI is a continuous process that organisations need to keep track of to improve their strategic and tactical marketing planning. These processes target the three activities that MI is defined by[9]. The model can be adjusted and adapted when required and can be implemented all at once or by sections.[11] 
Collection is the first step in the MI model, it involves the gathering of data and information of a particular market sector. Such data and information can be gathered from external sources, such as other organisations and their market strategies, research institutes and business reports[11]. 
Internal factors can include looking into current strategy processes and personal customer trends[11]. It is estimated that 70% to 80% of intelligence resides within organisations employees or, internal MI network, as they are the team who gains information’s when interacting with suppliers, customers and other industry contacts. To involve employees into an intelligence program to gain data and information the following considerations can be noted: developing a rewards program to promote participation, providing MI goals, requirements and a timeframe for information to be given in and creating a proper communication method to promote the intelligence program with employees such as using an e-mail system. 
A challenge that arises in the collection of data and information is the identification of relevant information, this is a result of enterprises not clearly defining a market sector. 
Validation is the second step in the MI model, it is the “normalisation and correction of data and information collected”.[11] The maintaining of good data quality is important as data and information is being retrieved from many different sources. Data and information obtained from sources can be dirty, meaning that it is incomplete, wrong, inappropriate, duplicated. This step will allow data and information to be adjusted and understandable to the organisation, furthermore it allows for consistency and compliance to be present.[11] If data quality is not maintained correctly it can lead to organisational losses with revenue and governmental regulation failures.
Data cleansing is a complex process that involves several stages in order to get good data quality for MI strategy use. Stages include defining the organisation’s level of data quality, detecting error from the data collected and then repairing the errors6, pg732. The five stages of data cleansing are data analysis to identify errors, eliminating the errors, checking to ensure elimination of error were done appropriately, refreshing the data in the data warehouse and finally replacing the dirty data with clean data6,pg734..
Processing is the third step in the MI model. It involves the use of translating the clean data using organisational rules, modelling, logic and analysis to produce readable information, reports and spreadsheets that allows the organisation to gain specified knowledge.[11] The interpretation of data into readable information is difficult as it is complex, it requires proper technology and heavy commitment from a top organisational level to match data and information gained and align it to a marketing strategy4,pg8.
Communication is the last step in the MI model. It involves the sharing, delivering and transmission of information gained from the processing step to figures in the organisation who will apply it accordingly to the market strategy[11]. As MI is a continuously changing, the communication of the MI strategy requires managers whom have expertise in the given market industry in order to determine the ongoing validity of the MI strategy and its implementation4,pg48. In order to make the communication of the MI strategy as successful as possible, this process must be performed by every level of an organisation, also known as the intelligence organisation.
Intelligence organisation refers to the “people and information resources who make the market intelligence process happen”.[3] The five elements of an intelligence organisation are, MI leadership who manages and leads the MI process, a MI team, a portfolio of external information sources that is set up by the MI team, internal MI network made up of MI users and the MI user’s personal information source network. An intelligence organisation element is made up of external and internal factors that allows for a continuous MI process.[3]
The gathering of MI data is different dependent on an organisation’s financial capabilities. Sources of data and information is separated into qualitative, quantitative, formal, informal, published and unpublished. With such sources being retrieved both internally and externally from the organisation.[6] It involves using search engines and corporate web sites to see competitor’s strategies, identifying business trends through reputable publications and existing customer clientele.[14] Organisations use different systems to gather MI, one system is that is used is Open-source Intelligence system.[15]
Sources of internal intelligence gathering is include but is not limited to, gathering data from customers, manufacturers, through research and development (R&D), employees, also known as Salesforce, physical evidence, sales quotes, sales records, trade shows and new hires11,pg8. These data sources were ranked by organisations on a scale measuring five for being very important to one being not important. It was founded that customers and manufacturers and R&D are the most important to organisations with one hundred percent of organisations ranking these data sources with the number four or higher11,pg7,. It shows that in the process of collection and gathering MI data and information, these data sources brought the most value to organisations.
Sources of external intelligence gathering is included but is not limited to, gathering data from client meetings, dealers/distributors, customers, business associates, market research projects, suppliers, on line services, periodicals and government publications11,pg8. These data sources were compared on the same scale as internal intelligence gathering sources, with results showing that intelligence gathered through client meetings being the most important to organisations, with one hundred percent of organisations ranking this data source with the number four or higher11,pg7. 
Marketing information systems allow for organisations to continuously acquire, generate, and maintain external and internal information12,pg4. They are systems that make use of artificial intelligence (AI) technology to aid in the planning of strategic and tactical marketing strategy of MI but also share marketing expertise13,pg4.
Open-source intelligence is a predominant form of MI gathering that organisations employ. OSINT is defined as the “scanning, finding, gathering, exploitation, validation, analysis, and sharing with intelligence-seeking clients of publicly available print and digital/electronic data from unclassified, non-secret, and grey literature.[16] It is frequently used as its system is user friendly, its inexpensive and that it processes an abundant amount of raw materials that can be further processed.[17]
Using MI can bring to organisations both benefits and issues depending on how MI is acquired, maintained, and implemented. Benefits that MI can bring includes but is not limited to gaining competitive advantage in their marketing strategies. Issues that MI can bring can include but is not limited to, financial losses and government regulatory failures.
MI processes have been used in many organisation’s strategic market planning, however, there are still difficulties in what the hard and soft benefits in using a MI process for an organisation. The benefits of a successful MI process can be sectioned into three categories, better and faster decisions, time and cost savings and organisational learning and new ideas, however, overall, it can improve profitability and the competitiveness of an organisation.[3]
An issue that can arise is the unethical and illegal collection of data and information. Organisations can collect data for MI illegally or unethically. Industrial espionage is the gathering. An example of illegal MI collection practice is when British Airways breached the Data Protect Act 1984 through accessing Virgin’s confidential flight details [19][11]
A standard of conduct was developed by the non-for-profit organisation Society of Competitive Intelligence Professionals, creating a code of ethics that can be adhered to by organisations when collecting market intelligence, to prevent the illegal and unethical collection of data and information.[11]
Dirty data that is collected needs to be cleansed to maintain good data quality. Challenges that arise in data cleansing is that there is a large volume of data being received leading to organisations being faced with many risks of failure to detect dirty data being processed through3,pg734[20]. If data quality is not managed properly, it can result in financial losses, inefficient implementation of MI strategies and failure to comply with government regulations[21]. A reason for financial loss is due operational costs, as there is an increase in resources and time spent to identify and fix the dirty data[22].
"
1026119464,2021-05-31 13:57:00,"Market intelligence (MI) is the information relevant to a company's market - trends, competitor and customer (existing, lost and targeted) monitoring, gathered and analysed. It is a subtype of competitive intelligence (CI), is data and information gathered by companies that provides continuous insight into market trends such as competitors and customers values and preferences. 
MI along with the marketing capabilities of an organisation provides a guideline into the allocation and implementation of resources and processes.[1] It is used for the purpose of continuously supplying strategic marketing planning for organisations to gauge marketing positions in order for companies to gain competitive advantage and best meet objectives.[2][3]
Organisations can develop MI frameworks and models that are suited to financial capabilities and desired market sectors but are mainly based on the four-step process of collection, validation, processing and communication of MI. The gathering of MI data is sorted into many different categories, including, but not limited to, qualitative, quantitative, formal, informal, published, and unpublished. MI data is gathered both internally and externally.
Benefits that MI can bring are that it provides customer, competitor and market insights allowing organisations to gain a competitive advantage in their marketing strategies. Issues that MI can bring is through acquiring data and information through illegal or unethical ways, which can lead to financial loss and government regulatory failures.
MI and its broader term, marketing intelligence, was first introduced in “Marketing Intelligence for Top Management” by Kelley,[4] to provide information that was analysed, reliable and consistent for an organisation to better create policies and make business decisions.  
Following Kelley, in “How to Develop a Marketing Intelligence System”, R. Pinkerton[5] shows the proactiveness of organisations as marketing intelligence systems is applied whilst the technological revolution arises.[6]  Contributions to MI include professional organisations such as “Global Intelligence Alliance” and “the Society of Competitive Intelligence Professionals” (SCIP). These organisations have contributed both empirical and theoretical research in an attempt to further define and understand MI[7].  
As research into MI comes from scholars and non-scholars of different backgrounds it has resulted in a fragmented state of research. This has led to MI being used interchangeably with other market terms such as competitive intelligence, business intelligence and strategic intelligence[8]. MI to this current date continues to change to meet organisational requirements. 
The implementation of MI varies depending on how organisations perceive it. MI is defined as being comprised of three main activities, these activities are Information Acquisition, the gathering of marketing information that is required for current and future customer needs, Information Analysis which is the intelligence gained from the information collected and Information Activation, which is using the intelligence to implement and develop marketing plans[9]. 
Frameworks can be flexible, however the basis that organisations use to model the MI surrounds a four-step process, which are, collection, validation, processing and communication. Data mining techniques are used throughout the processes to aid in the gathering and analysing of data and information retrieved[10].MI is a continuous process that organisations need to keep track of to improve their strategic and tactical marketing planning. These processes target the three activities that MI is defined by[9]. The model can be adjusted and adapted when required and can be implemented all at once or by sections.[11] 
Collection is the first step in the MI model, it involves the gathering of data and information of a particular market sector. Such data and information can be gathered from external sources, such as other organisations and their market strategies, research institutes and business reports[11]. 
Internal factors can include looking into current strategy processes and personal customer trends[11]. It is estimated that 70% to 80% of intelligence resides within organisations employees or, internal MI network, as they are the team who gains information’s when interacting with suppliers, customers and other industry contacts. To involve employees into an intelligence program to gain data and information the following considerations can be noted: developing a rewards program to promote participation, providing MI goals, requirements and a timeframe for information to be given in and creating a proper communication method to promote the intelligence program with employees such as using an e-mail system. 
A challenge that arises in the collection of data and information is the identification of relevant information, this is a result of enterprises not clearly defining a market sector. 
Validation is the second step in the MI model, it is the “normalisation and correction of data and information collected”.[11] The maintaining of good data quality is important as data and information is being retrieved from many different sources. Data and information obtained from sources can be dirty, meaning that it is incomplete, wrong, inappropriate, duplicated. This step will allow data and information to be adjusted and understandable to the organisation, furthermore it allows for consistency and compliance to be present.[11] If data quality is not maintained correctly it can lead to organisational losses with revenue and governmental regulation failures.
Data cleansing is a complex process that involves several stages in order to get good data quality for MI strategy use. Stages include defining the organisation’s level of data quality, detecting error from the data collected and then repairing the errors6, pg732. The five stages of data cleansing are data analysis to identify errors, eliminating the errors, checking to ensure elimination of error were done appropriately, refreshing the data in the data warehouse and finally replacing the dirty data with clean data6,pg734..
Processing is the third step in the MI model. It involves the use of translating the clean data using organisational rules, modelling, logic and analysis to produce readable information, reports and spreadsheets that allows the organisation to gain specified knowledge.[11] The interpretation of data into readable information is difficult as it is complex, it requires proper technology and heavy commitment from a top organisational level to match data and information gained and align it to a marketing strategy4,pg8.
Communication is the last step in the MI model. It involves the sharing, delivering and transmission of information gained from the processing step to figures in the organisation who will apply it accordingly to the market strategy[11]. As MI is a continuously changing, the communication of the MI strategy requires managers whom have expertise in the given market industry in order to determine the ongoing validity of the MI strategy and its implementation4,pg48. In order to make the communication of the MI strategy as successful as possible, this process must be performed by every level of an organisation, also known as the intelligence organisation.
Intelligence organisation refers to the “people and information resources who make the market intelligence process happen”.[3] The five elements of an intelligence organisation are, MI leadership who manages and leads the MI process, a MI team, a portfolio of external information sources that is set up by the MI team, internal MI network made up of MI users and the MI user’s personal information source network. An intelligence organisation element is made up of external and internal factors that allows for a continuous MI process.[3]
The gathering of MI data is different dependent on an organisation’s financial capabilities. Sources of data and information is separated into qualitative, quantitative, formal, informal, published and unpublished. With such sources being retrieved both internally and externally from the organisation.[6] It involves using search engines and corporate web sites to see competitor’s strategies, identifying business trends through reputable publications and existing customer clientele.[14] Organisations use different systems to gather MI, one system is that is used is Open-source Intelligence system.[15]
Sources of internal intelligence gathering is include but is not limited to, gathering data from customers, manufacturers, through research and development (R&D), employees, also known as Salesforce, physical evidence, sales quotes, sales records, trade shows and new hires11,pg8. These data sources were ranked by organisations on a scale measuring five for being very important to one being not important. It was founded that customers and manufacturers and R&D are the most important to organisations with one hundred percent of organisations ranking these data sources with the number four or higher11,pg7,. It shows that in the process of collection and gathering MI data and information, these data sources brought the most value to organisations.
Sources of external intelligence gathering is included but is not limited to, gathering data from client meetings, dealers/distributors, customers, business associates, market research projects, suppliers, on line services, periodicals and government publications11,pg8. These data sources were compared on the same scale as internal intelligence gathering sources, with results showing that intelligence gathered through client meetings being the most important to organisations, with one hundred percent of organisations ranking this data source with the number four or higher11,pg7. 
Marketing information systems allow for organisations to continuously acquire, generate, and maintain external and internal information12,pg4. They are systems that make use of artificial intelligence (AI) technology to aid in the planning of strategic and tactical marketing strategy of MI but also share marketing expertise13,pg4.
Open-source intelligence is a predominant form of MI gathering that organisations employ. OSINT is defined as the “scanning, finding, gathering, exploitation, validation, analysis, and sharing with intelligence-seeking clients of publicly available print and digital/electronic data from unclassified, non-secret, and grey literature.[16] It is frequently used as its system is user friendly, its inexpensive and that it processes an abundant amount of raw materials that can be further processed.[17]
Using MI can bring to organisations both benefits and issues depending on how MI is acquired, maintained, and implemented. Benefits that MI can bring includes but is not limited to gaining competitive advantage in their marketing strategies. Issues that MI can bring can include but is not limited to, financial losses and government regulatory failures.
MI processes have been used in many organisation’s strategic market planning, however, there are still difficulties in what the hard and soft benefits in using a MI process for an organisation. The benefits of a successful MI process can be sectioned into three categories, better and faster decisions, time and cost savings and organisational learning and new ideas, however, overall, it can improve profitability and the competitiveness of an organisation.[3]
An issue that can arise is the unethical and illegal collection of data and information. Organisations can collect data for MI illegally or unethically. Industrial espionage is the gathering. An example of illegal MI collection practice is when British Airways breached the Data Protect Act 1984 through accessing Virgin’s confidential flight details [19][11]
A standard of conduct was developed by the non-for-profit organisation Society of Competitive Intelligence Professionals, creating a code of ethics that can be adhered to by organisations when collecting market intelligence, to prevent the illegal and unethical collection of data and information.[11]
Dirty data that is collected needs to be cleansed to maintain good data quality. Challenges that arise in data cleansing is that there is a large volume of data being received leading to organisations being faced with many risks of failure to detect dirty data being processed through3,pg734[20]. If data quality is not managed properly, it can result in financial losses, inefficient implementation of MI strategies and failure to comply with government regulations[21]. A reason for financial loss is due operational costs, as there is an increase in resources and time spent to identify and fix the dirty data[22].
"
1026120318,2021-05-31 14:04:50,"Market intelligence (MI) is the information relevant to a company's market - trends, competitor and customer (existing, lost and targeted) monitoring, gathered and analysed. It is a subtype of competitive intelligence (CI), is data and information gathered by companies that provides continuous insight into market trends such as competitors and customers values and preferences. 
MI along with the marketing capabilities of an organisation provides a guideline into the allocation and implementation of resources and processes.[1] It is used for the purpose of continuously supplying strategic marketing planning for organisations to gauge marketing positions in order for companies to gain competitive advantage and best meet objectives.[2][3]
Organisations can develop MI frameworks and models that are suited to financial capabilities and desired market sectors but are mainly based on the four-step process of collection, validation, processing and communication of MI. The gathering of MI data is sorted into many different categories, including, but not limited to, qualitative, quantitative, formal, informal, published, and unpublished. MI data is gathered both internally and externally.
Benefits that MI can bring are that it provides customer, competitor and market insights allowing organisations to gain a competitive advantage in their marketing strategies. Issues that MI can bring is through acquiring data and information through illegal or unethical ways, which can lead to financial loss and government regulatory failures.
MI and its broader term, marketing intelligence, was first introduced in “Marketing Intelligence for Top Management” by Kelley,[4] to provide information that was analysed, reliable and consistent for an organisation to better create policies and make business decisions.  
Following Kelley, in “How to Develop a Marketing Intelligence System”, R. Pinkerton[5] shows the proactiveness of organisations as marketing intelligence systems is applied whilst the technological revolution arises.[6]  Contributions to MI include professional organisations such as “Global Intelligence Alliance” and “the Society of Competitive Intelligence Professionals” (SCIP). These organisations have contributed both empirical and theoretical research in an attempt to further define and understand MI[7].  
As research into MI comes from scholars and non-scholars of different backgrounds it has resulted in a fragmented state of research. This has led to MI being used interchangeably with other market terms such as competitive intelligence, business intelligence and strategic intelligence[8]. MI to this current date continues to change to meet organisational requirements. 
The implementation of MI varies depending on how organisations perceive it. MI is defined as being comprised of three main activities, these activities are Information Acquisition, the gathering of marketing information that is required for current and future customer needs, Information Analysis which is the intelligence gained from the information collected and Information Activation, which is using the intelligence to implement and develop marketing plans[9]. 
Frameworks can be flexible, however the basis that organisations use to model the MI surrounds a four-step process, which are, collection, validation, processing and communication. Data mining techniques are used throughout the processes to aid in the gathering and analysing of data and information retrieved[10].MI is a continuous process that organisations need to keep track of to improve their strategic and tactical marketing planning. These processes target the three activities that MI is defined by[9]. The model can be adjusted and adapted when required and can be implemented all at once or by sections.[11] 
Collection is the first step in the MI model, it involves the gathering of data and information of a particular market sector. Such data and information can be gathered from external sources, such as other organisations and their market strategies, research institutes and business reports[11]. 
Internal factors can include looking into current strategy processes and personal customer trends[11]. It is estimated that 70% to 80% of intelligence resides within organisations employees or, internal MI network, as they are the team who gains information’s when interacting with suppliers, customers and other industry contacts. To involve employees into an intelligence program to gain data and information the following considerations can be noted: developing a rewards program to promote participation, providing MI goals, requirements and a timeframe for information to be given in and creating a proper communication method to promote the intelligence program with employees such as using an e-mail system. 
A challenge that arises in the collection of data and information is the identification of relevant information, this is a result of organisations not clearly defining a market sector. 
Validation is the second step in the MI model, it is the “normalisation and correction of data and information collected”.[11] The maintaining of good data quality is important as data and information is being retrieved from many different sources. Data and information obtained from sources can be dirty, meaning that it is incomplete, wrong, inappropriate, duplicated. This step will allow data and information to be adjusted and understandable to the organisation, furthermore it allows for consistency and compliance to be present.[11] If data quality is not maintained correctly it can lead to organisational losses with revenue and governmental regulation failures.
Data cleansing is a complex process that involves several stages in order to get good data quality for MI strategy use. Stages include defining the organisation’s level of data quality, detecting error from the data collected and then repairing the errors6, pg732. The five stages of data cleansing are data analysis to identify errors, eliminating the errors, checking to ensure elimination of error are done appropriately, refreshing the data in the data warehouse and finally replacing the dirty data with clean data6,pg734.
Processing is the third step in the MI model. It involves the use of translating the clean data using organisational rules, modelling, logic and analysis to produce readable information, reports and spreadsheets that allows the organisation to gain specified knowledge.[11] The interpretation of data into readable information is difficult as it is complex, it requires proper technology and heavy commitment from a top organisational level to match data and information gained and align it to a marketing strategy4,pg8.
Communication is the last step in the MI model. It involves the sharing, delivering and transmission of information gained from the processing step to figures in the organisation who will apply it accordingly to the market strategy[11]. As MI is a continuously changing, the communication of the MI strategy requires managers whom have expertise in the given market industry in order to determine the ongoing validity of the MI strategy and its implementation4,pg48. In order to make the communication of the MI strategy as successful as possible, this process must be performed by every level of an organisation, also known as the intelligence organisation.
Intelligence organisation refers to the “people and information resources who make the market intelligence process happen”.[3] The five elements of an intelligence organisation are, MI leadership who manages and leads the MI process, a MI team, a portfolio of external information sources that is set up by the MI team, internal MI network made up of MI users and the MI user’s personal information source network. An intelligence organisation element is made up of external and internal factors that allows for a continuous MI process.[3]
The gathering of MI data is different dependent on an organisation’s financial capabilities. Sources of data and information is separated into qualitative, quantitative, formal, informal, published and unpublished. With such sources being retrieved both internally and externally from the organisation.[6] It involves using search engines and corporate web sites to see competitor’s strategies, identifying business trends through reputable publications and existing customer clientele.[14] Organisations use different systems to gather MI, one system is that is used is Open-source Intelligence system.[15]
Sources of internal intelligence gathering is include but is not limited to, gathering data from customers, manufacturers, through research and development (R&D), employees, also known as Salesforce, physical evidence, sales quotes, sales records, trade shows and new hires11,pg8. These data sources were ranked by organisations on a scale measuring five for being very important to one being not important. It was founded that customers and manufacturers and R&D are the most important to organisations with one hundred percent of organisations ranking these data sources with the number four or higher11,pg7,. It shows that in the process of collection and gathering MI data and information, these data sources brought the most value to organisations.
Sources of external intelligence gathering is included but is not limited to, gathering data from client meetings, dealers/distributors, customers, business associates, market research projects, suppliers, on line services, periodicals and government publications11,pg8. These data sources were compared on the same scale as internal intelligence gathering sources, with results showing that intelligence gathered through client meetings being the most important to organisations, with one hundred percent of organisations ranking this data source with the number four or higher11,pg7. 
Marketing information systems allow for organisations to continuously acquire, generate, and maintain external and internal information12,pg4. They are systems that make use of artificial intelligence (AI) technology to aid in the planning of strategic and tactical marketing strategy of MI but also share marketing expertise13,pg4.
Open-source intelligence is a predominant form of MI gathering that organisations employ. OSINT is defined as the “scanning, finding, gathering, exploitation, validation, analysis, and sharing with intelligence-seeking clients of publicly available print and digital/electronic data from unclassified, non-secret, and grey literature.[16] It is frequently used as its system is user friendly, its inexpensive and that it processes an abundant amount of raw materials that can be further processed.[17]
Using MI can bring to organisations both benefits and issues depending on how MI is acquired, maintained, and implemented. Benefits that MI can bring includes but is not limited to gaining competitive advantage in their marketing strategies. Issues that MI can bring can include but is not limited to, financial losses and government regulatory failures.
An issue that can arise is the unethical and illegal collection of data and information. Organisations can collect data for MI illegally or unethically. Industrial espionage is the gathering. An example of illegal MI collection practice is when British Airways breached the Data Protect Act 1984 through accessing Virgin’s confidential flight details [19][11]
A standard of conduct was developed by the non-for-profit organisation Society of Competitive Intelligence Professionals, creating a code of ethics that can be adhered to by organisations when collecting market intelligence, to prevent the illegal and unethical collection of data and information.[11]
Dirty data that is collected needs to be cleansed to maintain good data quality. Challenges that arise in data cleansing is that there is a large volume of data being received leading to organisations being faced with many risks of failure to detect dirty data being processed through3,pg734[20]. If data quality is not managed properly, it can result in financial losses, inefficient implementation of MI strategies and failure to comply with government regulations[21]. A reason for financial loss is due operational costs, as there is an increase in resources and time spent to identify and fix the dirty data[22].
MI processes have been used in many organisation’s strategic market planning, however, there are still difficulties in what the hard and soft benefits in using a MI process for an organisation. The benefits of a successful MI process can be sectioned into three categories, better and faster decisions, time and cost savings and organisational learning and new ideas, however, overall, it can improve profitability and the competitiveness of an organisation.[3]
"
1026123293,2021-05-31 14:28:41,"Market intelligence (MI) is the information relevant to a company's market - trends, competitor and customer (existing, lost and targeted) monitoring, gathered and analysed. It is a subtype of competitive intelligence (CI), is data and information gathered by companies that provides continuous insight into market trends such as competitors and customers values and preferences. 
MI along with the marketing capabilities of an organisation provides a guideline into the allocation and implementation of resources and processes.[1] It is used for the purpose of continuously supplying strategic marketing planning for organisations to gauge marketing positions in order for companies to gain competitive advantage and best meet objectives.[2][3]
Organisations can develop MI frameworks and models that are suited to financial capabilities and desired market sectors but are mainly based on the four-step process of collection, validation, processing and communication of MI. The gathering of MI data is sorted into many different categories, including, but not limited to, qualitative, quantitative, formal, informal, published, and unpublished. MI data is gathered both internally and externally.
Benefits that MI can bring are that it provides customer, competitor and market insights allowing organisations to gain a competitive advantage in their marketing strategies. Issues that MI can bring is through acquiring data and information through illegal or unethical ways, which can lead to financial loss and government regulatory failures.
MI and its broader term, marketing intelligence, was first introduced in “Marketing Intelligence for Top Management” by Kelley,[4] to provide information that was analysed, reliable and consistent for an organisation to better create policies and make business decisions.  
Following Kelley, in “How to Develop a Marketing Intelligence System”, R. Pinkerton[5] shows the proactiveness of organisations as marketing intelligence systems is applied whilst the technological revolution arises.[6]  Contributions to MI include professional organisations such as “Global Intelligence Alliance” and “the Society of Competitive Intelligence Professionals” (SCIP). These organisations have contributed both empirical and theoretical research in an attempt to further define and understand MI[7].  
As research into MI comes from scholars and non-scholars of different backgrounds it has resulted in a fragmented state of research. This has led to MI being used interchangeably with other market terms such as competitive intelligence, business intelligence and strategic intelligence[8]. MI to this current date continues to change to meet organisational requirements. 
The implementation of MI varies depending on how organisations perceive it. MI is defined as being comprised of three main activities, these activities are Information Acquisition, the gathering of marketing information that is required for current and future customer needs, Information Analysis which is the intelligence gained from the information collected and Information Activation, which is using the intelligence to implement and develop marketing plans[9]. 
Frameworks can be flexible, however the basis that organisations use to model the MI surrounds a four-step process, which are, collection, validation, processing and communication. Data mining techniques are used throughout the processes to aid in the gathering and analysing of data and information retrieved[10].MI is a continuous process that organisations need to keep track of to improve their strategic and tactical marketing planning. These processes target the three activities that MI is defined by[9]. The model can be adjusted and adapted when required and can be implemented all at once or by sections.[11] 
Collection is the first step in the MI model, it involves the gathering of data and information of a particular market sector. Such data and information can be gathered from external sources, such as other organisations and their market strategies, research institutes and business reports[11]. 
Internal factors can include looking into current strategy processes and personal customer trends[11]. It is estimated that 70% to 80% of intelligence resides within organisations employees or, internal MI network, as they are the team who gains information’s when interacting with suppliers, customers and other industry contacts[12]. To involve employees into an intelligence program to gain data and information the following considerations can be noted: developing a rewards program to promote participation, providing MI goals, requirements and a timeframe for information to be given in and creating a proper communication method to promote the intelligence program with employees such as using an e-mail system[13][14]. 
A challenge that arises in the collection of data and information is the identification of relevant information, this is a result of organisations not clearly defining a market sector[15].  
Validation is the second step in the MI model, it is the “normalisation and correction of data and information collected”, this which can be referred to as data cleansing[11] The maintaining of good data quality is important as data and information is being retrieved from many different sources[17]. Data and information obtained from sources can be dirty, meaning that it is incomplete, wrong, inappropriate, duplicated. This step will allow data and information to be adjusted and understandable to the organisation, furthermore it allows for consistency and compliance to be present.[11] If data quality is not maintained correctly it can lead to organisational losses with revenue and governmental regulation failures.
Data cleansing is a complex process that involves several stages in order to get good data quality for MI strategy use. Stages include defining the organisation’s level of data quality, detecting error from the data collected and then repairing the errors[18].The five stages of data cleansing are data analysis to identify errors, eliminating the errors, checking to ensure elimination of error are done appropriately, refreshing the data in the data warehouse and finally replacing the dirty data with clean data[19].
Processing is the third step in the MI model. It involves the use of translating the clean data using organisational rules, modelling, logic and analysis to produce readable information, reports and spreadsheets that allows the organisation to gain specified knowledge.[11] The interpretation of data into readable information is difficult as it is complex, it requires proper technology and heavy commitment from a top organisational level to match data and information gained and align it to a marketing strategy4,pg8.
Communication is the last step in the MI model. It involves the sharing, delivering and transmission of information gained from the processing step to figures in the organisation who will apply it accordingly to the market strategy[11]. As MI is a continuously changing, the communication of the MI strategy requires managers whom have expertise in the given market industry in order to determine the ongoing validity of the MI strategy and its implementation4,pg48. In order to make the communication of the MI strategy as successful as possible, this process must be performed by every level of an organisation, also known as the intelligence organisation.
Intelligence organisation refers to the “people and information resources who make the market intelligence process happen”.[3] The five elements of an intelligence organisation are, MI leadership who manages and leads the MI process, a MI team, a portfolio of external information sources that is set up by the MI team, internal MI network made up of MI users and the MI user’s personal information source network. An intelligence organisation element is made up of external and internal factors that allows for a continuous MI process.[3]
The gathering of MI data is different dependent on an organisation’s financial capabilities. Sources of data and information is separated into qualitative, quantitative, formal, informal, published and unpublished. With such sources being retrieved both internally and externally from the organisation.[6] It involves using search engines and corporate web sites to see competitor’s strategies, identifying business trends through reputable publications and existing customer clientele.[21] Organisations use different systems to gather MI, one system is that is used is Open-source Intelligence system.[22]
Sources of internal intelligence gathering is include but is not limited to, gathering data from customers, manufacturers, through research and development (R&D), employees, also known as Salesforce, physical evidence, sales quotes, sales records, trade shows and new hires11,pg8. These data sources were ranked by organisations on a scale measuring five for being very important to one being not important. It was founded that customers and manufacturers and R&D are the most important to organisations with one hundred percent of organisations ranking these data sources with the number four or higher11,pg7,. It shows that in the process of collection and gathering MI data and information, these data sources brought the most value to organisations.
Sources of external intelligence gathering is included but is not limited to, gathering data from client meetings, dealers/distributors, customers, business associates, market research projects, suppliers, on line services, periodicals and government publications11,pg8. These data sources were compared on the same scale as internal intelligence gathering sources, with results showing that intelligence gathered through client meetings being the most important to organisations, with one hundred percent of organisations ranking this data source with the number four or higher11,pg7. 
Marketing information systems allow for organisations to continuously acquire, generate, and maintain external and internal information12,pg4. They are systems that make use of artificial intelligence (AI) technology to aid in the planning of strategic and tactical marketing strategy of MI but also share marketing expertise13,pg4.
Open-source intelligence is a predominant form of MI gathering that organisations employ. OSINT is defined as the “scanning, finding, gathering, exploitation, validation, analysis, and sharing with intelligence-seeking clients of publicly available print and digital/electronic data from unclassified, non-secret, and grey literature.[23] It is frequently used as its system is user friendly, its inexpensive and that it processes an abundant amount of raw materials that can be further processed.[24]
Using MI can bring to organisations both benefits and issues depending on how MI is acquired, maintained, and implemented. Benefits that MI can bring includes but is not limited to gaining competitive advantage in their marketing strategies. Issues that MI can bring can include but is not limited to, financial losses and government regulatory failures.
An issue that can arise is the unethical and illegal collection of data and information. Organisations can collect data for MI illegally or unethically. Industrial espionage is the gathering. An example of illegal MI collection practice is when British Airways breached the Data Protect Act 1984 through accessing Virgin’s confidential flight details [26][11]
A standard of conduct was developed by the non-for-profit organisation Society of Competitive Intelligence Professionals, creating a code of ethics that can be adhered to by organisations when collecting market intelligence, to prevent the illegal and unethical collection of data and information.[11]
Dirty data that is collected needs to be cleansed to maintain good data quality. Challenges that arise in data cleansing is that there is a large volume of data being received leading to organisations being faced with many risks of failure to detect dirty data being processed through3,pg734[27]. If data quality is not managed properly, it can result in financial losses, inefficient implementation of MI strategies and failure to comply with government regulations[28]. A reason for financial loss is due operational costs, as there is an increase in resources and time spent to identify and fix the dirty data[29].
MI processes have been used in many organisation’s strategic market planning, however, there are still difficulties in what the hard and soft benefits in using a MI process for an organisation. The benefits of a successful MI process can be sectioned into three categories, better and faster decisions, time and cost savings and organisational learning and new ideas, however, overall, it can improve profitability and the competitiveness of an organisation.[3]
"
1026124433,2021-05-31 14:38:11,"Market intelligence (MI) is the information relevant to a company's market - trends, competitor and customer (existing, lost and targeted) monitoring, gathered and analysed. It is a subtype of competitive intelligence (CI), is data and information gathered by companies that provides continuous insight into market trends such as competitors and customers values and preferences. 
MI along with the marketing capabilities of an organisation provides a guideline into the allocation and implementation of resources and processes.[1] It is used for the purpose of continuously supplying strategic marketing planning for organisations to gauge marketing positions in order for companies to gain competitive advantage and best meet objectives.[2][3]
Organisations can develop MI frameworks and models that are suited to financial capabilities and desired market sectors but are mainly based on the four-step process of collection, validation, processing and communication of MI. The gathering of MI data is sorted into many different categories, including, but not limited to, qualitative, quantitative, formal, informal, published, and unpublished. MI data is gathered both internally and externally.
Benefits that MI can bring are that it provides customer, competitor and market insights allowing organisations to gain a competitive advantage in their marketing strategies. Issues that MI can bring is through acquiring data and information through illegal or unethical ways, which can lead to financial loss and government regulatory failures.
MI and its broader term, marketing intelligence, was first introduced in “Marketing Intelligence for Top Management” by Kelley,[4] to provide information that was analysed, reliable and consistent for an organisation to better create policies and make business decisions[4].  
Following Kelley, in “How to Develop a Marketing Intelligence System”, R. Pinkerton[5] shows the proactiveness of organisations as marketing intelligence systems is applied whilst the technological revolution arises.[6]  Contributions to MI include professional organisations such as “Global Intelligence Alliance” and “the Society of Competitive Intelligence Professionals” (SCIP). These organisations have contributed both empirical and theoretical research in an attempt to further define and understand MI[7].  
As research into MI comes from scholars and non-scholars of different backgrounds it has resulted in a fragmented state of research. This has led to MI being used interchangeably with other market terms such as competitive intelligence, business intelligence and strategic intelligence[8]. MI to this current date continues to change to meet organisational requirements. 
The implementation of MI varies depending on how organisations perceive it. MI is defined as being comprised of three main activities, these activities are Information Acquisition, the gathering of marketing information that is required for current and future customer needs, Information Analysis which is the intelligence gained from the information collected and Information Activation, which is using the intelligence to implement and develop marketing plans[9]. 
Frameworks can be flexible, however the basis that organisations use to model the MI surrounds a four-step process, which are, collection, validation, processing and communication. Data mining techniques are used throughout the processes to aid in the gathering and analysing of data and information retrieved[10].MI is a continuous process that organisations need to keep track of to improve their strategic and tactical marketing planning. These processes target the three activities that MI is defined by[9]. The model can be adjusted and adapted when required and can be implemented all at once or by sections.[11] 
Collection is the first step in the MI model, it involves the gathering of data and information of a particular market sector. Such data and information can be gathered from external sources, such as other organisations and their market strategies, research institutes and business reports[11]. 
Internal factors can include looking into current strategy processes and personal customer trends[11]. It is estimated that 70% to 80% of intelligence resides within organisations employees or, internal MI network, as they are the team who gains information’s when interacting with suppliers, customers and other industry contacts[12]. To involve employees into an intelligence program to gain data and information the following considerations can be noted: developing a rewards program to promote participation, providing MI goals, requirements and a timeframe for information to be given in and creating a proper communication method to promote the intelligence program with employees such as using an e-mail system[13][14]. 
A challenge that arises in the collection of data and information is the identification of relevant information, this is a result of organisations not clearly defining a market sector[15].  
Validation is the second step in the MI model, this which can be referred to as data cleansing[11] The maintainence of good data quality is important as data and information is being retrieved from many different sources[17]. Data and information obtained from sources can be dirty, meaning that it is incomplete, wrong, inappropriate, duplicated. This step will allow data and information to be adjusted and understandable to the organisation, furthermore it allows for consistency and compliance to be present.[11] If data quality is not maintained correctly it can lead to organisational losses with revenue and governmental regulation failures.
Data cleansing is a complex process that involves several stages in order to get good data quality for MI strategy use. Stages include defining the organisation’s level of data quality, detecting error from the data collected and then repairing the errors[18].The five stages of data cleansing are data analysis to identify errors, eliminating the errors, checking to ensure elimination of error are done appropriately, refreshing the data in the data warehouse and finally replacing the dirty data with clean data[19].
Processing is the third step in the MI model. It involves the use of translating the clean data using organisational rules, modelling, logic and analysis to produce readable information, reports and spreadsheets that allows the organisation to gain specified knowledge.[11] The interpretation of data into readable information is difficult as it is complex, it requires proper technology and heavy commitment from a top organisational level to match data and information gained and align it to a marketing strategy4,pg8.
Communication is the last step in the MI model. It involves the sharing, delivering and transmission of information gained from the processing step to figures in the organisation who will apply it accordingly to the market strategy[11]. As MI is a continuously changing, the communication of the MI strategy requires managers whom have expertise in the given market industry in order to determine the ongoing validity of the MI strategy and its implementation4,pg48. In order to make the communication of the MI strategy as successful as possible, this process must be performed by every level of an organisation, also known as the intelligence organisation.
Intelligence organisation refers to the “people and information resources who make the market intelligence process happen”.[3] The five elements of an intelligence organisation are, MI leadership who manages and leads the MI process, a MI team, a portfolio of external information sources that is set up by the MI team, internal MI network made up of MI users and the MI user’s personal information source network. An intelligence organisation element is made up of external and internal factors that allows for a continuous MI process.[3]
The gathering of MI data is different dependent on an organisation’s financial capabilities. Sources of data and information is separated into qualitative, quantitative, formal, informal, published and unpublished. With such sources being retrieved both internally and externally from the organisation.[6] It involves using search engines and corporate web sites to see competitor’s strategies, identifying business trends through reputable publications and existing customer clientele.[21] Organisations use different systems to gather MI, one system is that is used is Open-source Intelligence system.[22]
Sources of internal intelligence gathering is include but is not limited to, gathering data from customers, manufacturers, through research and development (R&D), employees, also known as salesforce, physical evidence, sales quotes, sales records, trade shows and new hires11,pg8. These data sources were ranked by organisations on a scale measuring five for being very important to one being not important. It was founded that customers and manufacturers and R&D are the most important to organisations with one hundred percent of organisations ranking these data sources with the number four or higher11,pg7,. It shows that in the process of collection and gathering MI data and information, these data sources brought the most value to organisations.
Sources of external intelligence gathering is included but is not limited to, gathering data from client meetings, dealers/distributors, customers, business associates, market research projects, suppliers, on line services, periodicals and government publications11,pg8. These data sources were compared on the same scale as internal intelligence gathering sources, with results showing that intelligence gathered through client meetings being the most important to organisations, with one hundred percent of organisations ranking this data source with the number four or higher11,pg7. 
Marketing information systems allow for organisations to continuously acquire, generate, and maintain external and internal information12,pg4. They are systems that make use of artificial intelligence (AI) technology to aid in the planning of strategic and tactical marketing strategy of MI but also share marketing expertise13,pg4.
Open-source intelligence is a predominant form of MI gathering that organisations employ. OSINT is defined as the “scanning, finding, gathering, exploitation, validation, analysis, and sharing with intelligence-seeking clients of publicly available print and digital/electronic data from unclassified, non-secret, and grey literature.[23] It is frequently used as its system is user friendly, its inexpensive and that it processes an abundant amount of raw materials that can be further processed.[24]
Using MI can bring to organisations both benefits and issues depending on how MI is acquired, maintained, and implemented. Benefits that MI can bring includes but is not limited to gaining competitive advantage in their marketing strategies. Issues that MI can bring can include but is not limited to, financial losses and government regulatory failures.
There are issues that arises in the process of acquiring MI data and information and the implementation of an organisations marketing strategy. Issues such as the acquiring intelligence unethically and illegally can lead to failures with government regulations, also, if dirty data is not properly cleansed and problems aren’t mitigated or resolved can lead to a range of negative impacts that can result in financial and reputational losses to the organisation.
An issue that can arise is the unethical and illegal collection of data and information. Organisations can collect data for MI illegally or unethically. Industrial espionage is the gathering. An example of illegal MI collection practice is when British Airways breached the Data Protect Act 1984 through accessing Virgin’s confidential flight details [26][11]
A standard of conduct was developed by the non-for-profit organisation Society of Competitive Intelligence Professionals, creating a code of ethics that can be adhered to by organisations when collecting market intelligence, to prevent the illegal and unethical collection of data and information.[11]
Dirty data that is collected needs to be cleansed to maintain good data quality. Challenges that arise in data cleansing is that there is a large volume of data being received leading to organisations being faced with many risks of failure to detect dirty data being processed through3,pg734[27]. If data quality is not managed properly, it can result in financial losses, inefficient implementation of MI strategies and failure to comply with government regulations[28]. A reason for financial loss is due operational costs, as there is an increase in resources and time spent to identify and fix the dirty data[29].
MI processes have been used in many organisation’s strategic market planning, however, there are still difficulties in what the hard and soft benefits in using a MI process for an organisation. The benefits of a successful MI process can be sectioned into three categories, better and faster decisions, time and cost savings and organisational learning and new ideas, however, overall, it can improve profitability and the competitiveness of an organisation.[3]
"
1026204454,2021-05-31 23:53:20,"Market intelligence (MI) is the information relevant to a company's market - trends, competitor and customer (existing, lost and targeted) monitoring, gathered and analysed. It is a subtype of competitive intelligence (CI), is data and information gathered by companies that provides continuous insight into market trends such as competitors and customers values and preferences. 
MI along with the marketing capabilities of an organisation provides a guideline into the allocation and implementation of resources and processes.[1] It is used for the purpose of continuously supplying strategic marketing planning for organisations to gauge marketing positions in order for companies to gain competitive advantage and best meet objectives.[2][3]
Organisations can develop MI frameworks and models that are suited to financial capabilities and desired market sectors but are mainly based on the four-step process of collection, validation, processing and communication of MI. The gathering of MI data is sorted into many different categories, including, but not limited to, qualitative, quantitative, formal, informal, published, and unpublished. MI data is gathered both internally and externally.
Benefits that MI can bring are that it provides customer, competitor and market insights allowing organisations to gain a competitive advantage in their marketing strategies. Issues that MI can bring is through acquiring data and information through illegal or unethical ways, which can lead to financial loss and government regulatory failures.
MI and its broader term, marketing intelligence, was first introduced in “Marketing Intelligence for Top Management” by Kelley,[4] to provide information that was analysed, reliable and consistent for an organisation to better create policies and make business decisions[4].  
Following Kelley, in “How to Develop a Marketing Intelligence System”, R. Pinkerton[5] shows the proactiveness of organisations as marketing intelligence systems is applied whilst the technological revolution arises.[6]  Contributions to MI include professional organisations such as “Global Intelligence Alliance” and “the Society of Competitive Intelligence Professionals” (SCIP). These organisations have contributed both empirical and theoretical research in an attempt to further define and understand MI[7].  
As research into MI comes from scholars and non-scholars of different backgrounds it has resulted in a fragmented state of research. This has led to MI being used interchangeably with other market terms such as competitive intelligence, business intelligence and strategic intelligence[8]. MI to this current date continues to change to meet organisational requirements. 
The implementation of MI varies depending on how organisations perceive it. MI is defined as being comprised of three main activities, these activities are Information Acquisition, the gathering of marketing information that is required for current and future customer needs, Information Analysis which is the intelligence gained from the information collected and Information Activation, which is using the intelligence to implement and develop marketing plans[9]. 
Frameworks can be flexible, however the basis that organisations use to model the MI surrounds a four-step process, which are, collection, validation, processing and communication. Data mining techniques are used throughout the processes to aid in the gathering and analysing of data and information retrieved[10].MI is a continuous process that organisations need to keep track of to improve their strategic and tactical marketing planning. These processes target the three activities that MI is defined by[9]. The model can be adjusted and adapted when required and can be implemented all at once or by sections.[11] 
Collection is the first step in the MI model, it involves the gathering of data and information of a particular market sector. Such data and information can be gathered from external sources, such as other organisations and their market strategies, research institutes and business reports[11]. 
Internal factors can include looking into current strategy processes and personal customer trends[11]. It is estimated that 70% to 80% of intelligence resides within organisations employees or, internal MI network, as they are the team who gains information’s when interacting with suppliers, customers and other industry contacts[12]. To involve employees into an intelligence program to gain data and information the following considerations can be noted: developing a rewards program to promote participation, providing MI goals, requirements and a timeframe for information to be given in and creating a proper communication method to promote the intelligence program with employees such as using an e-mail system[13][14]. 
A challenge that arises in the collection of data and information is the identification of relevant information, this is a result of organisations not clearly defining a market sector[15].  
Validation is the second step in the MI model, this which can be referred to as data cleansing[11] The maintainence of good data quality is important as data and information is being retrieved from many different sources[17]. Data and information obtained from sources can be dirty, meaning that it is incomplete, wrong, inappropriate, duplicated. This step will allow data and information to be adjusted and understandable to the organisation, furthermore it allows for consistency and compliance to be present.[11] If data quality is not maintained correctly it can lead to organisational losses with revenue and governmental regulation failures.
Data cleansing is a complex process that involves several stages in order to get good data quality for MI strategy use. Stages include defining the organisation’s level of data quality, detecting error from the data collected and then repairing the errors[18].The five stages of data cleansing are data analysis to identify errors, eliminating the errors, checking to ensure elimination of error are done appropriately, refreshing the data in the data warehouse and finally replacing the dirty data with clean data[19].
Processing is the third step in the MI model. It involves the use of translating the clean data using organisational rules, modelling, logic and analysis to produce readable information, reports and spreadsheets that allows the organisation to gain specified knowledge.[11] The interpretation of data into readable information is difficult as it is complex, it requires proper technology and heavy commitment from a top organisational level to match data and information gained and align it to a marketing strategy4,pg8.
Communication is the last step in the MI model. It involves the sharing, delivering and transmission of information gained from the processing step to figures in the organisation who will apply it accordingly to the market strategy[11]. As MI is a continuously changing, the communication of the MI strategy requires managers whom have expertise in the given market industry in order to determine the ongoing validity of the MI strategy and its implementation4,pg48. In order to make the communication of the MI strategy as successful as possible, this process must be performed by every level of an organisation, also known as the intelligence organisation.
Intelligence organisation refers to the “people and information resources who make the market intelligence process happen”.[3] The five elements of an intelligence organisation are, MI leadership who manages and leads the MI process, a MI team, a portfolio of external information sources that is set up by the MI team, internal MI network made up of MI users and the MI user’s personal information source network. An intelligence organisation element is made up of external and internal factors that allows for a continuous MI process.[3]
The gathering of MI data is different dependent on an organisation’s financial capabilities. Sources of data and information is separated into qualitative, quantitative, formal, informal, published and unpublished. With such sources being retrieved both internally and externally from the organisation.[6] It involves using search engines and corporate web sites to see competitor’s strategies, identifying business trends through reputable publications and existing customer clientele.[21] Organisations use different systems to gather MI, one system is that is used is Open-source Intelligence system.[22]
Sources of internal intelligence gathering is include but is not limited to, gathering data from customers, manufacturers, through research and development (R&D), employees, also known as salesforce, physical evidence, sales quotes, sales records, trade shows and new hires[23]. These data sources were ranked by organisations on a scale measuring five for being very important to one being not important. It was founded that customers and manufacturers and R&D are the most important to organisations with one hundred percent of organisations ranking these data sources with the number four or higher[24],. It shows that in the process of collection and gathering MI data and information, these data sources brought the most value to organisations.
Sources of external intelligence gathering is included but is not limited to, gathering data from client meetings, dealers/distributors, customers, business associates, market research projects, suppliers, on line services, periodicals and government publications[25]. These data sources were compared on the same scale as internal intelligence gathering sources, with results showing that intelligence gathered through client meetings being the most important to organisations, with one hundred percent of organisations ranking this data source with the number four or higher[26]. 
Marketing information systems allow for organisations to continuously acquire, generate, and maintain external and internal information[27]. They are systems that make use of artificial intelligence (AI) technology to aid in the planning of strategic and tactical marketing strategy of MI but also share marketing expertise[27].
Open-source intelligence is a predominant form of MI gathering that organisations employ. OSINT is defined as the “scanning, finding, gathering, exploitation, validation, analysis, and sharing with intelligence-seeking clients of publicly available print and digital/electronic data from unclassified, non-secret, and grey literature.[28] It is frequently used as its system is user friendly, its inexpensive and that it processes an abundant amount of raw materials that can be further processed.[29]
Using MI can bring to organisations both benefits and issues depending on how MI is acquired, maintained, and implemented. Benefits that MI can bring includes but is not limited to gaining competitive advantage in their marketing strategies. Issues that MI can bring can include but is not limited to, financial losses and government regulatory failures.
There are issues that arises in the process of acquiring MI data and information and the implementation of an organisations marketing strategy. Issues such as the acquiring intelligence unethically and illegally can lead to failures with government regulations, also, if dirty data is not properly cleansed and problems aren’t mitigated or resolved can lead to a range of negative impacts that can result in financial and reputational losses to the organisation.
An issue that can arise is the unethical and illegal collection of data and information. Organisations can collect data for MI illegally or unethically. Industrial espionage is the gathering. An example of illegal MI collection practice is when British Airways breached the Data Protect Act 1984 through accessing Virgin’s confidential flight details [31][11]
A standard of conduct was developed by the non-for-profit organisation Society of Competitive Intelligence Professionals, creating a code of ethics that can be adhered to by organisations when collecting market intelligence, to prevent the illegal and unethical collection of data and information.[11]
Dirty data that is collected needs to be cleansed to maintain good data quality. Challenges that arise in data cleansing is that there is a large volume of data being received leading to organisations being faced with many risks of failure to detect dirty data being processed through3,pg734[32]. If data quality is not managed properly, it can result in financial losses, inefficient implementation of MI strategies and failure to comply with government regulations[33]. A reason for financial loss is due operational costs, as there is an increase in resources and time spent to identify and fix the dirty data[34].
MI processes have been used in many organisation’s strategic market planning, however, there are still difficulties in what the hard and soft benefits in using a MI process for an organisation. The benefits of a successful MI process can be sectioned into three categories, better and faster decisions, time and cost savings and organisational learning and new ideas, however, overall, it can improve profitability and the competitiveness of an organisation.[3]
"
1026208660,2021-06-01 00:21:00,"Market intelligence (MI) is the information relevant to a company's market - trends, competitor and customer (existing, lost and targeted) monitoring, gathered and analysed. It is a subtype of competitive intelligence (CI), is data and information gathered by companies that provides continuous insight into market trends such as competitors and customers values and preferences. 
MI along with the marketing capabilities of an organisation provides a guideline into the allocation and implementation of resources and processes.[1] It is used for the purpose of continuously supplying strategic marketing planning for organisations to gauge marketing positions in order for companies to gain competitive advantage and best meet objectives.[2][3]
Organisations can develop MI frameworks and models that are suited to financial capabilities and desired market sectors but are mainly based on the four-step process of collection, validation, processing and communication of MI. The gathering of MI data is sorted into many different categories, including, but not limited to, qualitative, quantitative, formal, informal, published, and unpublished. MI data is gathered both internally and externally.
Benefits that MI can bring are that it provides customer, competitor and market insights allowing organisations to gain a competitive advantage in their marketing strategies. Issues that MI can bring is through acquiring data and information through illegal or unethical ways, which can lead to financial loss and government regulatory failures.
MI and its broader term, marketing intelligence, was first introduced in “Marketing Intelligence for Top Management” by Kelley,[4] to provide information that was analysed, reliable and consistent for an organisation to better create policies and make business decisions[4].  
Following Kelley, in “How to Develop a Marketing Intelligence System”, R. Pinkerton[5] shows the proactiveness of organisations as marketing intelligence systems is applied whilst the technological revolution arises.[6]  Contributions to MI include professional organisations such as “Global Intelligence Alliance” and “the Society of Competitive Intelligence Professionals” (SCIP). These organisations have contributed both empirical and theoretical research in an attempt to further define and understand MI[7].  
As research into MI comes from scholars and non-scholars of different backgrounds it has resulted in a fragmented state of research. This has led to MI being used interchangeably with other market terms such as competitive intelligence, business intelligence and strategic intelligence[8]. MI to this current date continues to change to meet organisational requirements[4]. 
The implementation of MI varies depending on how organisations perceive it[9]. MI is defined as being comprised of three main activities, these activities are Information Acquisition, the gathering of marketing information that is required for current and future customer needs, Information Analysis which is the intelligence gained from the information collected and Information Activation, which is using the intelligence to implement and develop marketing plans[10]. 
Frameworks can be flexible, however the basis that organisations use to model the MI surrounds a four-step process, which are, collection, validation, processing and communication. Data mining techniques are used throughout the processes to aid in the gathering and analysing of data and information retrieved[11].MI is a continuous process that organisations need to keep track of to improve their strategic and tactical marketing planning. These processes target the three activities that MI is defined by[10]. The model can be adjusted and adapted when required and can be implemented all at once or by sections.[9] 
Collection is the first step in the MI model, it involves the gathering of data and information of a particular market sector[9]. Such data and information can be gathered from external sources, such as other organisations and their market strategies, research institutes and business reports[9]. 
Internal factors can include looking into current strategy processes and personal customer trends[9]. It is estimated that 70% to 80% of intelligence resides within organisations employees or, internal MI network, as they are the team who gains information’s when interacting with suppliers, customers and other industry contacts[12]. To involve employees into an intelligence program to gain data and information the following considerations can be noted: developing a rewards program to promote participation, providing MI goals, requirements and a timeframe for information to be given in and creating a proper communication method to promote the intelligence program with employees such as using an e-mail system[13][14]. 
A challenge that arises in the collection of data and information is the identification of relevant information, this is a result of organisations not clearly defining a market sector[15].  
Validation is the second step in the MI model, this which can be referred to as data cleansing[9][17].The maintenance of good data quality is important as data and information is being retrieved from many different sources[17]. Data and information obtained from sources can be dirty, meaning that it is incomplete, wrong, inappropriate, duplicated[9]. This step will allow data and information to be adjusted and understandable to the organisation, furthermore it allows for consistency and compliance to be present.[9] If data quality is not maintained correctly it can lead to organisational losses with revenue and governmental regulation failures[18].
Data cleansing is a complex process that involves several stages in order to get good data quality for MI strategy use[17]. Stages include defining the organisation’s level of data quality, detecting error from the data collected and then repairing the errors[19].The five stages of data cleansing are data analysis to identify errors, eliminating the errors, checking to ensure elimination of error are done appropriately, refreshing the data in the data warehouse and finally replacing the dirty data with clean data[20].
Processing is the third step in the MI model. It involves the use of translating the clean data using organisational rules, modelling, logic and analysis to produce readable information, reports and spreadsheets that allows the organisation to gain specified knowledge.[9] The interpretation of data into readable information is difficult as it is complex, it requires proper technology and heavy commitment from a top organisational level to match data and information gained and align it to a marketing strategy4,pg8.
Communication is the last step in the MI model. It involves the sharing, delivering and transmission of information gained from the processing step to figures in the organisation who will apply it accordingly to the market strategy[9]. As MI is a continuously changing, the communication of the MI strategy requires managers whom have expertise in the given market industry in order to determine the ongoing validity of the MI strategy and its implementation4,pg48. In order to make the communication of the MI strategy as successful as possible, this process must be performed by every level of an organisation, also known as the intelligence organisation.
Intelligence organisation refers to the “people and information resources who make the market intelligence process happen”.[3] The five elements of an intelligence organisation are, MI leadership who manages and leads the MI process, a MI team, a portfolio of external information sources that is set up by the MI team, internal MI network made up of MI users and the MI user’s personal information source network[3]. An intelligence organisation element is made up of external and internal factors that allows for a continuous MI process[3][6].
The gathering of MI data is different dependent on an organisation’s financial capabilities. Sources of data and information is separated into qualitative, quantitative, formal, informal, published and unpublished. With such sources being retrieved both internally and externally from the organisation[6]. It involves using search engines and corporate web sites to see competitor’s strategies, identifying business trends through reputable publications and existing customer clientele.[22] Organisations use different systems to gather MI, one system is that is used is Open-source Intelligence system.[23]
Sources of internal intelligence gathering is include but is not limited to, gathering data from customers, manufacturers, through research and development (R&D), employees, also known as salesforce, physical evidence, sales quotes, sales records, trade shows and new hires[24]. These data sources were ranked by organisations on a scale measuring five for being very important to one being not important. It was founded that customers and manufacturers and R&D are the most important to organisations with one hundred percent of organisations ranking these data sources with the number four or higher[25],. It shows that in the process of collection and gathering MI data and information, these data sources brought the most value to organisations.
Sources of external intelligence gathering is included but is not limited to, gathering data from client meetings, dealers/distributors, customers, business associates, market research projects, suppliers, on line services, periodicals and government publications[26]. These data sources were compared on the same scale as internal intelligence gathering sources, with results showing that intelligence gathered through client meetings being the most important to organisations, with one hundred percent of organisations ranking this data source with the number four or higher[27]. 
Marketing information systems allow for organisations to continuously acquire, generate, and maintain external and internal information[28]. They are systems that make use of artificial intelligence (AI) technology to aid in the planning of strategic and tactical marketing strategy of MI but also share marketing expertise[28].
Open-source intelligence is a predominant form of MI gathering that organisations employ. OSINT is defined as the “scanning, finding, gathering, exploitation, validation, analysis, and sharing with intelligence-seeking clients of publicly available print and digital/electronic data from unclassified, non-secret, and grey literature[29]. It is frequently used as its system is user friendly, its inexpensive and that it processes an abundant amount of raw materials that can be further processed.[30]
Using MI can bring to organisations both benefits and issues depending on how MI is acquired, maintained, and implemented. Benefits that MI can bring includes but is not limited to gaining competitive advantage in their marketing strategies[3]. Issues that MI can bring can include but is not limited to, financial losses and government regulatory failures[9].
There are issues that arises in the process of acquiring MI data and information and the implementation of an organisations marketing strategy. Issues such as the acquiring intelligence unethically and illegally can lead to failures with government regulations, also, if dirty data is not properly cleansed and problems aren’t mitigated or resolved can lead to a range of negative impacts that can result in financial and reputational losses to the organisation[9][18].
An issue that can arise is the unethical and illegal collection of data and information. Organisations can collect data for MI illegally or unethically to try and gain competitive advantages, this is known asindustrial espionage[32]. An example of illegal MI collection practice is when British Airways breached the Data Protect Act 1984 through accessing Virgin’s confidential flight details [33][9]
A standard of conduct was developed by the non-for-profit organisation Society of Competitive Intelligence Professionals, creating a code of ethics that can be adhered to by organisations when collecting market intelligence, to prevent the illegal and unethical collection of data and information.[9]
Dirty data that is collected needs to be cleansed to maintain good data quality. Challenges that arise in data cleansing is that there is a large volume of data being received leading to organisations being faced with many risks of failure to detect dirty data being processed through[34]. If data quality is not managed properly, it can result in financial losses, inefficient implementation of MI strategies and failure to comply with government regulations[35]. A reason for financial loss is due operational costs, as there is an increase in resources and time spent to identify and fix the dirty data[18].
MI processes have been used in many organisation’s strategic market planning, however, there are still difficulties in what the hard and soft benefits in using a MI process for an organisation[3]. The benefits of a successful MI process can be sectioned into three categories, better and faster decisions, time and cost savings and organisational learning and new ideas, however, overall, it can improve profitability and the competitiveness of an organisation.[3]
"
1026210303,2021-06-01 00:33:43,"Market intelligence (MI) is the information relevant to a company's market - trends, competitor and customer (existing, lost and targeted) monitoring, gathered and analysed. It is a subtype of competitive intelligence (CI), is data and information gathered by companies that provides continuous insight into market trends such as competitors and customers values and preferences. 
MI along with the marketing capabilities of an organisation provides a guideline into the allocation and implementation of resources and processes.[1] It is used for the purpose of continuously supplying strategic marketing planning for organisations to gauge marketing positions in order for companies to gain competitive advantage and best meet objectives.[2][3]
Organisations can develop MI frameworks and models that are suited to financial capabilities and desired market sectors but are mainly based on the four-step process of collection, validation, processing and communication of MI. The gathering of MI data is sorted into many different categories, including, but not limited to, qualitative, quantitative, formal, informal, published, and unpublished. MI data is gathered both internally and externally.
Benefits that MI can bring are that it provides customer, competitor and market insights allowing organisations to gain a competitive advantage in their marketing strategies. Issues that MI can bring is through acquiring data and information through illegal or unethical ways, which can lead to financial loss and government regulatory failures.
MI and its broader term, marketing intelligence, was first introduced in “Marketing Intelligence for Top Management” by Kelley,[4] to provide information that was analysed, reliable and consistent for an organisation to better create policies and make business decisions[4].  
Following Kelley, in “How to Develop a Marketing Intelligence System”, R. Pinkerton[5] shows the proactiveness of organisations as marketing intelligence systems is applied whilst the technological revolution arises.[6]  Contributions to MI include professional organisations such as “Global Intelligence Alliance” and “the Society of Competitive Intelligence Professionals” (SCIP). These organisations have contributed both empirical and theoretical research in an attempt to further define and understand MI[7].  
As research into MI comes from scholars and non-scholars of different backgrounds it has resulted in a fragmented state of research. This has led to MI being used interchangeably with other market terms such as competitive intelligence, business intelligence and strategic intelligence[8]. MI to this current date continues to change to meet organisational requirements[4]. 
The implementation of MI varies depending on how organisations perceive it[9]. MI is defined as being comprised of three main activities, these activities are Information Acquisition, the gathering of marketing information that is required for current and future customer needs, Information Analysis which is the intelligence gained from the information collected and Information Activation, which is using the intelligence to implement and develop marketing plans[10]. 
Frameworks can be flexible, however the basis that organisations use to model the MI surrounds a four-step process, which are, collection, validation, processing and communication. Data mining techniques are used throughout the processes to aid in the gathering and analysing of data and information retrieved[11].MI is a continuous process that organisations need to keep track of to improve their strategic and tactical marketing planning. These processes target the three activities that MI is defined by[10]. The model can be adjusted and adapted when required and can be implemented all at once or by sections.[9] 
Collection is the first step in the MI model, it involves the gathering of data and information of a particular market sector[9]. Such data and information can be gathered from external sources, such as other organisations and their market strategies, research institutes and business reports[9]. 
Internal factors can include looking into current strategy processes and personal customer trends[9]. It is estimated that 70% to 80% of intelligence resides within organisations employees or, internal MI network, as they are the team who gains information’s when interacting with suppliers, customers and other industry contacts[12]. To involve employees into an intelligence program to gain data and information the following considerations can be noted: developing a rewards program to promote participation, providing MI goals, requirements and a timeframe for information to be given in and creating a proper communication method to promote the intelligence program with employees such as using an e-mail system[13][14]. 
A challenge that arises in the collection of data and information is the identification of relevant information, this is a result of organisations not clearly defining a market sector[15].  
Validation is the second step in the MI model, this which can be referred to as data cleansing[9][17].The maintenance of good data quality is important as data and information is being retrieved from many different sources[17]. Data and information obtained from sources can be dirty, meaning that it is incomplete, wrong, inappropriate, duplicated[9]. This step will allow data and information to be adjusted and understandable to the organisation, furthermore it allows for consistency and compliance to be present.[9] If data quality is not maintained correctly it can lead to organisational losses with revenue and governmental regulation failures[18].
Data cleansing is a complex process that involves several stages in order to get good data quality for MI strategy use[17]. Stages include defining the organisation’s level of data quality, detecting error from the data collected and then repairing the errors[19].The five stages of data cleansing are data analysis to identify errors, eliminating the errors, checking to ensure elimination of error are done appropriately, refreshing the data in the data warehouse and finally replacing the dirty data with clean data[20].
Processing is the third step in the MI model. It involves the use of translating the clean data using organisational rules, modelling, logic and analysis to produce readable information, reports and spreadsheets that allows the organisation to gain specified knowledge.[9] The interpretation of data into readable information is difficult as it is complex, it requires proper technology and heavy commitment from a top organisational level to match data and information gained and align it to a marketing strategy[21].
Communication is the last step in the MI model. It involves the sharing, delivering and transmission of information gained from the processing step to figures in the organisation who will apply it accordingly to the market strategy[22]. As MI is a continuously changing, the communication of the MI strategy requires managers whom have expertise in the given market industry in order to determine the ongoing validity of the MI strategy and its implementation[23]. In order to make the communication of the MI strategy as successful as possible, this process must be performed by every level of an organisation, also known as the intelligence organisation[6].
Intelligence organisation refers to the “people and information resources who make the market intelligence process happen”[3]. The five elements of an intelligence organisation are, MI leadership who manages and leads the MI process, a MI team, a portfolio of external information sources that is set up by the MI team, internal MI network made up of MI users and the MI user’s personal information source network[3]. An intelligence organisation element is made up of external and internal factors that allows for a continuous MI process[3][6].
The gathering of MI data is different dependent on an organisation’s financial capabilities. Sources of data and information is separated into qualitative, quantitative, formal, informal, published and unpublished. With such sources being retrieved both internally and externally from the organisation[6]. It involves using search engines and corporate web sites to see competitor’s strategies, identifying business trends through reputable publications and existing customer clientele.[25] Organisations use different systems to gather MI, one system is that is used is Open-source Intelligence system.[26]
Sources of internal intelligence gathering is include but is not limited to, gathering data from customers, manufacturers, through research and development (R&D), employees, also known as salesforce, physical evidence, sales quotes, sales records, trade shows and new hires[27]. These data sources were ranked by organisations on a scale measuring five for being very important to one being not important. It was founded that customers and manufacturers and R&D are the most important to organisations with one hundred percent of organisations ranking these data sources with the number four or higher[28],. It shows that in the process of collection and gathering MI data and information, these data sources brought the most value to organisations.
Sources of external intelligence gathering is included but is not limited to, gathering data from client meetings, dealers/distributors, customers, business associates, market research projects, suppliers, on line services, periodicals and government publications[29]. These data sources were compared on the same scale as internal intelligence gathering sources, with results showing that intelligence gathered through client meetings being the most important to organisations, with one hundred percent of organisations ranking this data source with the number four or higher[30]. 
Marketing information systems allow for organisations to continuously acquire, generate, and maintain external and internal information[31]. They are systems that make use of artificial intelligence (AI) technology to aid in the planning of strategic and tactical marketing strategy of MI but also share marketing expertise[31].
Open-source intelligence is a predominant form of MI gathering that organisations employ. OSINT is defined as the “scanning, finding, gathering, exploitation, validation, analysis, and sharing with intelligence-seeking clients of publicly available print and digital/electronic data from unclassified, non-secret, and grey literature[32]. It is frequently used as its system is user friendly, its inexpensive and that it processes an abundant amount of raw materials that can be further processed.[33]
Using MI can bring to organisations both benefits and issues depending on how MI is acquired, maintained, and implemented. Benefits that MI can bring includes but is not limited to gaining competitive advantage in their marketing strategies[3]. Issues that MI can bring can include but is not limited to, financial losses and government regulatory failures[9].
There are issues that arises in the process of acquiring MI data and information and the implementation of an organisations marketing strategy. Issues such as the acquiring intelligence unethically and illegally can lead to failures with government regulations, also, if dirty data is not properly cleansed and problems aren’t mitigated or resolved can lead to a range of negative impacts that can result in financial and reputational losses to the organisation[9][18].
An issue that can arise is the unethical and illegal collection of data and information. Organisations can collect data for MI illegally or unethically to try and gain competitive advantages, this is known asindustrial espionage[35]. An example of illegal MI collection practice is when British Airways breached the Data Protect Act 1984 through accessing Virgin’s confidential flight details [36][9]
A standard of conduct was developed by the non-for-profit organisation Society of Competitive Intelligence Professionals, creating a code of ethics that can be adhered to by organisations when collecting market intelligence, to prevent the illegal and unethical collection of data and information.[9]
Dirty data that is collected needs to be cleansed to maintain good data quality. Challenges that arise in data cleansing is that there is a large volume of data being received leading to organisations being faced with many risks of failure to detect dirty data being processed through[37]. If data quality is not managed properly, it can result in financial losses, inefficient implementation of MI strategies and failure to comply with government regulations[38]. A reason for financial loss is due operational costs, as there is an increase in resources and time spent to identify and fix the dirty data[18].
MI processes have been used in many organisation’s strategic market planning, however, there are still difficulties in what the hard and soft benefits in using a MI process for an organisation[3]. The benefits of a successful MI process can be sectioned into three categories, better and faster decisions, time and cost savings and organisational learning and new ideas, however, overall, it can improve profitability and the competitiveness of an organisation.[3]
"
1026211301,2021-06-01 00:41:23,"Market intelligence (MI) is the information relevant to a company's market - trends, competitor and customer (existing, lost and targeted) monitoring, gathered and analysed[1]. It is a subtype of competitive intelligence (CI), is data and information gathered by companies that provides continuous insight into market trends such as competitors and customers values and preferences[1]. 
MI along with the marketing capabilities of an organisation provides a guideline into the allocation and implementation of resources and processes[2]. It is used for the purpose of continuously supplying strategic marketing planning for organisations to gauge marketing positions in order for companies to gain competitive advantage and best meet objectives[3][1].
Organisations can develop MI frameworks and models that are suited to financial capabilities and desired market sectors but are mainly based on the four-step process of collection, validation, processing and communication of MI[4]. The gathering of MI data is sorted into many different categories, including, but not limited to, qualitative, quantitative, formal, informal, published, and unpublished. MI data is gathered both internally and externally[5].
Benefits that MI can bring are that it provides customer, competitor and market insights allowing organisations to gain a competitive advantage in their marketing strategies[1]. Issues that MI can bring is through acquiring data and information through illegal or unethical ways, which can lead to financial loss and government regulatory failures[6].
MI and its broader term, marketing intelligence, was first introduced in “Marketing Intelligence for Top Management” by Kelley,[7] to provide information that was analysed, reliable and consistent for an organisation to better create policies and make business decisions[7].  
Following Kelley, in “How to Develop a Marketing Intelligence System”, R. Pinkerton shows the proactiveness of organisations as marketing intelligence systems is applied whilst the technological revolution arises[8].  Contributions to MI include professional organisations such as “Global Intelligence Alliance” and “the Society of Competitive Intelligence Professionals” (SCIP)[9]. These organisations have contributed both empirical and theoretical research in an attempt to further define and understand MI[9].  
As research into MI comes from scholars and non-scholars of different backgrounds it has resulted in a fragmented state of research. This has led to MI being used interchangeably with other market terms such as competitive intelligence, business intelligence and strategic intelligence[10]. MI to this current date continues to change to meet organisational requirements[7]. 
The implementation of MI varies depending on how organisations perceive it[11]. MI is defined as being comprised of three main activities, these activities are Information Acquisition, the gathering of marketing information that is required for current and future customer needs, Information Analysis which is the intelligence gained from the information collected and Information Activation, which is using the intelligence to implement and develop marketing plans[5]. 
Frameworks can be flexible, however the basis that organisations use to model the MI surrounds a four-step process, which are, collection, validation, processing and communication[4]. Data mining techniques are used throughout the processes to aid in the gathering and analysing of data and information retrieved[12].MI is a continuous process that organisations need to keep track of to improve their strategic and tactical marketing planning. These processes target the three activities that MI is defined by[5]. The model can be adjusted and adapted when required and can be implemented all at once or by sections.[11] 
Collection is the first step in the MI model, it involves the gathering of data and information of a particular market sector[11]. Such data and information can be gathered from external sources, such as other organisations and their market strategies, research institutes and business reports[11]. 
Internal factors can include looking into current strategy processes and personal customer trends[11]. It is estimated that 70% to 80% of intelligence resides within organisations employees or, internal MI network, as they are the team who gains information’s when interacting with suppliers, customers and other industry contacts[13]. To involve employees into an intelligence program to gain data and information the following considerations can be noted: developing a rewards program to promote participation, providing MI goals, requirements and a timeframe for information to be given in and creating a proper communication method to promote the intelligence program with employees such as using an e-mail system[14][15]. 
A challenge that arises in the collection of data and information is the identification of relevant information, this is a result of organisations not clearly defining a market sector[16].  
Validation is the second step in the MI model, this which can be referred to as data cleansing[11][18].The maintenance of good data quality is important as data and information is being retrieved from many different sources[18]. Data and information obtained from sources can be dirty, meaning that it is incomplete, wrong, inappropriate, duplicated[11]. This step will allow data and information to be adjusted and understandable to the organisation, furthermore it allows for consistency and compliance to be present.[11] If data quality is not maintained correctly it can lead to organisational losses with revenue and governmental regulation failures[6].
Data cleansing is a complex process that involves several stages in order to get good data quality for MI strategy use[18]. Stages include defining the organisation’s level of data quality, detecting error from the data collected and then repairing the errors[19].The five stages of data cleansing are data analysis to identify errors, eliminating the errors, checking to ensure elimination of error are done appropriately, refreshing the data in the data warehouse and finally replacing the dirty data with clean data[20].
Processing is the third step in the MI model. It involves the use of translating the clean data using organisational rules, modelling, logic and analysis to produce readable information, reports and spreadsheets that allows the organisation to gain specified knowledge.[11] The interpretation of data into readable information is difficult as it is complex, it requires proper technology and heavy commitment from a top organisational level to match data and information gained and align it to a marketing strategy[21].
Communication is the last step in the MI model. It involves the sharing, delivering and transmission of information gained from the processing step to figures in the organisation who will apply it accordingly to the market strategy[22]. As MI is a continuously changing, the communication of the MI strategy requires managers whom have expertise in the given market industry in order to determine the ongoing validity of the MI strategy and its implementation[23]. In order to make the communication of the MI strategy as successful as possible, this process must be performed by every level of an organisation, also known as the intelligence organisation[8].
Intelligence organisation refers to the “people and information resources who make the market intelligence process happen”[1]. The five elements of an intelligence organisation are, MI leadership who manages and leads the MI process, a MI team, a portfolio of external information sources that is set up by the MI team, internal MI network made up of MI users and the MI user’s personal information source network[1]. An intelligence organisation element is made up of external and internal factors that allows for a continuous MI process[1][8].
The gathering of MI data is different dependent on an organisation’s financial capabilities. Sources of data and information is separated into qualitative, quantitative, formal, informal, published and unpublished. With such sources being retrieved both internally and externally from the organisation[8]. It involves using search engines and corporate web sites to see competitor’s strategies, identifying business trends through reputable publications and existing customer clientele.[25] Organisations use different systems to gather MI, one system is that is used is Open-source Intelligence system.[26]
Sources of internal intelligence gathering is include but is not limited to, gathering data from customers, manufacturers, through research and development (R&D), employees, also known as salesforce, physical evidence, sales quotes, sales records, trade shows and new hires[27]. These data sources were ranked by organisations on a scale measuring five for being very important to one being not important. It was founded that customers and manufacturers and R&D are the most important to organisations with one hundred percent of organisations ranking these data sources with the number four or higher[28],. It shows that in the process of collection and gathering MI data and information, these data sources brought the most value to organisations.
Sources of external intelligence gathering is included but is not limited to, gathering data from client meetings, dealers/distributors, customers, business associates, market research projects, suppliers, on line services, periodicals and government publications[29]. These data sources were compared on the same scale as internal intelligence gathering sources, with results showing that intelligence gathered through client meetings being the most important to organisations, with one hundred percent of organisations ranking this data source with the number four or higher[30]. 
Marketing information systems allow for organisations to continuously acquire, generate, and maintain external and internal information[31]. They are systems that make use of artificial intelligence (AI) technology to aid in the planning of strategic and tactical marketing strategy of MI but also share marketing expertise[31].
Open-source intelligence is a predominant form of MI gathering that organisations employ. OSINT is defined as the “scanning, finding, gathering, exploitation, validation, analysis, and sharing with intelligence-seeking clients of publicly available print and digital/electronic data from unclassified, non-secret, and grey literature[32]. It is frequently used as its system is user friendly, its inexpensive and that it processes an abundant amount of raw materials that can be further processed.[33]
Using MI can bring to organisations both benefits and issues depending on how MI is acquired, maintained, and implemented. Benefits that MI can bring includes but is not limited to gaining competitive advantage in their marketing strategies[1]. Issues that MI can bring can include but is not limited to, financial losses and government regulatory failures[11].
There are issues that arises in the process of acquiring MI data and information and the implementation of an organisations marketing strategy. Issues such as the acquiring intelligence unethically and illegally can lead to failures with government regulations, also, if dirty data is not properly cleansed and problems aren’t mitigated or resolved can lead to a range of negative impacts that can result in financial and reputational losses to the organisation[11][6].
An issue that can arise is the unethical and illegal collection of data and information. Organisations can collect data for MI illegally or unethically to try and gain competitive advantages, this is known asindustrial espionage[35]. An example of illegal MI collection practice is when British Airways breached the Data Protect Act 1984 through accessing Virgin’s confidential flight details [36][11]
A standard of conduct was developed by the non-for-profit organisation Society of Competitive Intelligence Professionals, creating a code of ethics that can be adhered to by organisations when collecting market intelligence, to prevent the illegal and unethical collection of data and information.[11]
Dirty data that is collected needs to be cleansed to maintain good data quality. Challenges that arise in data cleansing is that there is a large volume of data being received leading to organisations being faced with many risks of failure to detect dirty data being processed through[37]. If data quality is not managed properly, it can result in financial losses, inefficient implementation of MI strategies and failure to comply with government regulations[38]. A reason for financial loss is due operational costs, as there is an increase in resources and time spent to identify and fix the dirty data[6].
MI processes have been used in many organisation’s strategic market planning, however, there are still difficulties in what the hard and soft benefits in using a MI process for an organisation[1]. The benefits of a successful MI process can be sectioned into three categories, better and faster decisions, time and cost savings and organisational learning and new ideas, however, overall, it can improve profitability and the competitiveness of an organisation.[1]
"
1026229786,2021-06-01 03:22:33,"Market intelligence (MI) is the information relevant to a company's market - trends, competitor and customer (existing, lost and targeted) monitoring, gathered and analysed[1]. It is a subtype of competitive intelligence (CI), is data and information gathered by companies that provides continuous insight into market trends such as competitors and customers values and preferences[1]. 
MI along with the marketing capabilities of an organisation provides a guideline into the allocation and implementation of resources and processes[2]. It is used for the purpose of continuously supplying strategic marketing planning for organisations to gauge marketing positions in order for companies to gain competitive advantage and best meet objectives[3][1].
Organisations can develop MI frameworks and models that are suited to financial capabilities and desired market sectors but are mainly based on the four-step process of collection, validation, processing and communication of MI[4]. The gathering of MI data is sorted into many different categories, including, but not limited to, qualitative, quantitative, formal, informal, published, and unpublished. MI data is gathered both internally and externally[5].
Benefits that MI can bring are that it provides customer, competitor and market insights allowing organisations to gain a competitive advantage in their marketing strategies[1]. Issues that MI can bring is through acquiring data and information through illegal or unethical ways, which can lead to financial loss and government regulatory failures[6].
MI and its broader term, marketing intelligence, was first introduced in “Marketing Intelligence for Top Management” by Kelley,[7] to provide information that was analysed, reliable and consistent for an organisation to better create policies and make business decisions[7].  
Following Kelley, in “How to Develop a Marketing Intelligence System”, R. Pinkerton shows the proactiveness of organisations as marketing intelligence systems is applied whilst the technological revolution arises[8].  Contributions to MI include professional organisations such as “Global Intelligence Alliance” and “the Society of Competitive Intelligence Professionals” (SCIP)[9]. These organisations have contributed both empirical and theoretical research in an attempt to further define and understand MI[9].  
As research into MI comes from scholars and non-scholars of different backgrounds it has resulted in a fragmented state of research. This has led to MI being used interchangeably with other market terms such as competitive intelligence, business intelligence and strategic intelligence[10]. MI to this current date continues to change to meet organisational requirements[7]. 
The implementation of MI varies depending on how organisations perceive it[11]. MI is defined as being composed of three main activities, these activities are Information Acquisition, the gathering of marketing information that is required for current and future customer needs, Information Analysis which is the intelligence gained from the information collected and Information Activation, which is using the intelligence to implement and develop marketing plans[5]. 
Frameworks can be flexible, however the basis that organisations use to model the MI surrounds a four-step process, which are, collection, validation, processing and communication[4]. Data mining techniques are used throughout the processes to aid in the gathering and analysing of data and information retrieved[12].MI is a continuous process that organisations need to keep track of to improve their strategic and tactical marketing planning. These processes target the three activities that MI is defined by[5]. The model can be adjusted and adapted when required and can be implemented all at once or by sections.[11] 
Collection is the first step in the MI model, it involves the gathering of data and information of a particular market sector[11]. Such data and information can be gathered from external sources, such as other organisations and their market strategies, research institutes and business reports[11]. 
Internal factors can include looking into current strategy processes and personal customer trends[11]. It is estimated that 70% to 80% of intelligence resides within organisations employees or, internal MI network, as they are the team who gains information’s when interacting with suppliers, customers and other industry contacts[13]. To involve employees into an intelligence program to gain data and information the following considerations can be noted: developing a rewards program to promote participation, providing MI goals, requirements and a timeframe for information to be given in and creating a proper communication method to promote the intelligence program with employees such as using an e-mail system[14][15]. 
A challenge that arises in the collection of data and information is the identification of relevant information, this is a result of organisations not clearly defining a market sector[16].  
Validation is the second step in the MI model, this which can be referred to as data cleansing[11][18].The maintenance of good data quality is important as data and information is being retrieved from many different sources[18]. Data and information obtained from sources can be dirty, meaning that it is incomplete, wrong, inappropriate, duplicated[11]. This step will allow data and information to be adjusted and understandable to the organisation, furthermore it allows for consistency and compliance to be present.[11] If data quality is not maintained correctly it can lead to organisational losses with revenue and governmental regulation failures[6].
Data cleansing is a complex process that involves several stages in order to get good data quality for MI strategy use[18]. Stages include defining the organisation’s level of data quality, detecting error from the data collected and then repairing the errors[19].The five stages of data cleansing are data analysis to identify errors, eliminating the errors, checking to ensure elimination of error are done appropriately, refreshing the data in the data warehouse and finally replacing the dirty data with clean data[20].
Processing is the third step in the MI model. It involves the use of translating the clean data using organisational rules, modelling, logic and analysis to produce readable information, reports and spreadsheets that allows the organisation to gain specified knowledge.[11] The interpretation of data into readable information is difficult as it is complex, it requires proper technology and heavy commitment from a top organisational level to match data and information gained and align it to a marketing strategy[21].
Communication is the last step in the MI model. It involves the sharing, delivering and transmission of information gained from the processing step to figures in the organisation who will apply it accordingly to the market strategy[22]. As MI is a continuously changing, the communication of the MI strategy requires managers whom have expertise in the given market industry in order to determine the ongoing validity of the MI strategy and its implementation[23]. In order to make the communication of the MI strategy as successful as possible, this process must be performed by every level of an organisation, also known as the intelligence organisation[8].
Intelligence organisation refers to the “people and information resources who make the market intelligence process happen”[1]. The five elements of an intelligence organisation are, MI leadership who manages and leads the MI process, a MI team, a portfolio of external information sources that is set up by the MI team, internal MI network made up of MI users and the MI user’s personal information source network[1]. An intelligence organisation element is made up of external and internal factors that allows for a continuous MI process[1][8].
The gathering of MI data is different dependent on an organisation’s financial capabilities. Sources of data and information is separated into qualitative, quantitative, formal, informal, published and unpublished. With such sources being retrieved both internally and externally from the organisation[8]. It involves using search engines and corporate web sites to see competitor’s strategies, identifying business trends through reputable publications and existing customer clientele.[25] Organisations use different systems to gather MI, one system is that is used is Open-source Intelligence system.[26]
Sources of internal intelligence gathering is include but is not limited to, gathering data from customers, manufacturers, through research and development (R&D), employees, also known as salesforce, physical evidence, sales quotes, sales records, trade shows and new hires[27]. These data sources were ranked by organisations on a scale measuring five for being very important to one being not important. It was founded that customers and manufacturers and R&D are the most important to organisations with one hundred percent of organisations ranking these data sources with the number four or higher[28],. It shows that in the process of collection and gathering MI data and information, these data sources brought the most value to organisations.
Sources of external intelligence gathering is included but is not limited to, gathering data from client meetings, dealers/distributors, customers, business associates, market research projects, suppliers, on line services, periodicals and government publications[29]. These data sources were compared on the same scale as internal intelligence gathering sources, with results showing that intelligence gathered through client meetings being the most important to organisations, with one hundred percent of organisations ranking this data source with the number four or higher[30]. 
Marketing information systems allow for organisations to continuously acquire, generate, and maintain external and internal information[31]. They are systems that make use of artificial intelligence (AI) technology to aid in the planning of strategic and tactical marketing strategy of MI but also share marketing expertise[31].
Open-source intelligence is a predominant form of MI gathering that organisations employ. OSINT is defined as the “scanning, finding, gathering, exploitation, validation, analysis, and sharing with intelligence-seeking clients of publicly available print and digital/electronic data from unclassified, non-secret, and grey literature[32]. It is frequently used as its system is user friendly, its inexpensive and that it processes an abundant amount of raw materials that can be further processed.[33]
Using MI can bring to organisations both benefits and issues depending on how MI is acquired, maintained, and implemented. Benefits that MI can bring includes but is not limited to gaining competitive advantage in their marketing strategies[1]. Issues that MI can bring can include but is not limited to, financial losses and government regulatory failures[11].
There are issues that arises in the process of acquiring MI data and information and the implementation of an organisations marketing strategy. Issues such as the acquiring intelligence unethically and illegally can lead to failures with government regulations, also, if dirty data is not properly cleansed and problems aren’t mitigated or resolved can lead to a range of negative impacts that can result in financial and reputational losses to the organisation[11][6].
An issue that can arise is the unethical and illegal collection of data and information. Organisations can collect data for MI illegally or unethically to try and gain competitive advantages, this is known asindustrial espionage[35]. An example of illegal MI collection practice is when British Airways breached the Data Protect Act 1984 through accessing Virgin’s confidential flight details [36][11]
A standard of conduct was developed by the non-for-profit organisation Society of Competitive Intelligence Professionals, creating a code of ethics that can be adhered to by organisations when collecting market intelligence, to prevent the illegal and unethical collection of data and information.[11]
Dirty data that is collected needs to be cleansed to maintain good data quality. Challenges that arise in data cleansing is that there is a large volume of data being received leading to organisations being faced with many risks of failure to detect dirty data being processed through[37]. If data quality is not managed properly, it can result in financial losses, inefficient implementation of MI strategies and failure to comply with government regulations[38]. A reason for financial loss is due operational costs, as there is an increase in resources and time spent to identify and fix the dirty data[6].
MI processes have been used in many organisation’s strategic market planning, however, there are still difficulties in what the hard and soft benefits in using a MI process for an organisation[1]. The benefits of a successful MI process can be sectioned into three categories, better and faster decisions, time and cost savings and organisational learning and new ideas, however, overall, it can improve profitability and the competitiveness of an organisation.[1]
"
1026470342,2021-06-02 13:21:59,"Market intelligence (MI) is the information relevant to a company's market - trends, competitor and customer (existing, lost and targeted) monitoring, gathered and analysed[1]. It is a subtype of competitive intelligence (CI), is data and information gathered by companies that provides continuous insight into market trends such as competitors and customers values and preferences[1]. 
MI along with the marketing capabilities of an organisation provides a guideline into the allocation and implementation of resources and processes[2]. It is used for the purpose of continuously supplying strategic marketing planning for organisations to gauge marketing positions in order for companies to gain competitive advantage and best meet objectives[3][1].
Organisations can develop MI frameworks and models that are suited to financial capabilities and desired market sectors but are mainly based on the four-step process of collection, validation, processing and communication of MI[4]. The gathering of MI data is sorted into many different categories, including, but not limited to, qualitative, quantitative, formal, informal, published, and unpublished[5]. MI data is gathered both internally and externally[5].
Benefits that MI can bring are that it provides customer, competitor and market insights allowing organisations to gain a competitive advantage in their marketing strategies[1]. Issues that MI can bring is through acquiring data and information through illegal or unethical ways, it can lead to financial loss and government regulatory failures[6].
MI and its broader term, marketing intelligence, was first introduced in “Marketing Intelligence for Top Management” by Kelley,[7] to provide information that was analysed, reliable and consistent for an organisation to better create policies and make business decisions[7].  
Following Kelley, in “How to Develop a Marketing Intelligence System”, R. Pinkerton shows the proactiveness of organisations as marketing intelligence systems is applied whilst the technological revolution arises[8].  Contributions to MI include professional organisations such as “Global Intelligence Alliance” and “the Society of Competitive Intelligence Professionals” (SCIP)[9]. These organisations have contributed both empirical and theoretical research in an attempt to further define and understand MI[9].  
As research into MI comes from scholars and non-scholars of different backgrounds it has resulted in a fragmented state of research. This has led to MI being used interchangeably with other market terms such as competitive intelligence, business intelligence and strategic intelligence[10]. MI to this current date continues to change to meet organisational requirements[7]. 
The implementation of MI varies depending on how organisations perceive it[11]. MI is defined as being composed of three main activities, these activities are Information Acquisition, the gathering of marketing information that is required for current and future customer needs, Information Analysis which is the intelligence gained from the information collected and Information Activation, which is using the intelligence to implement and develop marketing plans[5]. 
Frameworks can be flexible, however the basis that organisations use to model the MI surrounds a four-step process, which are, collection, validation, processing and communication[4]. Data mining techniques are used throughout the processes to aid in the gathering and analysing of data and information retrieved[12].MI is a continuous process that organisations need to keep track of to improve their strategic and tactical marketing planning. These processes target the three activities that MI is defined by[5]. The model can be adjusted and adapted when required and can be implemented all at once or by sections[11]. 
Collection is the first step in the MI model, it involves the gathering of data and information of a particular market sector[11]. Such data and information can be gathered from external sources, such as other organisations and their market strategies, research institutes and business reports[11]. 
Internal factors can include looking into current strategy processes and personal customer trends[11]. It is estimated that 70% to 80% of intelligence resides within organisations employees or, internal MI network, as they are the team who gains information’s when interacting with suppliers, customers and other industry contacts[13]. To involve employees into an intelligence program to gain data and information the following considerations can be noted: developing a rewards program to promote participation, providing MI goals, requirements and a timeframe for information to be given in and creating a proper communication method to promote the intelligence program with employees such as using an e-mail system[14][15]. 
A challenge that arises in the collection of data and information is the identification of relevant information, this is a result of organisations not clearly defining a market sector[16].  
Validation is the second step in the MI model, this which can be referred to as data cleansing[11][18].The maintenance of good data quality is important as data and information is being retrieved from many different sources[18]. Data and information obtained from sources can be dirty, meaning that it is incomplete, wrong, inappropriate, duplicated[11]. This step will allow data and information to be adjusted and understandable to the organisation, furthermore it allows for consistency and compliance to be present.[11] If data quality is not maintained correctly it can lead to organisational losses with revenue and governmental regulation failures[6].
Data cleansing is a complex process that involves several stages in order to get good data quality for MI strategy use[18]. Stages include defining the organisation’s level of data quality, detecting error from the data collected and then repairing the errors[19].The five stages of data cleansing are data analysis to identify errors, eliminating the errors, checking to ensure elimination of error are done appropriately, refreshing the data in the data warehouse and finally replacing the dirty data with clean data[20].
Processing is the third step in the MI model[4]. It involves the use of translating the clean data using organisational rules, modelling, logic and analysis to produce readable information, reports and spreadsheets that allows the organisation to gain specified knowledge.[11] The interpretation of data into readable information is difficult as it is complex, it requires proper technology and heavy commitment from a top organisational level to match data and information gained and align it to a marketing strategy[21].
Communication is the last step in the MI model. It involves the sharing, delivering and transmission of information gained from the processing step to figures in the organisation who will apply it accordingly to the market strategy[22]. As MI is a continuously changing, the communication of the MI strategy requires managers whom have expertise in the given market industry in order to determine the ongoing validity of the MI strategy and its implementation[23]. In order to make the communication of the MI strategy as successful as possible, this process must be performed by every level of an organisation, also known as the intelligence organisation[8].
Intelligence organisation refers to the “people and information resources who make the market intelligence process happen”[1]. The five elements of an intelligence organisation are, MI leadership who manages and leads the MI process, a MI team, a portfolio of external information sources that is set up by the MI team, internal MI network made up of MI users and the MI user’s personal information source network[1]. An intelligence organisation element is made up of external and internal factors that allows for a continuous MI process[1][8].
The gathering of MI data is different dependent on an organisation’s financial capabilities. Sources of data and information is separated into qualitative, quantitative, formal, informal, published and unpublished. With such sources being retrieved both internally and externally from the organisation[8]. It involves using search engines and corporate web sites to see competitor’s strategies, identifying business trends through reputable publications and existing customer clientele.[25] Organisations use different systems to gather MI, one system is that is used is Open-source Intelligence system.[26]
Sources of internal intelligence gathering is include but is not limited to, gathering data from customers, manufacturers, through research and development (R&D), employees, also known as salesforce, physical evidence, sales quotes, sales records, trade shows and new hires[27]. These data sources were ranked by organisations on a scale measuring five for being very important to one being not important. It was founded that customers and manufacturers and R&D are the most important to organisations with one hundred percent of organisations ranking these data sources with the number four or higher[28],. It shows that in the process of collection and gathering MI data and information, these data sources brought the most value to organisations.
Sources of external intelligence gathering is included but is not limited to, gathering data from client meetings, dealers/distributors, customers, business associates, market research projects, suppliers, on line services, periodicals and government publications[29]. These data sources were compared on the same scale as internal intelligence gathering sources, with results showing that intelligence gathered through client meetings being the most important to organisations, with one hundred percent of organisations ranking this data source with the number four or higher[30]. 
Marketing information systems allow for organisations to continuously acquire, generate, and maintain external and internal information[31]. They are systems that make use of artificial intelligence (AI) technology to aid in the planning of strategic and tactical marketing strategy of MI but also share marketing expertise[31].
Open-source intelligence is a predominant form of MI gathering that organisations employ. OSINT is defined as the scanning, finding, gathering, exploitation, validation, analysis, and sharing with intelligence-seeking clients of publicly available print and digital/electronic data from unclassified, non-secret, and grey literature[32]. It is frequently used as its system is user friendly, its inexpensive and that it processes an abundant amount of raw materials that can be further processed.[33]
Using MI can bring to organisations both benefits and issues depending on how MI is acquired, maintained, and implemented. Benefits that MI can bring includes but is not limited to gaining competitive advantage in their marketing strategies[1]. Issues that MI can bring can include but is not limited to, financial losses and government regulatory failures[11].
There are issues that arises in the process of acquiring MI data and information and the implementation of an organisations marketing strategy. Issues such as the acquiring intelligence unethically and illegally can lead to failures with government regulations, also, if dirty data is not properly cleansed and problems aren’t mitigated or resolved can lead to a range of negative impacts that can result in financial and reputational losses to the organisation[11][6].
An issue that can arise is the unethical and illegal collection of data and information. Organisations can collect data for MI illegally or unethically to try and gain competitive advantages, this is known as industrial espionage[35]. An example of illegal MI collection practice is when British Airways breached the Data Protect Act 1984 through accessing Virgin’s confidential flight details [36][11].
A standard of conduct was developed by the non-for-profit organisation Society of Competitive Intelligence Professionals, creating a code of ethics that can be adhered to by organisations when collecting market intelligence, to prevent the illegal and unethical collection of data and information.[11]
Dirty data that is collected needs to be cleansed to maintain good data quality. Challenges that arise in data cleansing is that there is a large volume of data being received leading to organisations being faced with many risks of failure to detect dirty data being processed through[37]. If data quality is not managed properly, it can result in financial losses, inefficient implementation of MI strategies and failure to comply with government regulations[38]. A reason for financial loss is due operational costs, as there is an increase in resources and time spent to identify and fix the dirty data[6].
MI processes have been used in many organisation’s strategic market planning, however, there are still difficulties in what the hard and soft benefits in using a MI process for an organisation[1]. The benefits of a successful MI process can be sectioned into three categories, better and faster decisions, time and cost savings and organisational learning and new ideas, however, overall, it can improve profitability and the competitiveness of an organisation[1].
"
1026472216,2021-06-02 13:37:33,"Market intelligence (MI) is the information relevant to a company's market - trends, competitor and customer (existing, lost and targeted) monitoring, gathered and analysed[1]. It is a subtype of competitive intelligence (CI), is data and information gathered by companies that provides continuous insight into market trends such as competitors and customers values and preferences[1]. 
MI along with the marketing capabilities of an organisation provides a guideline into the allocation and implementation of resources and processes[2]. It is used for the purpose of continuously supplying strategic marketing planning for organisations to gauge marketing positions in order for companies to gain competitive advantage and best meet objectives[3][1].
Organisations can develop MI frameworks and models that are suited to financial capabilities and desired market sectors but are mainly based on the four-step process of collection, validation, processing and communication of MI[4]. The gathering of MI data is sorted into many different categories, including, but not limited to, qualitative, quantitative, formal, informal, published, and unpublished[5]. MI data is gathered both internally and externally[5].
Benefits that MI can bring are that it provides customer, competitor and market insights allowing organisations to gain a competitive advantage in their marketing strategies[1]. Issues that MI can bring is through acquiring data and information through illegal or unethical ways, it can lead to financial loss and government regulatory failures[6].
MI and its broader term, marketing intelligence, was first introduced in “Marketing Intelligence for Top Management” by Kelley,[7] to provide information that was analysed, reliable and consistent for an organisation to better create policies and make business decisions[7].  
Following Kelley, in “How to Develop a Marketing Intelligence System”, R. Pinkerton shows the proactiveness of organisations as marketing intelligence systems is applied whilst the technological revolution arises[8].  Contributions to MI include professional organisations such as “Global Intelligence Alliance” and “the Society of Competitive Intelligence Professionals” (SCIP)[9]. These organisations have contributed both empirical and theoretical research in an attempt to further define and understand MI[9].  
As research into MI comes from scholars and non-scholars of different backgrounds it has resulted in a fragmented state of research. This has led to MI being used interchangeably with other market terms such as competitive intelligence, business intelligence and strategic intelligence[10]. MI to this current date continues to change to meet organisational requirements[7]. 
The implementation of MI varies depending on how organisations perceive it[11]. MI is defined as being composed of three main activities, these activities are Information Acquisition, the gathering of marketing information that is required for current and future customer needs, Information Analysis which is the intelligence gained from the information collected and Information Activation, which is using the intelligence to implement and develop marketing plans[5]. 
Frameworks can be flexible, however the basis that organisations use to model the MI surrounds a four-step process, which are, collection, validation, processing and communication[4]. Data mining techniques are used throughout the processes to aid in the gathering and analysing of data and information retrieved[12].MI is a continuous process that organisations need to keep track of to improve their strategic and tactical marketing planning. These processes target the three activities that MI is defined by[5]. The model can be adjusted and adapted when required and can be implemented all at once or by sections[11]. 
Collection is the first step in the MI model, it involves the gathering of data and information of a particular market sector[11]. Such data and information can be gathered from external sources, such as other organisations and their market strategies, research institutes and business reports[11]. 
Internal factors can include looking into current strategy processes and personal customer trends[11]. It is estimated that 70% to 80% of intelligence resides within organisations employees or, internal MI network, as they are the team who gains information’s when interacting with suppliers, customers and other industry contacts[13]. To involve employees into an intelligence program to gain data and information the following considerations can be noted: developing a rewards program to promote participation, providing MI goals, requirements and a timeframe for information to be given in and creating a proper communication method to promote the intelligence program with employees such as using an e-mail system[14][15]. 
A challenge that arises in the collection of data and information is the identification of relevant information, this is a result of organisations not clearly defining a market sector[16].  
Validation is the second step in the MI model, this which can be referred to as data cleansing[11][18].The maintenance of good data quality is important as data and information is being retrieved from many different sources[18]. Data and information obtained from sources can be dirty, meaning that it is incomplete, wrong, inappropriate, duplicated[11]. This step will allow data and information to be adjusted and understandable to the organisation, furthermore it allows for consistency and compliance to be present.[11] If data quality is not maintained correctly it can lead to organisational losses with revenue and governmental regulation failures[6].
Data cleansing is a complex process that involves several stages in order to get good data quality for MI strategy use[18]. Stages include defining the organisation’s level of data quality, detecting error from the data collected and then repairing the errors[19].The five stages of data cleansing are data analysis to identify errors, eliminating the errors, checking to ensure elimination of error are done appropriately, refreshing the data in the data warehouse and finally replacing the dirty data with clean data[20].
Processing is the third step in the MI model[4]. It involves the use of translating the clean data using organisational rules, modelling, logic and analysis to produce readable information, reports and spreadsheets that allows the organisation to gain specified knowledge.[11] The interpretation of data into readable information is difficult as it is complex, it requires proper technology and heavy commitment from a top organisational level to match data and information gained and align it to a marketing strategy[21].
Communication is the last step in the MI model. It involves the sharing, delivering and transmission of information gained from the processing step to figures in the organisation who will apply it accordingly to the market strategy[22]. As MI is a continuously changing, the communication of the MI strategy requires managers whom have expertise in the given market industry in order to determine the ongoing validity of the MI strategy and its implementation[23]. In order to make the communication of the MI strategy as successful as possible, this process must be performed by every level of an organisation, also known as the intelligence organisation[8].
Intelligence organisation refers to the “people and information resources who make the market intelligence process happen”[1]. The five elements of an intelligence organisation are, MI leadership who manages and leads the MI process, a MI team, a portfolio of external information sources that is set up by the MI team, internal MI network made up of MI users and the MI user’s personal information source network[1]. An intelligence organisation element is made up of external and internal factors that allows for a continuous MI process[1][8].
The gathering of MI data is different dependent on an organisation’s financial capabilities. Sources of data and information is separated into qualitative, quantitative, formal, informal, published and unpublished. With such sources being retrieved both internally and externally from the organisation[8]. It involves using search engines and corporate web sites to see competitor’s strategies, identifying business trends through reputable publications and existing customer clientele.[25] Organisations use different systems to gather MI, one system is that is used is Open-source Intelligence system.[26]
Sources of internal intelligence gathering is include but is not limited to, gathering data from customers, manufacturers, through research and development (R&D), employees, also known as salesforce, physical evidence, sales quotes, sales records, trade shows and new hires[27]. These data sources were ranked by organisations on a scale measuring five for being very important to one being not important. It was founded that customers and manufacturers and R&D are the most important to organisations with one hundred percent of organisations ranking these data sources with the number four or higher[28],. It shows that in the process of collection and gathering MI data and information, these data sources brought the most value to organisations.
Sources of external intelligence gathering is included but is not limited to, gathering data from client meetings, dealers/distributors, customers, business associates, market research projects, suppliers, on line services, periodicals and government publications[29]. These data sources were compared on the same scale as internal intelligence gathering sources, with results showing that intelligence gathered through client meetings being the most important to organisations, with one hundred percent of organisations ranking this data source with the number four or higher[30]. 
Marketing information systems allow for organisations to continuously acquire, generate, and maintain external and internal information[31]. They are systems that make use of artificial intelligence (AI) technology to aid in the planning of strategic and tactical marketing strategy of MI but also share marketing expertise[31].
Open-source intelligence is a predominant form of MI gathering that organisations employ. OSINT is defined as the scanning, finding, gathering, exploitation, validation, analysis, and sharing with intelligence-seeking clients of publicly available print and digital/electronic data from unclassified, non-secret, and grey literature[32]. It is frequently used as its system is user friendly, its inexpensive and that it processes an abundant amount of raw materials that can be further processed.[33]
Using MI can bring to organisations both benefits and issues depending on how MI is acquired, maintained, and implemented. Benefits that MI can bring includes but is not limited to gaining competitive advantage in their marketing strategies[1]. Issues that MI can bring can include but is not limited to, financial losses and government regulatory failures[11].
There are issues that arises in the process of acquiring MI data and information and the implementation of an organisations marketing strategy. Issues such as the acquiring intelligence unethically and illegally can lead to failures with government regulations, also, if dirty data is not properly cleansed and problems aren’t mitigated or resolved can lead to a range of negative impacts that can result in financial and reputational losses to the organisation[11][6].
An issue that can arise is the unethical and illegal collection of data and information. Organisations can collect data for MI illegally or unethically to try and gain competitive advantages, this is known as industrial espionage[35]. An example of illegal MI collection practice is when British Airways breached the Data Protect Act 1984 through accessing Virgin’s confidential flight details [36][11].
A standard of conduct was developed by the non-for-profit organisation Society of Competitive Intelligence Professionals, creating a code of ethics that can be adhered to by organisations when collecting market intelligence, to prevent the illegal and unethical collection of data and information.[11]
Dirty data that is collected needs to be cleansed to maintain good data quality. Challenges that arise in data cleansing is that there is a large volume of data being received leading to organisations being faced with many risks of failure to detect dirty data being processed through[37]. If data quality is not managed properly, it can result in financial losses, inefficient implementation of MI strategies and failure to comply with government regulations[38]. A reason for financial loss is due operational costs, as there is an increase in resources and time spent to identify and fix the dirty data[6].
MI processes have been used in many organisation’s strategic market planning, however, there are still difficulties in what the hard and soft benefits in using a MI process for an organisation[1]. The benefits of a successful MI process can be sectioned into three categories, better and faster decisions, time and cost savings and organisational learning and new ideas, however, overall, it can improve profitability and the competitiveness of an organisation[1]. The competitiveness of an organisation increases as with more MI gathered it'll provide a way for organisations to innovate through improving current methods and increasing the ability to find and create new products[39].
"
1026566001,2021-06-03 01:18:02,"Market intelligence (MI) is the information relevant to a company's market - trends, competitor and customer (existing, lost and targeted) monitoring, gathered and analysed.[1] It is a subtype of competitive intelligence (CI), is data and information gathered by companies that provides continuous insight into market trends such as competitors and customers values and preferences.[1] 
MI along with the marketing capabilities of an organisation provides a guideline into the allocation and implementation of resources and processes.[2] It is used for the purpose of continuously supplying strategic marketing planning for organisations to gauge marketing positions in order for companies to gain competitive advantage and best meet objectives.[3][1]
Organisations can develop MI frameworks and models that are suited to financial capabilities and desired market sectors but are mainly based on the four-step process of collection, validation, processing and communication of MI.[4] The gathering of MI data is sorted into many different categories, including, but not limited to, qualitative, quantitative, formal, informal, published, and unpublished.[5] MI data is gathered both internally and externally.[5]
Benefits that MI can bring are that it provides customer, competitor and market insights allowing organisations to gain a competitive advantage in their marketing strategies.[1] Issues that MI can bring is through acquiring data and information through illegal or unethical ways, it can lead to financial loss and government regulatory failures.[6]
MI and its broader term, marketing intelligence, was first introduced in “Marketing Intelligence for Top Management” by Kelley,[7] to provide information that was analysed, reliable and consistent for an organisation to better create policies and make business decisions.[7]  
Following Kelley, in “How to Develop a Marketing Intelligence System”, R. Pinkerton shows the proactiveness of organisations as marketing intelligence systems is applied whilst the technological revolution arises.[8]  Contributions to MI include professional organisations such as “Global Intelligence Alliance” and “the Society of Competitive Intelligence Professionals” (SCIP).[9] These organisations have contributed both empirical and theoretical research in an attempt to further define and understand MI.[9]  
As research into MI comes from scholars and non-scholars of different backgrounds it has resulted in a fragmented state of research. This has led to MI being used interchangeably with other market terms such as competitive intelligence, business intelligence and strategic intelligence.[10] MI to this current date continues to change to meet organisational requirements.[7] 
The implementation of MI varies depending on how organisations perceive it.[11] MI is defined as being composed of three main activities, these activities are Information Acquisition, the gathering of marketing information that is required for current and future customer needs, Information Analysis which is the intelligence gained from the information collected and Information Activation, which is using the intelligence to implement and develop marketing plans.[5] 
Frameworks can be flexible, however the basis that organisations use to model the MI surrounds a four-step process, which are, collection, validation, processing and communication.[4] Data mining techniques are used throughout the processes to aid in the gathering and analysing of data and information retrieved[12].MI is a continuous process that organisations need to keep track of to improve their strategic and tactical marketing planning. These processes target the three activities that MI is defined by.[5] The model can be adjusted and adapted when required and can be implemented all at once or by sections.[11] 
Collection is the first step in the MI model, it involves the gathering of data and information of a particular market sector.[11] Such data and information can be gathered from external sources, such as other organisations and their market strategies, research institutes and business reports.[11] 
Internal factors can include looking into current strategy processes and personal customer trends.[11] It is estimated that 70% to 80% of intelligence resides within organisations employees or, internal MI network, as they are the team who gains information’s when interacting with suppliers, customers and other industry contacts.[13] To involve employees into an intelligence program to gain data and information the following considerations can be noted: developing a rewards program to promote participation, providing MI goals, requirements and a timeframe for information to be given in and creating a proper communication method to promote the intelligence program with employees such as using an e-mail system.[14][15] 
A challenge that arises in the collection of data and information is the identification of relevant information, this is a result of organisations not clearly defining a market sector.[16]  
Validation is the second step in the MI model, this which can be referred to as data cleansing[11][18].The maintenance of good data quality is important as data and information is being retrieved from many different sources.[18] Data and information obtained from sources can be dirty, meaning that it is incomplete, wrong, inappropriate, duplicated.[11] This step will allow data and information to be adjusted and understandable to the organisation, furthermore it allows for consistency and compliance to be present.[11] If data quality is not maintained correctly it can lead to organisational losses with revenue and governmental regulation failures.[6]
Data cleansing is a complex process that involves several stages in order to get good data quality for MI strategy use.[18] Stages include defining the organisation’s level of data quality, detecting error from the data collected and then repairing the errors[19].The five stages of data cleansing are data analysis to identify errors, eliminating the errors, checking to ensure elimination of error are done appropriately, refreshing the data in the data warehouse and finally replacing the dirty data with clean data.[20]
Processing is the third step in the MI model.[4] It involves the use of translating the clean data using organisational rules, modelling, logic and analysis to produce readable information, reports and spreadsheets that allows the organisation to gain specified knowledge.[11] The interpretation of data into readable information is difficult as it is complex, it requires proper technology and heavy commitment from a top organisational level to match data and information gained and align it to a marketing strategy.[21]
Communication is the last step in the MI model. It involves the sharing, delivering and transmission of information gained from the processing step to figures in the organisation who will apply it accordingly to the market strategy[22]. As MI is a continuously changing, the communication of the MI strategy requires managers whom have expertise in the given market industry in order to determine the ongoing validity of the MI strategy and its implementation.[23] In order to make the communication of the MI strategy as successful as possible, this process must be performed by every level of an organisation, also known as the intelligence organisation.[8]
Intelligence organisation refers to the “people and information resources who make the market intelligence process happen”.[1] The five elements of an intelligence organisation are, MI leadership who manages and leads the MI process, a MI team, a portfolio of external information sources that is set up by the MI team, internal MI network made up of MI users and the MI user’s personal information source network.[1] An intelligence organisation element is made up of external and internal factors that allows for a continuous MI process.[1][8]
The gathering of MI data is different dependent on an organisation’s financial capabilities. Sources of data and information is separated into qualitative, quantitative, formal, informal, published and unpublished. With such sources being retrieved both internally and externally from the organisation.[8] It involves using search engines and corporate web sites to see competitor’s strategies, identifying business trends through reputable publications and existing customer clientele.[25] Organisations use different systems to gather MI, one system is that is used is Open-source Intelligence system.[26]
Sources of internal intelligence gathering is include but is not limited to, gathering data from customers, manufacturers, through research and development (R&D), employees, also known as salesforce, physical evidence, sales quotes, sales records, trade shows and new hires.[27] These data sources were ranked by organisations on a scale measuring five for being very important to one being not important. It was founded that customers and manufacturers and R&D are the most important to organisations with one hundred percent of organisations ranking these data sources with the number four or higher[28],. It shows that in the process of collection and gathering MI data and information, these data sources brought the most value to organisations.
Sources of external intelligence gathering is included but is not limited to, gathering data from client meetings, dealers/distributors, customers, business associates, market research projects, suppliers, on line services, periodicals and government publications.[29] These data sources were compared on the same scale as internal intelligence gathering sources, with results showing that intelligence gathered through client meetings being the most important to organisations, with one hundred percent of organisations ranking this data source with the number four or higher.[30] 
Marketing information systems allow for organisations to continuously acquire, generate, and maintain external and internal information.[31] They are systems that make use of artificial intelligence (AI) technology to aid in the planning of strategic and tactical marketing strategy of MI but also share marketing expertise.[31]
Open-source intelligence is a predominant form of MI gathering that organisations employ. OSINT is defined as the scanning, finding, gathering, exploitation, validation, analysis, and sharing with intelligence-seeking clients of publicly available print and digital/electronic data from unclassified, non-secret, and grey literature.[32] It is frequently used as its system is user friendly, its inexpensive and that it processes an abundant amount of raw materials that can be further processed.[33]
Using MI can bring to organisations both benefits and issues depending on how MI is acquired, maintained, and implemented. Benefits that MI can bring includes but is not limited to gaining competitive advantage in their marketing strategies.[1] Issues that MI can bring can include but is not limited to, financial losses and government regulatory failures.[11]
There are issues that arises in the process of acquiring MI data and information and the implementation of an organisations marketing strategy. Issues such as the acquiring intelligence unethically and illegally can lead to failures with government regulations, also, if dirty data is not properly cleansed and problems aren’t mitigated or resolved can lead to a range of negative impacts that can result in financial and reputational losses to the organisation.[11][6]
An issue that can arise is the unethical and illegal collection of data and information. Organisations can collect data for MI illegally or unethically to try and gain competitive advantages, this is known as industrial espionage.[35] An example of illegal MI collection practice is when British Airways breached the Data Protect Act 1984 through accessing Virgin’s confidential flight details.[36][11]
A standard of conduct was developed by the non-for-profit organisation Society of Competitive Intelligence Professionals, creating a code of ethics that can be adhered to by organisations when collecting market intelligence, to prevent the illegal and unethical collection of data and information.[11]
Dirty data that is collected needs to be cleansed to maintain good data quality. Challenges that arise in data cleansing is that there is a large volume of data being received leading to organisations being faced with many risks of failure to detect dirty data being processed through.[37] If data quality is not managed properly, it can result in financial losses, inefficient implementation of MI strategies and failure to comply with government regulations.[38] A reason for financial loss is due operational costs, as there is an increase in resources and time spent to identify and fix the dirty data.[6]
MI processes have been used in many organisation’s strategic market planning, however, there are still difficulties in what the hard and soft benefits in using a MI process for an organisation.[1] The benefits of a successful MI process can be sectioned into three categories, better and faster decisions, time and cost savings and organisational learning and new ideas, however, overall, it can improve profitability and the competitiveness of an organisation.[1] The competitiveness of an organisation increases as with more MI gathered it'll provide a way for organisations to innovate through improving current methods and increasing the ability to find and create new products.[39]
"
1027266569,2021-06-07 00:31:42,"Market intelligence (MI) is the information relevant to a company's market - trends, competitor and customer (existing, lost and targeted) monitoring, gathered and analysed.[1] It is a subtype of competitive intelligence (CI), is data and information gathered by companies that provides continuous insight into market trends such as competitors and customers values and preferences.[1] 
MI along with the marketing capabilities of an organisation provides a guideline into the allocation and implementation of resources and processes.[2] It is used for the purpose of continuously supplying strategic marketing planning for organisations to gauge marketing positions in order for companies to gain competitive advantage and best meet objectives.[3][1]
Organisations can develop MI frameworks and models that are suited to financial capabilities and desired market sectors but are mainly based on the four-step process of collection, validation, processing and communication of MI.[4] The gathering of MI data is sorted into many different categories, including, but not limited to, qualitative, quantitative, formal, informal, published, and unpublished.[5] MI data is gathered both internally and externally.[5]
Benefits that MI can bring are that it provides customer, competitor and market insights allowing organisations to gain a competitive advantage in their marketing strategies.[1] Issues that MI can bring is through acquiring data and information through illegal or unethical ways, it can lead to financial loss and government regulatory failures.[6]
MI and its broader term, marketing intelligence, was first introduced in “Marketing Intelligence for Top Management” by Kelley,[7] to provide information that was analysed, reliable and consistent for an organisation to better create policies and make business decisions.[7]  
Following Kelley, in “How to Develop a Marketing Intelligence System”, R. Pinkerton shows the proactiveness of organisations as marketing intelligence systems is applied whilst the technological revolution arises.[8]  Contributions to MI include professional organisations such as “Global Intelligence Alliance” and “the Society of Competitive Intelligence Professionals” (SCIP).[9] These organisations have contributed both empirical and theoretical research in an attempt to further define and understand MI.[9]  
As research into MI comes from scholars and non-scholars of different backgrounds it has resulted in a fragmented state of research. This has led to MI being used interchangeably with other market terms such as competitive intelligence, business intelligence and strategic intelligence.[10] MI to this current date continues to change to meet organisational requirements.[7] 
The implementation of MI varies depending on how organisations perceive it.[11] MI is defined as being composed of three main activities, these activities are Information Acquisition, the gathering of marketing information that is required for current and future customer needs, Information Analysis which is the intelligence gained from the information collected and Information Activation, which is using the intelligence to implement and develop marketing plans.[5] 
Frameworks can be flexible, however the basis that organisations use to model the MI surrounds a four-step process, which are, collection, validation, processing and communication.[4] Data mining techniques are used throughout the processes to aid in the gathering and analysing of data and information retrieved[12].MI is a continuous process that organisations need to keep track of to improve their strategic and tactical marketing planning. These processes target the three activities that MI is defined by.[5] The model can be adjusted and adapted when required and can be implemented all at once or by sections.[11] 
Collection is the first step in the MI model, it involves the gathering of data and information of a particular market sector.[11] Such data and information can be gathered from external sources, such as other organisations and their market strategies, research institutes and business reports.[11] 
Internal factors can include looking into current strategy processes and personal customer trends.[11] It is estimated that 70% to 80% of intelligence resides within organisations employees or, internal MI network, as they are the team who gains information’s when interacting with suppliers, customers and other industry contacts.[13] To involve employees into an intelligence program to gain data and information the following considerations can be noted: developing a rewards program to promote participation, providing MI goals, requirements and a timeframe for information to be given in and creating a proper communication method to promote the intelligence program with employees such as using an e-mail system.[14][15] 
A challenge that arises in the collection of data and information is the identification of relevant information, this is a result of organisations not clearly defining a market sector.[16]  
Validation is the second step in the MI model, this which can be referred to as data cleansing[11][18].The maintenance of good data quality is important as data and information is being retrieved from many different sources.[18] Data and information obtained from sources can be dirty, meaning that it is incomplete, wrong, inappropriate, duplicated.[11] This step will allow data and information to be adjusted and understandable to the organisation, furthermore it allows for consistency and compliance to be present.[11] If data quality is not maintained correctly it can lead to organisational losses with revenue and governmental regulation failures.[6]
Data cleansing is a complex process that involves several stages in order to get good data quality for MI strategy use.[18] Stages include defining the organisation’s level of data quality, detecting error from the data collected and then repairing the errors[19].The five stages of data cleansing are data analysis to identify errors, eliminating the errors, checking to ensure elimination of error are done appropriately, refreshing the data in the data warehouse and finally replacing the dirty data with clean data.[20]
Processing is the third step in the MI model.[4] It involves the use of translating the clean data using organisational rules, modelling, logic and analysis to produce readable information, reports and spreadsheets that allows the organisation to gain specified knowledge.[11] The interpretation of data into readable information is difficult as it is complex, it requires proper technology and heavy commitment from a top organisational level to match data and information gained and align it to a marketing strategy.[21]
Communication is the last step in the MI model. It involves the sharing, delivering and transmission of information gained from the processing step to figures in the organisation who will apply it accordingly to the market strategy[22]. As MI is a continuously changing, the communication of the MI strategy requires managers whom have expertise in the given market industry in order to determine the ongoing validity of the MI strategy and its implementation.[23] In order to make the communication of the MI strategy as successful as possible, this process must be performed by every level of an organisation, also known as the intelligence organisation.[8]
Intelligence organisation refers to the “people and information resources who make the market intelligence process happen”.[1] The five elements of an intelligence organisation are, MI leadership who manages and leads the MI process, a MI team, a portfolio of external information sources that is set up by the MI team, internal MI network made up of MI users and the MI user’s personal information source network.[1] An intelligence organisation element is made up of external and internal factors that allows for a continuous MI process.[1][8]
The gathering of MI data is different dependent on an organisation’s financial capabilities. Sources of data and information is separated into qualitative, quantitative, formal, informal, published and unpublished. With such sources being retrieved both internally and externally from the organisation.[8] It involves using search engines and corporate web sites to see competitor’s strategies, identifying business trends through reputable publications and existing customer clientele.[25] Organisations use different systems to gather MI, one system is that is used is Open-source Intelligence system.[26]
Sources of internal intelligence gathering is include but is not limited to, gathering data from customers, manufacturers, through research and development (R&D), employees, also known as salesforce, physical evidence, sales quotes, sales records, trade shows and new hires.[27] These data sources were ranked by organisations on a scale measuring five for being very important to one being not important. It was founded that customers and manufacturers and R&D are the most important to organisations with one hundred percent of organisations ranking these data sources with the number four or higher[28],. It shows that in the process of collection and gathering MI data and information, these data sources brought the most value to organisations.
Sources of external intelligence gathering is included but is not limited to, gathering data from client meetings, dealers/distributors, customers, business associates, market research projects, suppliers, on line services, periodicals and government publications.[29] These data sources were compared on the same scale as internal intelligence gathering sources, with results showing that intelligence gathered through client meetings being the most important to organisations, with one hundred percent of organisations ranking this data source with the number four or higher.[30] 
Marketing information systems allow for organisations to continuously acquire, generate, and maintain external and internal information.[31] They are systems that make use of artificial intelligence (AI) technology to aid in the planning of strategic and tactical marketing strategy of MI but also share marketing expertise.[31]
Open-source intelligence is a predominant form of MI gathering that organisations employ. OSINT is defined as the scanning, finding, gathering, exploitation, validation, analysis, and sharing with intelligence-seeking clients of publicly available print and digital/electronic data from unclassified, non-secret, and grey literature.[32] It is frequently used as its system is user friendly, its inexpensive and that it processes an abundant amount of raw materials that can be further processed.[33]
Using MI can bring to organisations both benefits and issues depending on how MI is acquired, maintained, and implemented. Benefits that MI can bring includes but is not limited to gaining competitive advantage in their marketing strategies.[1] Issues that MI can bring can include but is not limited to, financial losses and government regulatory failures.[11]
There are issues that arises in the process of acquiring MI data and information and the implementation of an organisations marketing strategy. Issues such as the acquiring intelligence unethically and illegally can lead to failures with government regulations, also, if dirty data is not properly cleansed and problems aren’t mitigated or resolved can lead to a range of negative impacts that can result in financial and reputational losses to the organisation.[11][6]
An issue that can arise is the unethical and illegal collection of data and information. Organisations can collect data for MI illegally or unethically to try and gain competitive advantages, this is known as industrial espionage.[35] An example of illegal MI collection practice is when British Airways breached the Data Protect Act 1984 through accessing Virgin’s confidential flight details.[36][11]
A standard of conduct was developed by the non-for-profit organisation Society of Competitive Intelligence Professionals, creating a code of ethics that can be adhered to by organisations when collecting market intelligence, to prevent the illegal and unethical collection of data and information.[11]
Dirty data that is collected needs to be cleansed to maintain good data quality. Challenges that arise in data cleansing is that there is a large volume of data being received leading to organisations being faced with many risks of failure to detect dirty data being processed through.[37] If data quality is not managed properly, it can result in financial losses, inefficient implementation of MI strategies and failure to comply with government regulations.[38] A reason for financial loss is due operational costs, as there is an increase in resources and time spent to identify and fix the dirty data.[6]
MI processes have been used in many organisation’s strategic market planning, however, there are still difficulties in what the hard and soft benefits in using a MI process for an organisation.[1] The benefits of a successful MI process can be sectioned into three categories, better and faster decisions, time and cost savings and organisational learning and new ideas, however, overall, it can improve profitability and the competitiveness of an organisation.[1] The competitiveness of an organisation increases as with more MI gathered it'll provide a way for organisations to innovate through improving current methods and increasing the ability to find and create new products.[39]
"
1012808131,2021-03-18 12:44:28,"Open-source intelligence (OSINT) is a multi-methods (qualitative, quantitative) methodology for collecting, analyzing and making decisions about data accessible in publicly available sources to be used in an intelligence context. In the intelligence community, the term ""open"" refers to overt, publicly available sources (as opposed to covert or clandestine sources). OSINT under one name or another has been around for hundreds of years. With the advent of instant communications and rapid information transfer, a great deal of actionable and predictive intelligence can now be obtained from public, unclassified sources. It is not related to open-source software or collective intelligence.
Open Source Intelligence (OSINT) is the collection and analysis of information that is gathered from public, or open, sources.[1] OSINT is primarily used in national security, law enforcement, and business intelligence functions and is of value to analysts who use non-sensitive intelligence in answering classified, unclassified, or proprietary intelligence requirements across the previous intelligence disciplines.
OSINT sources can be divided up into six different categories of information flow:[2]
OSINT is distinguished from research in that it applies the process of intelligence to create tailored knowledge supportive of a specific decision by a specific individual or group.[3]
OSINT is defined by both the U.S. Director of National Intelligence and the U.S. Department of Defense (DoD), as intelligence ""produced from publicly available information that is collected, exploited, and disseminated in a timely manner to an appropriate audience for the purpose of addressing a specific intelligence requirement.""[4]
According to political scientist Jeffrey T. Richelson, “open source acquisition involves procuring verbal, written, or electronically transmitted material that can be obtained legally. In addition to documents and videos available via the Internet or provided by a human source, others are obtained after U.S. or allied forces have taken control of a facility or site formerly operated by a foreign government or terrorist group.”[5]
Security researcher Mark M. Lowenthal defines OSINT as “any and all information that can be derived from overt collection: all types of media, government reports and other documents, scientific research and reports, commercial vendors of information, the Internet, and so on. The main qualifiers to open-source information are that it does not require any type of clandestine collection techniques to obtain it and that it must be obtained through means that entirely meet the copyright and commercial requirements of the vendors where applicable.""[6]
OSINT in the United States traces its origins to the creation of the Foreign Broadcast Monitoring Service (FBMS), an agency responsible for the monitoring of foreign broadcasts. An example of their work is reflected in the application of the correlation of changes in the price of oranges in Paris with that of railway bridges being bombed successfully.[7]
The Aspin-Brown Commission stated in 1996 that US access to open sources was ""severely deficient"" and that this should be a ""top priority"" for both funding and DCI attention.[8]
In July 2004, following the September 11 attacks, the 9/11 Commission recommended the creation of an open-source intelligence agency.[9] In March 2005, the Iraq Intelligence Commission recommended[1] the creation of an open-source directorate at the CIA.
Following these recommendations, in November 2005 the Director of National Intelligence announced the creation of the DNI Open Source Center. The Center was established to collect information available from ""the Internet, databases, press, radio, television, video, geospatial data, photos and commercial imagery.""[10] In addition to collecting openly available information, it would train analysts to make better use of this information. The center absorbed the CIA's previously existing Foreign Broadcast Information Service (FBIS), originally established in 1941, with FBIS head Douglas Naquin named as director of the center.[11] Then, following the events of 9/11 the Intelligence Reform and Terrorism Prevention Act merged FBIS and other research elements into the Office of the Director of National Intelligence creating the Open Source Enterprise.
Furthermore, the private sector has invested in tools which aid in OSINT collection and analysis. Specifically, In-Q-Tel, a Central Intelligence Agency supported venture capital firm in Arlington, VA assisted companies develop web-monitoring and predictive analysis tools.
In December 2005, the Director of National Intelligence appointed Eliot A. Jardines as the Assistant Deputy Director of National Intelligence for Open Source to serve as the Intelligence Community's senior intelligence officer for open source and to provide strategy, guidance and oversight for the National Open Source Enterprise.[12] Mr. Jardines has established the National Open Source Enterprise[13] and authored intelligence community directive 301. In 2008, Mr. Jardines returned to the private sector and was succeeded by Dan Butler who is ADDNI/OS[14] and previously Mr. Jardines' Senior Advisor for Policy.[15]
OSINT is valuable because it has less rigorous processing and exploitation processes and timelines than more technical intelligence disciplines such as HUMINT, SIGINT, MASINT, GEOINT, etc. Additionally, OSINT collects a valuable variety of opinions because it encompasses a great variety of sources.
According to the Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction report submitted in March 2005, OSINT must be included in the all-source intelligence process for the following reasons (as stated in the report):
OSINT is a highly diverse form of intelligence collection and analysis. It does not have its own agency, however, units are scattered within the Department of Defense and the State Department.[17] Most OSINT collectors need to take precautions while collecting information from the Internet. This can come in the form of using a VPN to anonymize their identity and collect information more discreetly. This is where evaluating sources becomes important to the overall OSINT collection and analysis process. An OSINT analyst needs intelligence evaluation to determine a true process or expose a false process that would affect predicting the future. Finally, the analysts need to find use of the evaluated intelligence so that it can be incorporated into a finished classified, unclassified, or proprietary intelligence product.
Information collection in OSINT is generally a different problem from collection in other intelligence disciplines where obtaining the raw information to be analyzed may be the major difficulty, particularly if it is to be obtained from non-cooperative targets. In OSINT, the chief difficulty is in identifying relevant, reliable sources from the vast amount of publicly available information.[18] However, this is not as great a challenge for those who know how to access local knowledge and how to leverage human experts who can create new tailored knowledge on the fly.[citation needed]
There are several categories of tools intended for OSINT analysis. The first category includes open source tools to query multiple search engines simultaneously such as IntelTechniques or search engines that provide results separately such as All in One or DuckDuckGo. This category also includes social media search engines and search engines of domains and people such as Pipl.com, Whois.net, Website.informer. The second category is designed for big data analytics platforms such as DataWalk which combine OSINT insight with local, internal data for further visual analysis and to conduct link analysis to identify connections across a large volume of records.
A list of some tools used for collecting open source intelligence:
There are a large number of open-source activities taking place throughout the US Government. Frequently, these open-source activities are described as ""media monitoring"", ""media analysis"", ""internet research"" and ""public surveys"" but are open source nonetheless.
The Library of Congress sponsors the Federal Research Division (FRD) which conducts a great deal of tailored open-source research on a fee-for-service basis for the executive branch.
The US Intelligence Community's open-source activities (known as the National Open Source Enterprise) are dictated by Intelligence Community Directive 301 promulgated by the Director of National Intelligence.[19] The Directive establishes the authorities and responsibilities of the Assistant Deputy Director of National Intelligence for Open Source (ADDNI/OS), the DNI's Open Source Center and the National Open Source Committee.
Prior to the establishment of the National Open Source Enterprise, the Foreign Broadcast Information Service (FBIS), established in 1941, was the government's primary open-source unit, transcribing and translating foreign broadcasts. It absorbed the Defense Department's Joint Publications Research Service (JPRS), which did a similar function with foreign printed materials, including newspapers, magazines, and technical journals.
The former Under-Secretary of Defense for Intelligence, Dr. Stephen Cambone encouraged in part by the Defense Science Board reports on strategic communication and transition to and from hostilities, created the Defense Open Source Program (DOSP). The current under-secretary of defense for intelligence is assigned executive agency for this program to the Defense Intelligence Agency.
U.S. military offices that engage in OSINT activities include:
The Department of Homeland Security has an active open-source intelligence unit. In congressional testimony before the House Homeland Security Committee's Intelligence, Information Sharing and Terrorism Risk Assessment Subcommittee, Undersecretary of Homeland Security Charles Allen indicated on February 14, 2007, that he had established the ""Domestic Open Source Enterprise"" to support the Department's OSINT needs and that of state, local,Ο and tribal partners.
The law enforcement OSINT community applies open-source intelligence (OSINT) to the prediction, prevention, investigation, and prosecution of criminals including terrorists. Additionally, fusion centers around the US are increasingly utilizing OSINT to support their intelligence generation and investigations.
Examples of successful law enforcement OSINT include Scotland Yard OSINT; Royal Canadian Mounted Police (RCMP) OSINT.
INTERPOL and EUROPOL experimented with OSINT units for a time, but they appear to have atrophied with the departure of their individual champions.
New York Police Department (NYPD) is known to have an OSINT unit, as does the Los Angeles County Sheriff's Department, housed within the Emergency Operations Bureau and affiliated with the LA Joint Regional Intelligence Center.
Business OSINT encompasses Commercial Intelligence, Competitor Intelligence, and Business Intelligence, and is often a chief area of practice of private intelligence agencies.
Businesses may use information brokers and private investigators to collect and analyze relevant information for business purposes which may include the media, deep web, web 2.0 and commercial content.
Another related business group within the United States that relies upon OSINT is the commercial bail-bond industry. This related industry, servicing the court system, is apart from the above Business Intelligence sector.
OSINT is useful to bail-bond agencies that employ a private fugitive recovery agency to locate and apprehend their absent client; i.e., a criminal defendant who has failed to appear for court and subsequently a warrant for arrest was issued. OSINT is the first method of discovery to help locate the defendant when initial interviewing of the bond co-signers, defendant's relatives and friends is lacking. OSINT gathering leads the investigator to discover an alternate hypothesis to analyze and then match relevant data for making a prediction regarding the fugitive's location; e.g., data is scrubbed from web access on Facebook entries, Twitter messages, and Snapchat.
Should those methods fail, the next step is to seek the specialized behavioral intelligence services that reference OSINT to aid in establishing the veracity of subjects during the forensic interview and is used to create a behavioral profile. OSINT data is correlated with interview data to include a variety behavioral patterns; e.g., a list of daily personal contacts, habits of activities, visited places of interest, vehicles used, favorite group involvements, etc. According to the director, psychologist and forensic interviewer at MN-Behavioral Intelligence Agency, (2016) OSINT data base has to be critically filtered and analyzed before it can be applied within investigative interviewing and interrogation.
The US is the birth place to one of the most advanced capabilities in conducting OSINT for usernames. The leading community backed capability currently available is UserSearch. It claims to offer the most robust Search Engine for Usernames, with supporting capabilities in emails and phone numbers. It's noted to provide reverse username searches for over 400 social networks and online communities, which is double that of any other Reverse Username capability online, including charged services. It claims to be free and community focused.
SPECTRUM is a technology project that delivers various OSINT solutions for government and business - based on artificial intelligence, machine learning and big data. In early 2014, various modules were developed and in 2016 the first government tender was won. At the end of 2016, the Spectrum team created the first pipeline to process and analyze textual information in more than 50 languages.[citation needed]
A main hindrance to practical OSINT is the volume of information it has to deal with (""information explosion""). The amount of data being distributed increases at a rate that it becomes difficult to evaluate sources in intelligence analysis.
Accredited journalists have some protection in asking questions, and researching for recognized media outlets. Even so, they can be imprisoned, even executed, for seeking out OSINT. Private individuals illegally collecting data for a foreign military or intelligence agency is considered espionage in most countries. Of course, espionage that is not treason (i.e. betraying one's country of citizenship) has been a tool of statecraft since ancient times.[20]
CITEREFUsername_Search | access-date=2018-05-29}} An OSINT Deep Web Search for usersnames and email addresses
"
1013336910,2021-03-21 01:48:37,"Open-source intelligence (OSINT) is a multi-methods (qualitative, quantitative) methodology for collecting, analyzing and making decisions about data accessible in publicly available sources to be used in an intelligence context. In the intelligence community, the term ""open"" refers to overt, publicly available sources (as opposed to covert or clandestine sources). OSINT under one name or another has been around for hundreds of years. With the advent of instant communications and rapid information transfer, a great deal of actionable and predictive intelligence can now be obtained from public, unclassified sources. It is not related to open-source software or collective intelligence.
OSINT is the collection and analysis of information that is gathered from public, or open, sources.[1] OSINT is primarily used in national security, law enforcement, and business intelligence functions and is of value to analysts who use non-sensitive intelligence in answering classified, unclassified, or proprietary intelligence requirements across the previous intelligence disciplines.
OSINT sources can be divided up into six different categories of information flow:[2]
OSINT is distinguished from research in that it applies the process of intelligence to create tailored knowledge supportive of a specific decision by a specific individual or group.[3]
OSINT is defined by both the U.S. Director of National Intelligence and the U.S. Department of Defense (DoD), as intelligence ""produced from publicly available information that is collected, exploited, and disseminated in a timely manner to an appropriate audience for the purpose of addressing a specific intelligence requirement.""[4]
According to political scientist Jeffrey T. Richelson, “open source acquisition involves procuring verbal, written, or electronically transmitted material that can be obtained legally. In addition to documents and videos available via the Internet or provided by a human source, others are obtained after U.S. or allied forces have taken control of a facility or site formerly operated by a foreign government or terrorist group.”[5]
Security researcher Mark M. Lowenthal defines OSINT as “any and all information that can be derived from overt collection: all types of media, government reports and other documents, scientific research and reports, commercial vendors of information, the Internet, and so on. The main qualifiers to open-source information are that it does not require any type of clandestine collection techniques to obtain it and that it must be obtained through means that entirely meet the copyright and commercial requirements of the vendors where applicable.""[6]
OSINT in the United States traces its origins to the creation of the Foreign Broadcast Monitoring Service (FBMS), an agency responsible for the monitoring of foreign broadcasts. An example of their work is reflected in the application of the correlation of changes in the price of oranges in Paris with that of railway bridges being bombed successfully.[7]
The Aspin-Brown Commission stated in 1996 that US access to open sources was ""severely deficient"" and that this should be a ""top priority"" for both funding and DCI attention.[8]
In July 2004, following the September 11 attacks, the 9/11 Commission recommended the creation of an open-source intelligence agency.[9] In March 2005, the Iraq Intelligence Commission recommended[1] the creation of an open-source directorate at the CIA.
Following these recommendations, in November 2005 the Director of National Intelligence announced the creation of the DNI Open Source Center. The Center was established to collect information available from ""the Internet, databases, press, radio, television, video, geospatial data, photos and commercial imagery.""[10] In addition to collecting openly available information, it would train analysts to make better use of this information. The center absorbed the CIA's previously existing Foreign Broadcast Information Service (FBIS), originally established in 1941, with FBIS head Douglas Naquin named as director of the center.[11] Then, following the events of 9/11 the Intelligence Reform and Terrorism Prevention Act merged FBIS and other research elements into the Office of the Director of National Intelligence creating the Open Source Enterprise.
Furthermore, the private sector has invested in tools which aid in OSINT collection and analysis. Specifically, In-Q-Tel, a Central Intelligence Agency supported venture capital firm in Arlington, VA assisted companies develop web-monitoring and predictive analysis tools.
In December 2005, the Director of National Intelligence appointed Eliot A. Jardines as the Assistant Deputy Director of National Intelligence for Open Source to serve as the Intelligence Community's senior intelligence officer for open source and to provide strategy, guidance and oversight for the National Open Source Enterprise.[12] Mr. Jardines has established the National Open Source Enterprise[13] and authored intelligence community directive 301. In 2008, Mr. Jardines returned to the private sector and was succeeded by Dan Butler who is ADDNI/OS[14] and previously Mr. Jardines' Senior Advisor for Policy.[15]
OSINT is valuable because it has less rigorous processing and exploitation processes and timelines than more technical intelligence disciplines such as HUMINT, SIGINT, MASINT, GEOINT, etc. Additionally, OSINT collects a valuable variety of opinions because it encompasses a great variety of sources.
According to the Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction report submitted in March 2005, OSINT must be included in the all-source intelligence process for the following reasons (as stated in the report):
OSINT is a highly diverse form of intelligence collection and analysis. It does not have its own agency, however, units are scattered within the Department of Defense and the State Department.[17] Most OSINT collectors need to take precautions while collecting information from the Internet. This can come in the form of using a VPN to anonymize their identity and collect information more discreetly. This is where evaluating sources becomes important to the overall OSINT collection and analysis process. An OSINT analyst needs intelligence evaluation to determine a true process or expose a false process that would affect predicting the future. Finally, the analysts need to find use of the evaluated intelligence so that it can be incorporated into a finished classified, unclassified, or proprietary intelligence product.
Information collection in OSINT is generally a different problem from collection in other intelligence disciplines where obtaining the raw information to be analyzed may be the major difficulty, particularly if it is to be obtained from non-cooperative targets. In OSINT, the chief difficulty is in identifying relevant, reliable sources from the vast amount of publicly available information.[18] However, this is not as great a challenge for those who know how to access local knowledge and how to leverage human experts who can create new tailored knowledge on the fly.[citation needed]
There are several categories of tools intended for OSINT analysis. The first category includes open source tools to query multiple search engines simultaneously such as IntelTechniques or search engines that provide results separately such as All in One or DuckDuckGo. This category also includes social media search engines and search engines of domains and people such as Pipl.com, Whois.net, Website.informer. The second category is designed for big data analytics platforms such as DataWalk which combine OSINT insight with local, internal data for further visual analysis and to conduct link analysis to identify connections across a large volume of records.
A list of some tools used for collecting open source intelligence:
There are a large number of open-source activities taking place throughout the US Government. Frequently, these open-source activities are described as ""media monitoring"", ""media analysis"", ""internet research"" and ""public surveys"" but are open source nonetheless.
The Library of Congress sponsors the Federal Research Division (FRD) which conducts a great deal of tailored open-source research on a fee-for-service basis for the executive branch.
The US Intelligence Community's open-source activities (known as the National Open Source Enterprise) are dictated by Intelligence Community Directive 301 promulgated by the Director of National Intelligence.[19] The Directive establishes the authorities and responsibilities of the Assistant Deputy Director of National Intelligence for Open Source (ADDNI/OS), the DNI's Open Source Center and the National Open Source Committee.
Prior to the establishment of the National Open Source Enterprise, the Foreign Broadcast Information Service (FBIS), established in 1941, was the government's primary open-source unit, transcribing and translating foreign broadcasts. It absorbed the Defense Department's Joint Publications Research Service (JPRS), which did a similar function with foreign printed materials, including newspapers, magazines, and technical journals.
The former Under-Secretary of Defense for Intelligence, Dr. Stephen Cambone encouraged in part by the Defense Science Board reports on strategic communication and transition to and from hostilities, created the Defense Open Source Program (DOSP). The current under-secretary of defense for intelligence is assigned executive agency for this program to the Defense Intelligence Agency.
U.S. military offices that engage in OSINT activities include:
The Department of Homeland Security has an active open-source intelligence unit. In congressional testimony before the House Homeland Security Committee's Intelligence, Information Sharing and Terrorism Risk Assessment Subcommittee, Undersecretary of Homeland Security Charles Allen indicated on February 14, 2007, that he had established the ""Domestic Open Source Enterprise"" to support the Department's OSINT needs and that of state, local,Ο and tribal partners.
The law enforcement OSINT community applies open-source intelligence (OSINT) to the prediction, prevention, investigation, and prosecution of criminals including terrorists. Additionally, fusion centers around the US are increasingly utilizing OSINT to support their intelligence generation and investigations.
Examples of successful law enforcement OSINT include Scotland Yard OSINT; Royal Canadian Mounted Police (RCMP) OSINT.
INTERPOL and EUROPOL experimented with OSINT units for a time, but they appear to have atrophied with the departure of their individual champions.
New York Police Department (NYPD) is known to have an OSINT unit, as does the Los Angeles County Sheriff's Department, housed within the Emergency Operations Bureau and affiliated with the LA Joint Regional Intelligence Center.
Business OSINT encompasses Commercial Intelligence, Competitor Intelligence, and Business Intelligence, and is often a chief area of practice of private intelligence agencies.
Businesses may use information brokers and private investigators to collect and analyze relevant information for business purposes which may include the media, deep web, web 2.0 and commercial content.
Another related business group within the United States that relies upon OSINT is the commercial bail-bond industry. This related industry, servicing the court system, is apart from the above Business Intelligence sector.
OSINT is useful to bail-bond agencies that employ a private fugitive recovery agency to locate and apprehend their absent client; i.e., a criminal defendant who has failed to appear for court and subsequently a warrant for arrest was issued. OSINT is the first method of discovery to help locate the defendant when initial interviewing of the bond co-signers, defendant's relatives and friends is lacking. OSINT gathering leads the investigator to discover an alternate hypothesis to analyze and then match relevant data for making a prediction regarding the fugitive's location; e.g., data is scrubbed from web access on Facebook entries, Twitter messages, and Snapchat.
Should those methods fail, the next step is to seek the specialized behavioral intelligence services that reference OSINT to aid in establishing the veracity of subjects during the forensic interview and is used to create a behavioral profile. OSINT data is correlated with interview data to include a variety behavioral patterns; e.g., a list of daily personal contacts, habits of activities, visited places of interest, vehicles used, favorite group involvements, etc. According to the director, psychologist and forensic interviewer at MN-Behavioral Intelligence Agency, (2016) OSINT data base has to be critically filtered and analyzed before it can be applied within investigative interviewing and interrogation.
The US is the birth place to one of the most advanced capabilities in conducting OSINT for usernames. The leading community backed capability currently available is UserSearch. It claims to offer the most robust Search Engine for Usernames, with supporting capabilities in emails and phone numbers. It's noted to provide reverse username searches for over 400 social networks and online communities, which is double that of any other Reverse Username capability online, including charged services. It claims to be free and community focused.
SPECTRUM is a technology project that delivers various OSINT solutions for government and business - based on artificial intelligence, machine learning and big data. In early 2014, various modules were developed and in 2016 the first government tender was won. At the end of 2016, the Spectrum team created the first pipeline to process and analyze textual information in more than 50 languages.[citation needed]
A main hindrance to practical OSINT is the volume of information it has to deal with (""information explosion""). The amount of data being distributed increases at a rate that it becomes difficult to evaluate sources in intelligence analysis.
Accredited journalists have some protection in asking questions, and researching for recognized media outlets. Even so, they can be imprisoned, even executed, for seeking out OSINT. Private individuals illegally collecting data for a foreign military or intelligence agency is considered espionage in most countries. Of course, espionage that is not treason (i.e. betraying one's country of citizenship) has been a tool of statecraft since ancient times.[20]
CITEREFUsername_Search | access-date=2018-05-29}} An OSINT Deep Web Search for usersnames and email addresses
"
1014614356,2021-03-28 03:29:30,"Open-source intelligence (OSINT) is a multi-methods (qualitative, quantitative) methodology for collecting, analyzing and making decisions about data accessible in publicly available sources to be used in an intelligence context. In the intelligence community, the term ""open"" refers to overt, publicly available sources (as opposed to covert or clandestine sources). OSINT under one name or another has been around for hundreds of years. With the advent of instant communications and rapid information transfer, a great deal of actionable and predictive intelligence can now be obtained from public, unclassified sources. It is not related to open-source software or collective intelligence.
OSINT is the collection and analysis of information that is gathered from public, or open, sources.[1] OSINT is primarily used in national security, law enforcement, and business intelligence functions and is of value to analysts who use non-sensitive intelligence in answering classified, unclassified, or proprietary intelligence requirements across the previous intelligence disciplines.
OSINT sources can be divided up into six different categories of information flow:[2]
OSINT is distinguished from research in that it applies the process of intelligence to create tailored knowledge supportive of a specific decision by a specific individual or group.[3]
OSINT is defined by both the U.S. Director of National Intelligence and the U.S. Department of Defense (DoD), as intelligence ""produced from publicly available information that is collected, exploited, and disseminated in a timely manner to an appropriate audience for the purpose of addressing a specific intelligence requirement.""[4]
According to political scientist Jeffrey T. Richelson, “open source acquisition involves procuring verbal, written, or electronically transmitted material that can be obtained legally. In addition to documents and videos available via the Internet or provided by a human source, others are obtained after U.S. or allied forces have taken control of a facility or site formerly operated by a foreign government or terrorist group.”[5]
Security researcher Mark M. Lowenthal defines OSINT as “any and all information that can be derived from overt collection: all types of media, government reports and other documents, scientific research and reports, commercial vendors of information, the Internet, and so on. The main qualifiers to open-source information are that it does not require any type of clandestine collection techniques to obtain it and that it must be obtained through means that entirely meet the copyright and commercial requirements of the vendors where applicable.""[6]
OSINT in the United States traces its origins to the creation of the Foreign Broadcast Monitoring Service (FBMS), an agency responsible for the monitoring of foreign broadcasts. An example of their work is reflected in the application of the correlation of changes in the price of oranges in Paris with that of railway bridges being bombed successfully.[7]
The Aspin-Brown Commission stated in 1996 that US access to open sources was ""severely deficient"" and that this should be a ""top priority"" for both funding and DCI attention.[8]
In July 2004, following the September 11 attacks, the 9/11 Commission recommended the creation of an open-source intelligence agency.[9] In March 2005, the Iraq Intelligence Commission recommended[1] the creation of an open-source directorate at the CIA.
Following these recommendations, in November 2005 the Director of National Intelligence announced the creation of the DNI Open Source Center. The Center was established to collect information available from ""the Internet, databases, press, radio, television, video, geospatial data, photos and commercial imagery.""[10] In addition to collecting openly available information, it would train analysts to make better use of this information. The center absorbed the CIA's previously existing Foreign Broadcast Information Service (FBIS), originally established in 1941, with FBIS head Douglas Naquin named as director of the center.[11] Then, following the events of 9/11 the Intelligence Reform and Terrorism Prevention Act merged FBIS and other research elements into the Office of the Director of National Intelligence creating the Open Source Enterprise.
Furthermore, the private sector has invested in tools which aid in OSINT collection and analysis. Specifically, In-Q-Tel, a Central Intelligence Agency supported venture capital firm in Arlington, VA assisted companies develop web-monitoring and predictive analysis tools.
In December 2005, the Director of National Intelligence appointed Eliot A. Jardines as the Assistant Deputy Director of National Intelligence for Open Source to serve as the Intelligence Community's senior intelligence officer for open source and to provide strategy, guidance and oversight for the National Open Source Enterprise.[12] Mr. Jardines has established the National Open Source Enterprise[13] and authored intelligence community directive 301. In 2008, Mr. Jardines returned to the private sector and was succeeded by Dan Butler who is ADDNI/OS[14] and previously Mr. Jardines' Senior Advisor for Policy.[15]
OSINT is valuable because it has less rigorous processing and exploitation processes and timelines than more technical intelligence disciplines such as HUMINT, SIGINT, MASINT, GEOINT, etc. Additionally, OSINT collects a valuable variety of opinions because it encompasses a great variety of sources.
According to the Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction report submitted in March 2005, OSINT must be included in the all-source intelligence process for the following reasons (as stated in the report):
OSINT is a highly diverse form of intelligence collection and analysis. It does not have its own agency, however, units are scattered within the Department of Defense and the State Department.[17] Most OSINT collectors need to take precautions while collecting information from the Internet. This can come in the form of using a VPN to anonymize their identity and collect information more discreetly. This is where evaluating sources becomes important to the overall OSINT collection and analysis process. An OSINT analyst needs intelligence evaluation to determine a true process or expose a false process that would affect predicting the future. Finally, the analysts need to find use of the evaluated intelligence so that it can be incorporated into a finished classified, unclassified, or proprietary intelligence product.
Information collection in OSINT is generally a different problem from collection in other intelligence disciplines where obtaining the raw information to be analyzed may be the major difficulty, particularly if it is to be obtained from non-cooperative targets. In OSINT, the chief difficulty is in identifying relevant, reliable sources from the vast amount of publicly available information.[18] However, this is not as great a challenge for those who know how to access local knowledge and how to leverage human experts who can create new tailored knowledge on the fly.[citation needed]
There are several categories of tools intended for OSINT analysis. The first category includes open source tools to query multiple search engines simultaneously such as IntelTechniques or search engines that provide results separately such as All in One or DuckDuckGo. This category also includes social media search engines and search engines of domains and people such as Pipl.com, Whois.net, Website.informer. The second category is designed for big data analytics platforms such as DataWalk which combine OSINT insight with local, internal data for further visual analysis and to conduct link analysis to identify connections across a large volume of records.
A list of some tools used for collecting open source intelligence:
There are a large number of open-source activities taking place throughout the US Government. Frequently, these open-source activities are described as ""media monitoring"", ""media analysis"", ""internet research"" and ""public surveys"" but are open source nonetheless.
The Library of Congress sponsors the Federal Research Division (FRD) which conducts a great deal of tailored open-source research on a fee-for-service basis for the executive branch.
The US Intelligence Community's open-source activities (known as the National Open Source Enterprise) are dictated by Intelligence Community Directive 301 promulgated by the Director of National Intelligence.[19] The Directive establishes the authorities and responsibilities of the Assistant Deputy Director of National Intelligence for Open Source (ADDNI/OS), the DNI's Open Source Center and the National Open Source Committee.
Prior to the establishment of the National Open Source Enterprise, the Foreign Broadcast Information Service (FBIS), established in 1941, was the government's primary open-source unit, transcribing and translating foreign broadcasts. It absorbed the Defense Department's Joint Publications Research Service (JPRS), which did a similar function with foreign printed materials, including newspapers, magazines, and technical journals.
The former Under-Secretary of Defense for Intelligence, Dr. Stephen Cambone encouraged in part by the Defense Science Board reports on strategic communication and transition to and from hostilities, created the Defense Open Source Program (DOSP). The current under-secretary of defense for intelligence is assigned executive agency for this program to the Defense Intelligence Agency.
U.S. military offices that engage in OSINT activities include:
The Department of Homeland Security has an active open-source intelligence unit. In congressional testimony before the House Homeland Security Committee's Intelligence, Information Sharing and Terrorism Risk Assessment Subcommittee, Undersecretary of Homeland Security Charles Allen indicated on February 14, 2007, that he had established the ""Domestic Open Source Enterprise"" to support the Department's OSINT needs and that of state, local,Ο and tribal partners.
The law enforcement OSINT community applies open-source intelligence (OSINT) to the prediction, prevention, investigation, and prosecution of criminals including terrorists. Additionally, fusion centers around the US are increasingly utilizing OSINT to support their intelligence generation and investigations.
Examples of successful law enforcement OSINT include Scotland Yard OSINT; Royal Canadian Mounted Police (RCMP) OSINT.
INTERPOL and EUROPOL experimented with OSINT units for a time, but they appear to have atrophied with the departure of their individual champions.
New York Police Department (NYPD) is known to have an OSINT unit, as does the Los Angeles County Sheriff's Department, housed within the Emergency Operations Bureau and affiliated with the LA Joint Regional Intelligence Center.
Business OSINT encompasses Commercial Intelligence, Competitor Intelligence, and Business Intelligence, and is often a chief area of practice of private intelligence agencies.
Businesses may use information brokers and private investigators to collect and analyze relevant information for business purposes which may include the media, deep web, web 2.0 and commercial content.
Another related business group within the United States that relies upon OSINT is the commercial bail-bond industry. This related industry, servicing the court system, is apart from the above Business Intelligence sector.
OSINT is useful to bail-bond agencies that employ a private fugitive recovery agency to locate and apprehend their absent client; i.e., a criminal defendant who has failed to appear for court and subsequently a warrant for arrest was issued. OSINT is the first method of discovery to help locate the defendant when initial interviewing of the bond co-signers, defendant's relatives and friends is lacking. OSINT gathering leads the investigator to discover an alternate hypothesis to analyze and then match relevant data for making a prediction regarding the fugitive's location; e.g., data is scrubbed from web access on Facebook entries, Twitter messages, and Snapchat.
Should those methods fail, the next step is to seek the specialized behavioral intelligence services that reference OSINT to aid in establishing the veracity of subjects during the forensic interview and is used to create a behavioral profile. OSINT data is correlated with interview data to include a variety behavioral patterns; e.g., a list of daily personal contacts, habits of activities, visited places of interest, vehicles used, favorite group involvements, etc. According to the director, psychologist and forensic interviewer at MN-Behavioral Intelligence Agency, (2016) OSINT data base has to be critically filtered and analyzed before it can be applied within investigative interviewing and interrogation.
The US is the birth place to one of the most advanced capabilities in conducting OSINT for usernames. The leading community backed capability currently available is UserSearch. It claims to offer the most robust Search Engine for Usernames, with supporting capabilities in emails and phone numbers. It's noted to provide reverse username searches for over 400 social networks and online communities, which is double that of any other Reverse Username capability online, including charged services. It claims to be free and community focused.
SPECTRUM is a technology project that delivers various OSINT solutions for government and business - based on artificial intelligence, machine learning and big data. In early 2014, various modules were developed and in 2016 the first government tender was won. At the end of 2016, the Spectrum team created the first pipeline to process and analyze textual information in more than 50 languages.[citation needed]
A main hindrance to practical OSINT is the volume of information it has to deal with (""information explosion""). The amount of data being distributed increases at a rate that it becomes difficult to evaluate sources in intelligence analysis.
Accredited journalists have some protection in asking questions, and researching for recognized media outlets. Even so, they can be imprisoned, even executed, for seeking out OSINT. Private individuals illegally collecting data for a foreign military or intelligence agency is considered espionage in most countries. Of course, espionage that is not treason (i.e. betraying one's country of citizenship) has been a tool of statecraft since ancient times.[20]
CITEREFUsername_Search | access-date=2018-05-29}} An OSINT Deep Web Search for usersnames and email addresses
"
1014999752,2021-03-30 04:19:22,"Open-source intelligence (OSINT) is a multi-methods (qualitative, quantitative) methodology for collecting, analyzing and making decisions about data accessible in publicly available sources to be used in an intelligence context. In the intelligence community, the term ""open"" refers to overt, publicly available sources (as opposed to covert or clandestine sources). OSINT under one name or another has been around for hundreds of years. With the advent of instant communications and rapid information transfer, a great deal of actionable and predictive intelligence can now be obtained from public, unclassified sources. It is not related to open-source software or collective intelligence.
OSINT is the collection and analysis of information that is gathered from public, or open, sources.[1] OSINT is primarily used in national security, law enforcement, and business intelligence functions and is of value to analysts who use non-sensitive intelligence in answering classified, unclassified, or proprietary intelligence requirements across the previous intelligence disciplines.
OSINT sources can be divided up into six different categories of information flow:[2]
OSINT is distinguished from research in that it applies the process of intelligence to create tailored knowledge supportive of a specific decision by a specific individual or group.[3]
OSINT is defined by both the U.S. Director of National Intelligence and the U.S. Department of Defense (DoD), as intelligence ""produced from publicly available information that is collected, exploited, and disseminated in a timely manner to an appropriate audience for the purpose of addressing a specific intelligence requirement.""[4]
According to political scientist Jeffrey T. Richelson, “open source acquisition involves procuring verbal, written, or electronically transmitted material that can be obtained legally. In addition to documents and videos available via the Internet or provided by a human source, others are obtained after U.S. or allied forces have taken control of a facility or site formerly operated by a foreign government or terrorist group.”[5]
Security researcher Mark M. Lowenthal defines OSINT as “any and all information that can be derived from overt collection: all types of media, government reports and other documents, scientific research and reports, commercial vendors of information, the Internet, and so on. The main qualifiers to open-source information are that it does not require any type of clandestine collection techniques to obtain it and that it must be obtained through means that entirely meet the copyright and commercial requirements of the vendors where applicable.""[6]
OSINT in the United States traces its origins to the creation of the Foreign Broadcast Monitoring Service (FBMS), an agency responsible for the monitoring of foreign broadcasts. An example of their work is reflected in the application of the correlation of changes in the price of oranges in Paris with that of railway bridges being bombed successfully.[7]
The Aspin-Brown Commission stated in 1996 that US access to open sources was ""severely deficient"" and that this should be a ""top priority"" for both funding and DCI attention.[8]
In July 2004, following the September 11 attacks, the 9/11 Commission recommended the creation of an open-source intelligence agency.[9] In March 2005, the Iraq Intelligence Commission recommended[1] the creation of an open-source directorate at the CIA.
Following these recommendations, in November 2005 the Director of National Intelligence announced the creation of the DNI Open Source Center. The Center was established to collect information available from ""the Internet, databases, press, radio, television, video, geospatial data, photos and commercial imagery.""[10] In addition to collecting openly available information, it would train analysts to make better use of this information. The center absorbed the CIA's previously existing Foreign Broadcast Information Service (FBIS), originally established in 1941, with FBIS head Douglas Naquin named as director of the center.[11] Then, following the events of 9/11 the Intelligence Reform and Terrorism Prevention Act merged FBIS and other research elements into the Office of the Director of National Intelligence creating the Open Source Enterprise.
Furthermore, the private sector has invested in tools which aid in OSINT collection and analysis. Specifically, In-Q-Tel, a Central Intelligence Agency supported venture capital firm in Arlington, VA assisted companies develop web-monitoring and predictive analysis tools.
In December 2005, the Director of National Intelligence appointed Eliot A. Jardines as the Assistant Deputy Director of National Intelligence for Open Source to serve as the Intelligence Community's senior intelligence officer for open source and to provide strategy, guidance and oversight for the National Open Source Enterprise.[12] Mr. Jardines has established the National Open Source Enterprise[13] and authored intelligence community directive 301. In 2008, Mr. Jardines returned to the private sector and was succeeded by Dan Butler who is ADDNI/OS[14] and previously Mr. Jardines' Senior Advisor for Policy.[15]
OSINT is valuable because it has less rigorous processing and exploitation processes and timelines than more technical intelligence disciplines such as HUMINT, SIGINT, MASINT, GEOINT, etc. Additionally, OSINT collects a valuable variety of opinions because it encompasses a great variety of sources.
According to the Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction report submitted in March 2005, OSINT must be included in the all-source intelligence process for the following reasons (as stated in the report):
OSINT is a highly diverse form of intelligence collection and analysis. It does not have its own agency, however, units are scattered within the Department of Defense and the State Department.[17] Most OSINT collectors need to take precautions while collecting information from the Internet. This can come in the form of using a VPN to anonymize their identity and collect information more discreetly. This is where evaluating sources becomes important to the overall OSINT collection and analysis process. An OSINT analyst needs intelligence evaluation to determine a true process or expose a false process that would affect predicting the future. Finally, the analysts need to find use of the evaluated intelligence so that it can be incorporated into a finished classified, unclassified, or proprietary intelligence product.
Information collection in OSINT is generally a different problem from collection in other intelligence disciplines where obtaining the raw information to be analyzed may be the major difficulty, particularly if it is to be obtained from non-cooperative targets. In OSINT, the chief difficulty is in identifying relevant, reliable sources from the vast amount of publicly available information.[18] However, this is not as great a challenge for those who know how to access local knowledge and how to leverage human experts who can create new tailored knowledge on the fly.[citation needed]
There are several categories of tools intended for OSINT analysis. The first category includes open source tools to query multiple search engines simultaneously such as IntelTechniques or search engines that provide results separately such as All in One or DuckDuckGo. This category also includes social media search engines and search engines of domains and people such as Pipl.com, Whois.net, Website.informer. The second category is designed for big data analytics platforms such as DataWalk which combine OSINT insight with local, internal data for further visual analysis and to conduct link analysis to identify connections across a large volume of records.
A list of some tools used for collecting open source intelligence:
There are a large number of open-source activities taking place throughout the US Government. Frequently, these open-source activities are described as ""media monitoring"", ""media analysis"", ""internet research"" and ""public surveys"" but are open source nonetheless.
The Library of Congress sponsors the Federal Research Division (FRD) which conducts a great deal of tailored open-source research on a fee-for-service basis for the executive branch.
The US Intelligence Community's open-source activities (known as the National Open Source Enterprise) are dictated by Intelligence Community Directive 301 promulgated by the Director of National Intelligence.[19] The Directive establishes the authorities and responsibilities of the Assistant Deputy Director of National Intelligence for Open Source (ADDNI/OS), the DNI's Open Source Center and the National Open Source Committee.
Prior to the establishment of the National Open Source Enterprise, the Foreign Broadcast Information Service (FBIS), established in 1941, was the government's primary open-source unit, transcribing and translating foreign broadcasts. It absorbed the Defense Department's Joint Publications Research Service (JPRS), which did a similar function with foreign printed materials, including newspapers, magazines, and technical journals.
The former Under-Secretary of Defense for Intelligence, Dr. Stephen Cambone encouraged in part by the Defense Science Board reports on strategic communication and transition to and from hostilities, created the Defense Open Source Program (DOSP). The current under-secretary of defense for intelligence is assigned executive agency for this program to the Defense Intelligence Agency.
U.S. military offices that engage in OSINT activities include:
The Department of Homeland Security has an active open-source intelligence unit. In congressional testimony before the House Homeland Security Committee's Intelligence, Information Sharing and Terrorism Risk Assessment Subcommittee, Undersecretary of Homeland Security Charles Allen indicated on February 14, 2007, that he had established the ""Domestic Open Source Enterprise"" to support the Department's OSINT needs and that of state, local,Ο and tribal partners.
The law enforcement OSINT community applies open-source intelligence (OSINT) to the prediction, prevention, investigation, and prosecution of criminals including terrorists. Additionally, fusion centers around the US are increasingly utilizing OSINT to support their intelligence generation and investigations.
Examples of successful law enforcement OSINT include Scotland Yard OSINT; Royal Canadian Mounted Police (RCMP) OSINT.
INTERPOL and EUROPOL experimented with OSINT units for a time, but they appear to have atrophied with the departure of their individual champions.
New York Police Department (NYPD) is known to have an OSINT unit, as does the Los Angeles County Sheriff's Department, housed within the Emergency Operations Bureau and affiliated with the LA Joint Regional Intelligence Center.
Business OSINT encompasses Commercial Intelligence, Competitor Intelligence, and Business Intelligence, and is often a chief area of practice of private intelligence agencies.
Businesses may use information brokers and private investigators to collect and analyze relevant information for business purposes which may include the media, deep web, web 2.0 and commercial content.
Another related business group within the United States that relies upon OSINT is the commercial bail-bond industry. This related industry, servicing the court system, is apart from the above Business Intelligence sector.
OSINT is useful to bail-bond agencies that employ a private fugitive recovery agency to locate and apprehend their absent client; i.e., a criminal defendant who has failed to appear for court and subsequently a warrant for arrest was issued. OSINT is the first method of discovery to help locate the defendant when initial interviewing of the bond co-signers, defendant's relatives and friends is lacking. OSINT gathering leads the investigator to discover an alternate hypothesis to analyze and then match relevant data for making a prediction regarding the fugitive's location; e.g., data is scrubbed from web access on Facebook entries, Twitter messages, and Snapchat.
Should those methods fail, the next step is to seek the specialized behavioral intelligence services that reference OSINT to aid in establishing the veracity of subjects during the forensic interview and is used to create a behavioral profile. OSINT data is correlated with interview data to include a variety behavioral patterns; e.g., a list of daily personal contacts, habits of activities, visited places of interest, vehicles used, favorite group involvements, etc. According to the director, psychologist and forensic interviewer at MN-Behavioral Intelligence Agency, (2016) OSINT data base has to be critically filtered and analyzed before it can be applied within investigative interviewing and interrogation.
The US is the birth place to one of the most advanced capabilities in conducting OSINT for usernames. The leading community backed capability currently available is UserSearch. It claims to offer the most robust Search Engine for Usernames, with supporting capabilities in emails and phone numbers. It's noted to provide reverse username searches for over 400 social networks and online communities, which is double that of any other Reverse Username capability online, including charged services. It claims to be free and community focused.
SPECTRUM is a technology project that delivers various OSINT solutions for government and business - based on artificial intelligence, machine learning and big data. In early 2014, various modules were developed and in 2016 the first government tender was won. At the end of 2016, the Spectrum team created the first pipeline to process and analyze textual information in more than 50 languages.[citation needed]
A main hindrance to practical OSINT is the volume of information it has to deal with (""information explosion""). The amount of data being distributed increases at a rate that it becomes difficult to evaluate sources in intelligence analysis.
Accredited journalists have some protection in asking questions, and researching for recognized media outlets. Even so, they can be imprisoned, even executed, for seeking out OSINT. Private individuals illegally collecting data for a foreign military or intelligence agency is considered espionage in most countries. Of course, espionage that is not treason (i.e. betraying one's country of citizenship) has been a tool of statecraft since ancient times.[20]
CITEREFUsername_Search | access-date=2018-05-29}} An OSINT Deep Web Search for usersnames and email addresses
"
1015608105,2021-04-02 13:01:35,"Open-source intelligence (OSINT) is a multi-factor (qualitative, quantitative) methodology for collecting, analyzing and making decisions about data accessible in publicly available sources to be used in an intelligence context. In the intelligence community, the term ""open"" refers to overt, publicly available sources (as opposed to covert or clandestine sources). OSINT under one name or another has been around for hundreds of years. With the advent of instant communications and rapid information transfer, a great deal of actionable and predictive intelligence can now be obtained from public, unclassified sources. It is not related to open-source software or collective intelligence.
OSINT is the collection and analysis of information that is gathered from public, or open, sources.[1] OSINT is primarily used in national security, law enforcement, and business intelligence functions and is of value to analysts who use non-sensitive intelligence in answering classified, unclassified, or proprietary intelligence requirements across the previous intelligence disciplines.
OSINT sources can be divided up into six different categories of information flow:[2]
OSINT is distinguished from research in that it applies the process of intelligence to create tailored knowledge supportive of a specific decision by a specific individual or group.[3]
OSINT is defined by both the U.S. Director of National Intelligence and the U.S. Department of Defense (DoD), as intelligence ""produced from publicly available information that is collected, exploited, and disseminated in a timely manner to an appropriate audience for the purpose of addressing a specific intelligence requirement.""[4]  As defined by NATO, OSINT is intelligence ""derived from publicly available information, as well as other unclassified information that has limited public distribution or access.""[5]
According to political scientist Jeffrey T. Richelson, “open source acquisition involves procuring verbal, written, or electronically transmitted material that can be obtained legally. In addition to documents and videos available via the Internet or provided by a human source, others are obtained after U.S. or allied forces have taken control of a facility or site formerly operated by a foreign government or terrorist group.”[6]
Security researcher Mark M. Lowenthal defines OSINT as “any and all information that can be derived from overt collection: all types of media, government reports and other documents, scientific research and reports, commercial vendors of information, the Internet, and so on. The main qualifiers to open-source information are that it does not require any type of clandestine collection techniques to obtain it and that it must be obtained through means that entirely meet the copyright and commercial requirements of the vendors where applicable.""[7]
OSINT in the United States traces its origins to the creation of the Foreign Broadcast Monitoring Service (FBMS), an agency responsible for the monitoring of foreign broadcasts. An example of their work is reflected in the application of the correlation of changes in the price of oranges in Paris with that of railway bridges being bombed successfully.[8]
The Aspin-Brown Commission stated in 1996 that US access to open sources was ""severely deficient"" and that this should be a ""top priority"" for both funding and DCI attention.[9]
In July 2004, following the September 11 attacks, the 9/11 Commission recommended the creation of an open-source intelligence agency.[10] In March 2005, the Iraq Intelligence Commission recommended[1] the creation of an open-source directorate at the CIA.
Following these recommendations, in November 2005 the Director of National Intelligence announced the creation of the DNI Open Source Center. The Center was established to collect information available from ""the Internet, databases, press, radio, television, video, geospatial data, photos and commercial imagery.""[11] In addition to collecting openly available information, it would train analysts to make better use of this information. The center absorbed the CIA's previously existing Foreign Broadcast Information Service (FBIS), originally established in 1941, with FBIS head Douglas Naquin named as director of the center.[12] Then, following the events of 9/11 the Intelligence Reform and Terrorism Prevention Act merged FBIS and other research elements into the Office of the Director of National Intelligence creating the Open Source Enterprise.
Furthermore, the private sector has invested in tools which aid in OSINT collection and analysis. Specifically, In-Q-Tel, a Central Intelligence Agency supported venture capital firm in Arlington, VA assisted companies develop web-monitoring and predictive analysis tools.
In December 2005, the Director of National Intelligence appointed Eliot A. Jardines as the Assistant Deputy Director of National Intelligence for Open Source to serve as the Intelligence Community's senior intelligence officer for open source and to provide strategy, guidance and oversight for the National Open Source Enterprise.[13] Mr. Jardines has established the National Open Source Enterprise[14] and authored intelligence community directive 301. In 2008, Mr. Jardines returned to the private sector and was succeeded by Dan Butler who is ADDNI/OS[15] and previously Mr. Jardines' Senior Advisor for Policy.[16]
OSINT is valuable because it has less rigorous processing and exploitation processes and timelines than more technical intelligence disciplines such as HUMINT, SIGINT, MASINT, GEOINT, etc. Additionally, OSINT collects a valuable variety of opinions because it encompasses a great variety of sources.
According to the Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction report submitted in March 2005, OSINT must be included in the all-source intelligence process for the following reasons (as stated in the report):
OSINT is a highly diverse form of intelligence collection and analysis. It does not have its own agency, however, units are scattered within the Department of Defense and the State Department.[18] Most OSINT collectors need to take precautions while collecting information from the Internet. This can come in the form of using a VPN to anonymize their identity and collect information more discreetly. This is where evaluating sources becomes important to the overall OSINT collection and analysis process. An OSINT analyst needs intelligence evaluation to determine a true process or expose a false process that would affect predicting the future. Finally, the analysts need to find use of the evaluated intelligence so that it can be incorporated into a finished classified, unclassified, or proprietary intelligence product.
Information collection in OSINT is generally a different problem from collection in other intelligence disciplines where obtaining the raw information to be analyzed may be the major difficulty, particularly if it is to be obtained from non-cooperative targets. In OSINT, the chief difficulty is in identifying relevant, reliable sources from the vast amount of publicly available information.[19] However, this is not as great a challenge for those who know how to access local knowledge and how to leverage human experts who can create new tailored knowledge on the fly.[citation needed]
There are several categories of tools intended for OSINT analysis. The first category includes open source tools to query multiple search engines simultaneously such as IntelTechniques or search engines that provide results separately such as All in One or DuckDuckGo. This category also includes social media search engines and search engines of domains and people such as Pipl.com, Whois.net, Website.informer. The second category is designed for big data analytics platforms such as DataWalk which combine OSINT insight with local, internal data for further visual analysis and to conduct link analysis to identify connections across a large volume of records.
A list of some tools used for collecting open source intelligence:
There are a large number of open-source activities taking place throughout the US Government. Frequently, these open-source activities are described as ""media monitoring"", ""media analysis"", ""internet research"" and ""public surveys"" but are open source nonetheless.
The Library of Congress sponsors the Federal Research Division (FRD) which conducts a great deal of tailored open-source research on a fee-for-service basis for the executive branch.
The US Intelligence Community's open-source activities (known as the National Open Source Enterprise) are dictated by Intelligence Community Directive 301 promulgated by the Director of National Intelligence.[20] The Directive establishes the authorities and responsibilities of the Assistant Deputy Director of National Intelligence for Open Source (ADDNI/OS), the DNI's Open Source Center and the National Open Source Committee.
Prior to the establishment of the National Open Source Enterprise, the Foreign Broadcast Information Service (FBIS), established in 1941, was the government's primary open-source unit, transcribing and translating foreign broadcasts. It absorbed the Defense Department's Joint Publications Research Service (JPRS), which did a similar function with foreign printed materials, including newspapers, magazines, and technical journals.
The former Under-Secretary of Defense for Intelligence, Dr. Stephen Cambone encouraged in part by the Defense Science Board reports on strategic communication and transition to and from hostilities, created the Defense Open Source Program (DOSP). The current under-secretary of defense for intelligence is assigned executive agency for this program to the Defense Intelligence Agency.
U.S. military offices that engage in OSINT activities include:
The Department of Homeland Security has an active open-source intelligence unit. In congressional testimony before the House Homeland Security Committee's Intelligence, Information Sharing and Terrorism Risk Assessment Subcommittee, Undersecretary of Homeland Security Charles Allen indicated on February 14, 2007, that he had established the ""Domestic Open Source Enterprise"" to support the Department's OSINT needs and that of state, local,Ο and tribal partners.
The law enforcement OSINT community applies open-source intelligence (OSINT) to the prediction, prevention, investigation, and prosecution of criminals including terrorists. Additionally, fusion centers around the US are increasingly utilizing OSINT to support their intelligence generation and investigations.
Examples of successful law enforcement OSINT include Scotland Yard OSINT; Royal Canadian Mounted Police (RCMP) OSINT.
INTERPOL and EUROPOL experimented with OSINT units for a time, but they appear to have atrophied with the departure of their individual champions.
New York Police Department (NYPD) is known to have an OSINT unit, as does the Los Angeles County Sheriff's Department, housed within the Emergency Operations Bureau and affiliated with the LA Joint Regional Intelligence Center.
Business OSINT encompasses Commercial Intelligence, Competitor Intelligence, and Business Intelligence, and is often a chief area of practice of private intelligence agencies.
Businesses may use information brokers and private investigators to collect and analyze relevant information for business purposes which may include the media, deep web, web 2.0 and commercial content.
Another related business group within the United States that relies upon OSINT is the commercial bail-bond industry. This related industry, servicing the court system, is apart from the above Business Intelligence sector.
OSINT is useful to bail-bond agencies that employ a private fugitive recovery agency to locate and apprehend their absent client; i.e., a criminal defendant who has failed to appear for court and subsequently a warrant for arrest was issued. OSINT is the first method of discovery to help locate the defendant when initial interviewing of the bond co-signers, defendant's relatives and friends is lacking. OSINT gathering leads the investigator to discover an alternate hypothesis to analyze and then match relevant data for making a prediction regarding the fugitive's location; e.g., data is scrubbed from web access on Facebook entries, Twitter messages, and Snapchat.
Should those methods fail, the next step is to seek the specialized behavioral intelligence services that reference OSINT to aid in establishing the veracity of subjects during the forensic interview and is used to create a behavioral profile. OSINT data is correlated with interview data to include a variety behavioral patterns; e.g., a list of daily personal contacts, habits of activities, visited places of interest, vehicles used, favorite group involvements, etc. According to the director, psychologist and forensic interviewer at MN-Behavioral Intelligence Agency, (2016) OSINT data base has to be critically filtered and analyzed before it can be applied within investigative interviewing and interrogation.
The US is the birth place to one of the most advanced capabilities in conducting OSINT for usernames. The leading community backed capability currently available is UserSearch. It claims to offer the most robust Search Engine for Usernames, with supporting capabilities in emails and phone numbers. It's noted to provide reverse username searches for over 400 social networks and online communities, which is double that of any other Reverse Username capability online, including charged services. It claims to be free and community focused.
SPECTRUM is a technology project that delivers various OSINT solutions for government and business - based on artificial intelligence, machine learning and big data. In early 2014, various modules were developed and in 2016 the first government tender was won. At the end of 2016, the Spectrum team created the first pipeline to process and analyze textual information in more than 50 languages.[citation needed]
A main hindrance to practical OSINT is the volume of information it has to deal with (""information explosion""). The amount of data being distributed increases at a rate that it becomes difficult to evaluate sources in intelligence analysis.
Accredited journalists have some protection in asking questions, and researching for recognized media outlets. Even so, they can be imprisoned, even executed, for seeking out OSINT. Private individuals illegally collecting data for a foreign military or intelligence agency is considered espionage in most countries. Of course, espionage that is not treason (i.e. betraying one's country of citizenship) has been a tool of statecraft since ancient times.[21]
CITEREFUsername_Search | access-date=2018-05-29}} An OSINT Deep Web Search for usersnames and email addresses
"
1015619258,2021-04-02 14:34:02,"Open-source intelligence (OSINT) is a multi-factor (qualitative, quantitative) methodology for collecting, analyzing and making decisions about data accessible in publicly available sources to be used in an intelligence context. In the intelligence community, the term ""open"" refers to overt, publicly available sources (as opposed to covert or clandestine sources). OSINT under one name or another has been around for hundreds of years. With the advent of instant communications and rapid information transfer, a great deal of actionable and predictive intelligence can now be obtained from public, unclassified sources. It is not related to open-source software or collective intelligence.
OSINT is the collection and analysis of information that is gathered from public, or open, sources.[1] OSINT is primarily used in national security, law enforcement, and business intelligence functions and is of value to analysts who use non-sensitive intelligence in answering classified, unclassified, or proprietary intelligence requirements across the previous intelligence disciplines.
OSINT sources can be divided up into six different categories of information flow:[2]
OSINT is distinguished from research in that it applies the process of intelligence to create tailored knowledge supportive of a specific decision by a specific individual or group.[3]
OSINT is defined by both the U.S. Director of National Intelligence and the U.S. Department of Defense (DoD), as intelligence ""produced from publicly available information that is collected, exploited, and disseminated in a timely manner to an appropriate audience for the purpose of addressing a specific intelligence requirement.""[4]  As defined by NATO, OSINT is intelligence ""derived from publicly available information, as well as other unclassified information that has limited public distribution or access.""[5]
According to political scientist Jeffrey T. Richelson, “open source acquisition involves procuring verbal, written, or electronically transmitted material that can be obtained legally. In addition to documents and videos available via the Internet or provided by a human source, others are obtained after U.S. or allied forces have taken control of a facility or site formerly operated by a foreign government or terrorist group.”[6]
Security researcher Mark M. Lowenthal defines OSINT as “any and all information that can be derived from overt collection: all types of media, government reports and other documents, scientific research and reports, commercial vendors of information, the Internet, and so on. The main qualifiers to open-source information are that it does not require any type of clandestine collection techniques to obtain it and that it must be obtained through means that entirely meet the copyright and commercial requirements of the vendors where applicable.""[7]
OSINT in the United States traces its origins to the creation of the Foreign Broadcast Monitoring Service (FBMS), an agency responsible for the monitoring of foreign broadcasts. An example of their work is reflected in the application of the correlation of changes in the price of oranges in Paris with that of railway bridges being bombed successfully.[8]
The Aspin-Brown Commission stated in 1996 that US access to open sources was ""severely deficient"" and that this should be a ""top priority"" for both funding and DCI attention.[9]
In July 2004, following the September 11 attacks, the 9/11 Commission recommended the creation of an open-source intelligence agency.[10] In March 2005, the Iraq Intelligence Commission recommended[1] the creation of an open-source directorate at the CIA.
Following these recommendations, in November 2005 the Director of National Intelligence announced the creation of the DNI Open Source Center. The Center was established to collect information available from ""the Internet, databases, press, radio, television, video, geospatial data, photos and commercial imagery.""[11] In addition to collecting openly available information, it would train analysts to make better use of this information. The center absorbed the CIA's previously existing Foreign Broadcast Information Service (FBIS), originally established in 1941, with FBIS head Douglas Naquin named as director of the center.[12] Then, following the events of 9/11 the Intelligence Reform and Terrorism Prevention Act merged FBIS and other research elements into the Office of the Director of National Intelligence creating the Open Source Enterprise.
Furthermore, the private sector has invested in tools which aid in OSINT collection and analysis. Specifically, In-Q-Tel, a Central Intelligence Agency supported venture capital firm in Arlington, VA assisted companies develop web-monitoring and predictive analysis tools.
In December 2005, the Director of National Intelligence appointed Eliot A. Jardines as the Assistant Deputy Director of National Intelligence for Open Source to serve as the Intelligence Community's senior intelligence officer for open source and to provide strategy, guidance and oversight for the National Open Source Enterprise.[13] Mr. Jardines has established the National Open Source Enterprise[14] and authored intelligence community directive 301. In 2008, Mr. Jardines returned to the private sector and was succeeded by Dan Butler who is ADDNI/OS[15] and previously Mr. Jardines' Senior Advisor for Policy.[16]
OSINT is valuable because it has less rigorous processing and exploitation processes and timelines than more technical intelligence disciplines such as HUMINT, SIGINT, MASINT, GEOINT, etc. Additionally, OSINT collects a valuable variety of opinions because it encompasses a great variety of sources.
According to the Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction report submitted in March 2005, OSINT must be included in the all-source intelligence process for the following reasons (as stated in the report):
OSINT is a highly diverse form of intelligence collection and analysis. It does not have its own agency, however, units are scattered within the Department of Defense and the State Department.[18] Most OSINT collectors need to take precautions while collecting information from the Internet. This can come in the form of using a VPN to anonymize their identity and collect information more discreetly. This is where evaluating sources becomes important to the overall OSINT collection and analysis process. An OSINT analyst needs intelligence evaluation to determine a true process or expose a false process that would affect predicting the future. Finally, the analysts need to find use of the evaluated intelligence so that it can be incorporated into a finished classified, unclassified, or proprietary intelligence product.
Information collection in OSINT is generally a different problem from collection in other intelligence disciplines where obtaining the raw information to be analyzed may be the major difficulty, particularly if it is to be obtained from non-cooperative targets. In OSINT, the chief difficulty is in identifying relevant, reliable sources from the vast amount of publicly available information.[19] However, this is not as great a challenge for those who know how to access local knowledge and how to leverage human experts who can create new tailored knowledge on the fly.[citation needed]
There are several categories of tools intended for OSINT analysis. The first category includes open source tools to query multiple search engines simultaneously such as IntelTechniques or search engines that provide results separately such as All in One or DuckDuckGo. This category also includes social media search engines and search engines of domains and people such as Pipl.com, Whois.net, Website.informer. The second category is designed for big data analytics platforms such as DataWalk which combine OSINT insight with local, internal data for further visual analysis and to conduct link analysis to identify connections across a large volume of records.
A list of some tools used for collecting open source intelligence:
There are a large number of open-source activities taking place throughout the US Government. Frequently, these open-source activities are described as ""media monitoring"", ""media analysis"", ""internet research"" and ""public surveys"" but are open source nonetheless.
The Library of Congress sponsors the Federal Research Division (FRD) which conducts a great deal of tailored open-source research on a fee-for-service basis for the executive branch.
The US Intelligence Community's open-source activities (known as the National Open Source Enterprise) are dictated by Intelligence Community Directive 301 promulgated by the Director of National Intelligence.[20] The Directive establishes the authorities and responsibilities of the Assistant Deputy Director of National Intelligence for Open Source (ADDNI/OS), the DNI's Open Source Center and the National Open Source Committee.
Prior to the establishment of the National Open Source Enterprise, the Foreign Broadcast Information Service (FBIS), established in 1941, was the government's primary open-source unit, transcribing and translating foreign broadcasts. It absorbed the Defense Department's Joint Publications Research Service (JPRS), which did a similar function with foreign printed materials, including newspapers, magazines, and technical journals.
The former Under-Secretary of Defense for Intelligence, Dr. Stephen Cambone encouraged in part by the Defense Science Board reports on strategic communication and transition to and from hostilities, created the Defense Open Source Program (DOSP). The current under-secretary of defense for intelligence is assigned executive agency for this program to the Defense Intelligence Agency.
U.S. military offices that engage in OSINT activities include:
The Department of Homeland Security has an active open-source intelligence unit. In congressional testimony before the House Homeland Security Committee's Intelligence, Information Sharing and Terrorism Risk Assessment Subcommittee, Undersecretary of Homeland Security Charles Allen indicated on February 14, 2007, that he had established the ""Domestic Open Source Enterprise"" to support the Department's OSINT needs and that of state, local,Ο and tribal partners.
The law enforcement OSINT community applies open-source intelligence (OSINT) to the prediction, prevention, investigation, and prosecution of criminals including terrorists. Additionally, fusion centers around the US are increasingly utilizing OSINT to support their intelligence generation and investigations.
Examples of successful law enforcement OSINT include Scotland Yard OSINT; Royal Canadian Mounted Police (RCMP) OSINT.
INTERPOL and EUROPOL experimented with OSINT units for a time, but they appear to have atrophied with the departure of their individual champions.
New York Police Department (NYPD) is known to have an OSINT unit, as does the Los Angeles County Sheriff's Department, housed within the Emergency Operations Bureau and affiliated with the LA Joint Regional Intelligence Center.
Business OSINT encompasses Commercial Intelligence, Competitor Intelligence, and Business Intelligence, and is often a chief area of practice of private intelligence agencies.
Businesses may use information brokers and private investigators to collect and analyze relevant information for business purposes which may include the media, deep web, web 2.0 and commercial content.
Another related business group within the United States that relies upon OSINT is the commercial bail-bond industry. This related industry, servicing the court system, is apart from the above Business Intelligence sector.
OSINT is useful to bail-bond agencies that employ a private fugitive recovery agency to locate and apprehend their absent client; i.e., a criminal defendant who has failed to appear for court and subsequently a warrant for arrest was issued. OSINT is the first method of discovery to help locate the defendant when initial interviewing of the bond co-signers, defendant's relatives and friends is lacking. OSINT gathering leads the investigator to discover an alternate hypothesis to analyze and then match relevant data for making a prediction regarding the fugitive's location; e.g., data is scrubbed from web access on Facebook entries, Twitter messages, and Snapchat.
Should those methods fail, the next step is to seek the specialized behavioral intelligence services that reference OSINT to aid in establishing the veracity of subjects during the forensic interview and is used to create a behavioral profile. OSINT data is correlated with interview data to include a variety behavioral patterns; e.g., a list of daily personal contacts, habits of activities, visited places of interest, vehicles used, favorite group involvements, etc. According to the director, psychologist and forensic interviewer at MN-Behavioral Intelligence Agency, (2016) OSINT data base has to be critically filtered and analyzed before it can be applied within investigative interviewing and interrogation.
The US is the birth place to one of the most advanced capabilities in conducting OSINT for usernames. The leading community backed capability currently available is UserSearch. It claims to offer the most robust Search Engine for Usernames, with supporting capabilities in emails and phone numbers. It's noted to provide reverse username searches for over 400 social networks and online communities, which is double that of any other Reverse Username capability online, including charged services. It claims to be free and community focused.
SPECTRUM is a technology project that delivers various OSINT solutions for government and business - based on artificial intelligence, machine learning and big data. In early 2014, various modules were developed and in 2016 the first government tender was won. At the end of 2016, the Spectrum team created the first pipeline to process and analyze textual information in more than 50 languages.[citation needed]
A main hindrance to practical OSINT is the volume of information it has to deal with (""information explosion""). The amount of data being distributed increases at a rate that it becomes difficult to evaluate sources in intelligence analysis.
Accredited journalists have some protection in asking questions, and researching for recognized media outlets. Even so, they can be imprisoned, even executed, for seeking out OSINT. Private individuals illegally collecting data for a foreign military or intelligence agency is considered espionage in most countries. Of course, espionage that is not treason (i.e. betraying one's country of citizenship) has been a tool of statecraft since ancient times.[21]
CITEREFUsername_Search | access-date=2018-05-29}} An OSINT Deep Web Search for usersnames and email addresses
"
1019581536,2021-04-24 05:01:32,"Open-source intelligence (OSINT) is a multi-factor (qualitative, quantitative) methodology for collecting, analyzing and making decisions about data accessible in publicly available sources to be used in an intelligence context. In the intelligence community, the term ""open"" refers to overt, publicly available sources (as opposed to covert or clandestine sources). OSINT under one name or another has been around for hundreds of years. With the advent of instant communications and rapid information transfer, a great deal of actionable and predictive intelligence can now be obtained from public, unclassified sources. It is not related to open-source software or collective intelligence.
OSINT is the collection and analysis of information that is gathered from public, or open, sources.[1] OSINT is primarily used in national security, law enforcement, and business intelligence functions and is of value to analysts who use non-sensitive intelligence in answering classified, unclassified, or proprietary intelligence requirements across the previous intelligence disciplines.
OSINT sources can be divided up into six different categories of information flow:[2]
OSINT is distinguished from research in that it applies the process of intelligence to create tailored knowledge supportive of a specific decision by a specific individual or group.[3]
OSINT is defined by both the U.S. Director of National Intelligence and the U.S. Department of Defense (DoD), as intelligence ""produced from publicly available information that is collected, exploited, and disseminated in a timely manner to an appropriate audience for the purpose of addressing a specific intelligence requirement.""[4]  As defined by NATO, OSINT is intelligence ""derived from publicly available information, as well as other unclassified information that has limited public distribution or access.""[5]
According to political scientist Jeffrey T. Richelson, “open source acquisition involves procuring verbal, written, or electronically transmitted material that can be obtained legally. In addition to documents and videos available via the Internet or provided by a human source, others are obtained after U.S. or allied forces have taken control of a facility or site formerly operated by a foreign government or terrorist group.”[6]
Security researcher Mark M. Lowenthal defines OSINT as “any and all information that can be derived from overt collection: all types of media, government reports and other documents, scientific research and reports, commercial vendors of information, the Internet, and so on. The main qualifiers to open-source information are that it does not require any type of clandestine collection techniques to obtain it and that it must be obtained through means that entirely meet the copyright and commercial requirements of the vendors where applicable.""[7]
OSINT in the United States traces its origins to the creation of the Foreign Broadcast Monitoring Service (FBMS), an agency responsible for the monitoring of foreign broadcasts. An example of their work is reflected in the application of the correlation of changes in the price of oranges in Paris with that of railway bridges being bombed successfully.[8]
The Aspin-Brown Commission stated in 1996 that US access to open sources was ""severely deficient"" and that this should be a ""top priority"" for both funding and DCI attention.[9]
In July 2004, following the September 11 attacks, the 9/11 Commission recommended the creation of an open-source intelligence agency.[10] In March 2005, the Iraq Intelligence Commission recommended[1] the creation of an open-source directorate at the CIA.
Following these recommendations, in November 2005 the Director of National Intelligence announced the creation of the DNI Open Source Center. The Center was established to collect information available from ""the Internet, databases, press, radio, television, video, geospatial data, photos and commercial imagery.""[11] In addition to collecting openly available information, it would train analysts to make better use of this information. The center absorbed the CIA's previously existing Foreign Broadcast Information Service (FBIS), originally established in 1941, with FBIS head Douglas Naquin named as director of the center.[12] Then, following the events of 9/11 the Intelligence Reform and Terrorism Prevention Act merged FBIS and other research elements into the Office of the Director of National Intelligence creating the Open Source Enterprise.
Furthermore, the private sector has invested in tools which aid in OSINT collection and analysis. Specifically, In-Q-Tel, a Central Intelligence Agency supported venture capital firm in Arlington, VA assisted companies develop web-monitoring and predictive analysis tools.
In December 2005, the Director of National Intelligence appointed Eliot A. Jardines as the Assistant Deputy Director of National Intelligence for Open Source to serve as the Intelligence Community's senior intelligence officer for open source and to provide strategy, guidance and oversight for the National Open Source Enterprise.[13] Mr. Jardines has established the National Open Source Enterprise[14] and authored intelligence community directive 301. In 2008, Mr. Jardines returned to the private sector and was succeeded by Dan Butler who is ADDNI/OS[15] and previously Mr. Jardines' Senior Advisor for Policy.[16]
OSINT is valuable because it has less rigorous processing and exploitation processes and timelines than more technical intelligence disciplines such as HUMINT, SIGINT, MASINT, GEOINT, etc. Additionally, OSINT collects a valuable variety of opinions because it encompasses a great variety of sources.
According to the Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction report submitted in March 2005, OSINT must be included in the all-source intelligence process for the following reasons (as stated in the report):
OSINT is a highly diverse form of intelligence collection and analysis. It does not have its own agency, however, units are scattered within the Department of Defense and the State Department.[18] Most OSINT collectors need to take precautions while collecting information from the Internet. This can come in the form of using a VPN to anonymize their identity and collect information more discreetly. This is where evaluating sources becomes important to the overall OSINT collection and analysis process. An OSINT analyst needs intelligence evaluation to determine a true process or expose a false process that would affect predicting the future. Finally, the analysts need to find use of the evaluated intelligence so that it can be incorporated into a finished classified, unclassified, or proprietary intelligence product.
Information collection in OSINT is generally a different problem from collection in other intelligence disciplines where obtaining the raw information to be analyzed may be the major difficulty, particularly if it is to be obtained from non-cooperative targets. In OSINT, the chief difficulty is in identifying relevant, reliable sources from the vast amount of publicly available information.[19] However, this is not as great a challenge for those who know how to access local knowledge and how to leverage human experts who can create new tailored knowledge on the fly.[citation needed]
There are several categories of tools intended for OSINT analysis. The first category includes open source tools to query multiple search engines simultaneously such as IntelTechniques or search engines that provide results separately such as All in One or DuckDuckGo. This category also includes social media search engines and search engines of domains and people such as Pipl.com, Whois.net, Website.informer. The second category is designed for big data analytics platforms such as DataWalk which combine OSINT insight with local, internal data for further visual analysis and to conduct link analysis to identify connections across a large volume of records.
A list of some tools used for collecting open source intelligence:
There are a large number of open-source activities taking place throughout the US Government. Frequently, these open-source activities are described as ""media monitoring"", ""media analysis"", ""internet research"" and ""public surveys"" but are open source nonetheless.
The Library of Congress sponsors the Federal Research Division (FRD) which conducts a great deal of tailored open-source research on a fee-for-service basis for the executive branch.
The US Intelligence Community's open-source activities (known as the National Open Source Enterprise) are dictated by Intelligence Community Directive 301 promulgated by the Director of National Intelligence.[20] The Directive establishes the authorities and responsibilities of the Assistant Deputy Director of National Intelligence for Open Source (ADDNI/OS), the DNI's Open Source Center and the National Open Source Committee.
Prior to the establishment of the National Open Source Enterprise, the Foreign Broadcast Information Service (FBIS), established in 1941, was the government's primary open-source unit, transcribing and translating foreign broadcasts. It absorbed the Defense Department's Joint Publications Research Service (JPRS), which did a similar function with foreign printed materials, including newspapers, magazines, and technical journals.
The former Under-Secretary of Defense for Intelligence, Dr. Stephen Cambone encouraged in part by the Defense Science Board reports on strategic communication and transition to and from hostilities, created the Defense Open Source Program (DOSP). The current under-secretary of defense for intelligence is assigned executive agency for this program to the Defense Intelligence Agency.
U.S. military offices that engage in OSINT activities include:
The Department of Homeland Security has an active open-source intelligence unit. In congressional testimony before the House Homeland Security Committee's Intelligence, Information Sharing and Terrorism Risk Assessment Subcommittee, Undersecretary of Homeland Security Charles Allen indicated on February 14, 2007, that he had established the ""Domestic Open Source Enterprise"" to support the Department's OSINT needs and that of state, local,Ο and tribal partners.
The law enforcement OSINT community applies open-source intelligence (OSINT) to the prediction, prevention, investigation, and prosecution of criminals including terrorists. Additionally, fusion centers around the US are increasingly utilizing OSINT to support their intelligence generation and investigations.
Examples of successful law enforcement OSINT include Scotland Yard OSINT; Royal Canadian Mounted Police (RCMP) OSINT.
INTERPOL and EUROPOL experimented with OSINT units for a time, but they appear to have atrophied with the departure of their individual champions.
New York Police Department (NYPD) is known to have an OSINT unit, as does the Los Angeles County Sheriff's Department, housed within the Emergency Operations Bureau and affiliated with the LA Joint Regional Intelligence Center.
Business OSINT encompasses Commercial Intelligence, Competitor Intelligence, and Business Intelligence, and is often a chief area of practice of private intelligence agencies.
Businesses may use information brokers and private investigators to collect and analyze relevant information for business purposes which may include the media, deep web, web 2.0 and commercial content.
Another related business group within the United States that relies upon OSINT is the commercial bail-bond industry. This related industry, servicing the court system, is apart from the above Business Intelligence sector.
OSINT is useful to bail-bond agencies that employ a private fugitive recovery agency to locate and apprehend their absent client; i.e., a criminal defendant who has failed to appear for court and subsequently a warrant for arrest was issued. OSINT is the first method of discovery to help locate the defendant when initial interviewing of the bond co-signers, defendant's relatives and friends is lacking. OSINT gathering leads the investigator to discover an alternate hypothesis to analyze and then match relevant data for making a prediction regarding the fugitive's location; e.g., data is scrubbed from web access on Facebook entries, Twitter messages, and Snapchat.
Should those methods fail, the next step is to seek the specialized behavioral intelligence services that reference OSINT to aid in establishing the veracity of subjects during the forensic interview and is used to create a behavioral profile. OSINT data is correlated with interview data to include a variety behavioral patterns; e.g., a list of daily personal contacts, habits of activities, visited places of interest, vehicles used, favorite group involvements, etc. According to the director, psychologist and forensic interviewer at MN-Behavioral Intelligence Agency, (2016) OSINT data base has to be critically filtered and analyzed before it can be applied within investigative interviewing and interrogation.
A main hindrance to practical OSINT is the volume of information it has to deal with (""information explosion""). The amount of data being distributed increases at a rate that it becomes difficult to evaluate sources in intelligence analysis.
Accredited journalists have some protection in asking questions, and researching for recognized media outlets. Even so, they can be imprisoned, even executed, for seeking out OSINT. Private individuals illegally collecting data for a foreign military or intelligence agency is considered espionage in most countries. Of course, espionage that is not treason (i.e. betraying one's country of citizenship) has been a tool of statecraft since ancient times.[21]
CITEREFUsername_Search | access-date=2018-05-29}} An OSINT Deep Web Search for usersnames and email addresses
"
1024202345,2021-05-20 18:26:51,"Open-source intelligence (OSINT) is a multi-factor (qualitative, quantitative) methodology for collecting, analyzing and making decisions about data accessible in publicly available sources to be used in an intelligence context. In the intelligence community, the term ""open"" refers to overt, publicly available sources (as opposed to covert or clandestine sources). OSINT under one name or another has been around for hundreds of years. With the advent of instant communications and rapid information transfer, a great deal of actionable and predictive intelligence can now be obtained from public, unclassified sources. It is not related to open-source software or collective intelligence.
OSINT is the collection and analysis of information that is gathered from public, or open, sources.[1] OSINT is primarily used in national security, law enforcement, and business intelligence functions and is of value to analysts who use non-sensitive intelligence in answering classified, unclassified, or proprietary intelligence requirements across the previous intelligence disciplines.
OSINT sources can be divided up into six different categories of information flow:[2]
OSINT is distinguished from research in that it applies the process of intelligence to create tailored knowledge supportive of a specific decision by a specific individual or group.[3]
OSINT is defined by both the U.S. Director of National Intelligence and the U.S. Department of Defense (DoD), as intelligence ""produced from publicly available information that is collected, exploited, and disseminated in a timely manner to an appropriate audience for the purpose of addressing a specific intelligence requirement.""[4]  As defined by NATO, OSINT is intelligence ""derived from publicly available information, as well as other unclassified information that has limited public distribution or access.""[5]
According to political scientist Jeffrey T. Richelson, “open source acquisition involves procuring verbal, written, or electronically transmitted material that can be obtained legally. In addition to documents and videos available via the Internet or provided by a human source, others are obtained after U.S. or allied forces have taken control of a facility or site formerly operated by a foreign government or terrorist group.”[6]
Security researcher Mark M. Lowenthal defines OSINT as “any and all information that can be derived from overt collection: all types of media, government reports and other documents, scientific research and reports, commercial vendors of information, the Internet, and so on. The main qualifiers to open-source information are that it does not require any type of clandestine collection techniques to obtain it and that it must be obtained through means that entirely meet the copyright and commercial requirements of the vendors where applicable.""[7]
OSINT in the United States traces its origins to the creation of the Foreign Broadcast Monitoring Service (FBMS), an agency responsible for the monitoring of foreign broadcasts. An example of their work is reflected in the application of the correlation of changes in the price of oranges in Paris with that of railway bridges being bombed successfully.[8]
The Aspin-Brown Commission stated in 1996 that US access to open sources was ""severely deficient"" and that this should be a ""top priority"" for both funding and DCI attention.[9]
In July 2004, following the September 11 attacks, the 9/11 Commission recommended the creation of an open-source intelligence agency.[10] In March 2005, the Iraq Intelligence Commission recommended[1] the creation of an open-source directorate at the CIA.
Following these recommendations, in November 2005 the Director of National Intelligence announced the creation of the DNI Open Source Center. The Center was established to collect information available from ""the Internet, databases, press, radio, television, video, geospatial data, photos and commercial imagery.""[11] In addition to collecting openly available information, it would train analysts to make better use of this information. The center absorbed the CIA's previously existing Foreign Broadcast Information Service (FBIS), originally established in 1941, with FBIS head Douglas Naquin named as director of the center.[12] Then, following the events of 9/11 the Intelligence Reform and Terrorism Prevention Act merged FBIS and other research elements into the Office of the Director of National Intelligence creating the Open Source Enterprise.
Furthermore, the private sector has invested in tools which aid in OSINT collection and analysis. Specifically, In-Q-Tel, a Central Intelligence Agency supported venture capital firm in Arlington, VA assisted companies develop web-monitoring and predictive analysis tools.
In December 2005, the Director of National Intelligence appointed Eliot A. Jardines as the Assistant Deputy Director of National Intelligence for Open Source to serve as the Intelligence Community's senior intelligence officer for open source and to provide strategy, guidance and oversight for the National Open Source Enterprise.[13] Mr. Jardines has established the National Open Source Enterprise[14] and authored intelligence community directive 301. In 2008, Mr. Jardines returned to the private sector and was succeeded by Dan Butler who is ADDNI/OS[15] and previously Mr. Jardines' Senior Advisor for Policy.[16]
OSINT is valuable because it has less rigorous processing and exploitation processes and timelines than more technical intelligence disciplines such as HUMINT, SIGINT, MASINT, GEOINT, etc. Additionally, OSINT collects a valuable variety of opinions because it encompasses a great variety of sources.
According to the Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction report submitted in March 2005, OSINT must be included in the all-source intelligence process for the following reasons (as stated in the report):
OSINT is a highly diverse form of intelligence collection and analysis. It does not have its own agency, however, units are scattered within the Department of Defense and the State Department.[18] Most OSINT collectors need to take precautions while collecting information from the Internet. This can come in the form of using a VPN to anonymize their identity and collect information more discreetly. This is where evaluating sources becomes important to the overall OSINT collection and analysis process. An OSINT analyst needs intelligence evaluation to determine a true process or expose a false process that would affect predicting the future. Finally, the analysts need to find use of the evaluated intelligence so that it can be incorporated into a finished classified, unclassified, or proprietary intelligence product.
Information collection in OSINT is generally a different problem from collection in other intelligence disciplines where obtaining the raw information to be analyzed may be the major difficulty, particularly if it is to be obtained from non-cooperative targets. In OSINT, the chief difficulty is in identifying relevant, reliable sources from the vast amount of publicly available information.[19] However, this is not as great a challenge for those who know how to access local knowledge and how to leverage human experts who can create new tailored knowledge on the fly.[citation needed]
There are several categories of tools intended for OSINT analysis. The first category includes open source tools to query multiple search engines simultaneously such as IntelTechniques or search engines that provide results separately such as All in One or DuckDuckGo. This category also includes social media search engines and search engines of domains and people such as Pipl.com, Whois.net, Website.informer. The second category is designed for big data analytics platforms such as DataWalk which combine OSINT insight with local, internal data for further visual analysis and to conduct link analysis to identify connections across a large volume of records.
A list of some tools used for collecting open source intelligence:
There are a large number of open-source activities taking place throughout the US Government. Frequently, these open-source activities are described as ""media monitoring"", ""media analysis"", ""internet research"" and ""public surveys"" but are open source nonetheless.
The Library of Congress sponsors the Federal Research Division (FRD) which conducts a great deal of tailored open-source research on a fee-for-service basis for the executive branch.
The US Intelligence Community's open-source activities (known as the National Open Source Enterprise) are dictated by Intelligence Community Directive 301 promulgated by the Director of National Intelligence.[20] The Directive establishes the authorities and responsibilities of the Assistant Deputy Director of National Intelligence for Open Source (ADDNI/OS), the DNI's Open Source Center and the National Open Source Committee.
Prior to the establishment of the National Open Source Enterprise, the Foreign Broadcast Information Service (FBIS), established in 1941, was the government's primary open-source unit, transcribing and translating foreign broadcasts. It absorbed the Defense Department's Joint Publications Research Service (JPRS), which did a similar function with foreign printed materials, including newspapers, magazines, and technical journals.
The former Under-Secretary of Defense for Intelligence, Dr. Stephen Cambone encouraged in part by the Defense Science Board reports on strategic communication and transition to and from hostilities, created the Defense Open Source Program (DOSP). The current under-secretary of defense for intelligence is assigned executive agency for this program to the Defense Intelligence Agency.
U.S. military offices that engage in OSINT activities include:
The Department of Homeland Security has an active open-source intelligence unit. In congressional testimony before the House Homeland Security Committee's Intelligence, Information Sharing and Terrorism Risk Assessment Subcommittee, Undersecretary of Homeland Security Charles Allen indicated on February 14, 2007, that he had established the ""Domestic Open Source Enterprise"" to support the Department's OSINT needs and that of state, local,Ο and tribal partners.
The law enforcement OSINT community applies open-source intelligence (OSINT) to the prediction, prevention, investigation, and prosecution of criminals including terrorists. Additionally, fusion centers around the US are increasingly utilizing OSINT to support their intelligence generation and investigations.
Examples of successful law enforcement OSINT include Scotland Yard OSINT; Royal Canadian Mounted Police (RCMP) OSINT.
INTERPOL and EUROPOL experimented with OSINT units for a time, but they appear to have atrophied with the departure of their individual champions.
New York Police Department (NYPD) is known to have an OSINT unit, as does the Los Angeles County Sheriff's Department, housed within the Emergency Operations Bureau and affiliated with the LA Joint Regional Intelligence Center.
Business OSINT encompasses Commercial Intelligence, Competitor Intelligence, and Business Intelligence, and is often a chief area of practice of private intelligence agencies.
Businesses may use information brokers and private investigators to collect and analyze relevant information for business purposes which may include the media, deep web, web 2.0 and commercial content.
Another related business group within the United States that relies upon OSINT is the commercial bail-bond industry. This related industry, servicing the court system, is apart from the above Business Intelligence sector.
OSINT is useful to bail-bond agencies that employ a private fugitive recovery agency to locate and apprehend their absent client; i.e., a criminal defendant who has failed to appear for court and subsequently a warrant for arrest was issued. OSINT is the first method of discovery to help locate the defendant when initial interviewing of the bond co-signers, defendant's relatives and friends is lacking. OSINT gathering leads the investigator to discover an alternate hypothesis to analyze and then match relevant data for making a prediction regarding the fugitive's location; e.g., data is scrubbed from web access on Facebook entries, Twitter messages, and Snapchat.
Should those methods fail, the next step is to seek the specialized behavioral intelligence services that reference OSINT to aid in establishing the veracity of subjects during the forensic interview and is used to create a behavioral profile. OSINT data is correlated with interview data to include a variety behavioral patterns; e.g., a list of daily personal contacts, habits of activities, visited places of interest, vehicles used, favorite group involvements, etc. According to the director, psychologist and forensic interviewer at MN-Behavioral Intelligence Agency, (2016) OSINT data base has to be critically filtered and analyzed before it can be applied within investigative interviewing and interrogation.
A main hindrance to practical OSINT is the volume of information it has to deal with (""information explosion""). The amount of data being distributed increases at a rate that it becomes difficult to evaluate sources in intelligence analysis.
Accredited journalists have some protection in asking questions, and researching for recognized media outlets. Even so, they can be imprisoned, even executed, for seeking out OSINT. Private individuals illegally collecting data for a foreign military or intelligence agency is considered espionage in most countries. Of course, espionage that is not treason (i.e. betraying one's country of citizenship) has been a tool of statecraft since ancient times.[21]
CITEREFUsername_Search | access-date=2018-05-29}} An OSINT Deep Web Search for usersnames and email addresses
"
1027834092,2021-06-10 08:14:28,"Open-source intelligence (OSINT) is a multi-factor (qualitative, quantitative) methodology for collecting, analyzing and making decisions about data accessible in publicly available sources to be used in an intelligence context. In the intelligence community, the term ""open"" refers to overt, publicly available sources (as opposed to covert or clandestine sources). OSINT under one name or another has been around for hundreds of years. With the advent of instant communications and rapid information transfer, a great deal of actionable and predictive intelligence can now be obtained from public, unclassified sources. It is not related to open-source software or collective intelligence.
OSINT is the collection and analysis of information that is gathered from public, or open, sources.[1] OSINT is primarily used in national security, law enforcement, and business intelligence functions and is of value to analysts who use non-sensitive intelligence in answering classified, unclassified, or proprietary intelligence requirements across the previous intelligence disciplines.
OSINT sources can be divided up into six different categories of information flow:[2]
OSINT is distinguished from research in that it applies the process of intelligence to create tailored knowledge supportive of a specific decision by a specific individual or group.[3]
OSINT is defined by both the U.S. Director of National Intelligence and the U.S. Department of Defense (DoD), as intelligence ""produced from publicly available information that is collected, exploited, and disseminated in a timely manner to an appropriate audience for the purpose of addressing a specific intelligence requirement.""[4]  As defined by NATO, OSINT is intelligence ""derived from publicly available information, as well as other unclassified information that has limited public distribution or access.""[5]
According to political scientist Jeffrey T. Richelson, “open source acquisition involves procuring verbal, written, or electronically transmitted material that can be obtained legally. In addition to documents and videos available via the Internet or provided by a human source, others are obtained after U.S. or allied forces have taken control of a facility or site formerly operated by a foreign government or terrorist group.”[6]
Security researcher Mark M. Lowenthal defines OSINT as “any and all information that can be derived from overt collection: all types of media, government reports and other documents, scientific research and reports, commercial vendors of information, the Internet, and so on. The main qualifiers to open-source information are that it does not require any type of clandestine collection techniques to obtain it and that it must be obtained through means that entirely meet the copyright and commercial requirements of the vendors where applicable.""[7]
OSINT in the United States traces its origins to the creation of the Foreign Broadcast Monitoring Service (FBMS), an agency responsible for the monitoring of foreign broadcasts. An example of their work is reflected in the application of the correlation of changes in the price of oranges in Paris with that of railway bridges being bombed successfully.[8]
The Aspin-Brown Commission stated in 1996 that US access to open sources was ""severely deficient"" and that this should be a ""top priority"" for both funding and DCI attention.[9]
In July 2004, following the September 11 attacks, the 9/11 Commission recommended the creation of an open-source intelligence agency.[10] In March 2005, the Iraq Intelligence Commission recommended[1] the creation of an open-source directorate at the CIA.
Following these recommendations, in November 2005 the Director of National Intelligence announced the creation of the DNI Open Source Center. The Center was established to collect information available from ""the Internet, databases, press, radio, television, video, geospatial data, photos and commercial imagery.""[11] In addition to collecting openly available information, it would train analysts to make better use of this information. The center absorbed the CIA's previously existing Foreign Broadcast Information Service (FBIS), originally established in 1941, with FBIS head Douglas Naquin named as director of the center.[12] Then, following the events of 9/11 the Intelligence Reform and Terrorism Prevention Act merged FBIS and other research elements into the Office of the Director of National Intelligence creating the Open Source Enterprise.
Furthermore, the private sector has invested in tools which aid in OSINT collection and analysis. Specifically, In-Q-Tel, a Central Intelligence Agency supported venture capital firm in Arlington, VA assisted companies develop web-monitoring and predictive analysis tools.
In December 2005, the Director of National Intelligence appointed Eliot A. Jardines as the Assistant Deputy Director of National Intelligence for Open Source to serve as the Intelligence Community's senior intelligence officer for open source and to provide strategy, guidance and oversight for the National Open Source Enterprise.[13] Mr. Jardines has established the National Open Source Enterprise[14] and authored intelligence community directive 301. In 2008, Mr. Jardines returned to the private sector and was succeeded by Dan Butler who is ADDNI/OS[15] and previously Mr. Jardines' Senior Advisor for Policy.[16]
OSINT is valuable because it has less rigorous processing and exploitation processes and timelines than more technical intelligence disciplines such as HUMINT, SIGINT, MASINT, GEOINT, etc. Additionally, OSINT collects a valuable variety of opinions because it encompasses a great variety of sources.
According to the Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction report submitted in March 2005, OSINT must be included in the all-source intelligence process for the following reasons (as stated in the report):
OSINT is a highly diverse form of intelligence collection and analysis. It does not have its own agency, however, units are scattered within the Department of Defense and the State Department.[18] Most OSINT collectors need to take precautions while collecting information from the Internet. This can come in the form of using a VPN to anonymize their identity and collect information more discreetly. This is where evaluating sources becomes important to the overall OSINT collection and analysis process. An OSINT analyst needs intelligence evaluation to determine a true process or expose a false process that would affect predicting the future. Finally, the analysts need to find use of the evaluated intelligence so that it can be incorporated into a finished classified, unclassified, or proprietary intelligence product.
Information collection in OSINT is generally a different problem from collection in other intelligence disciplines where obtaining the raw information to be analyzed may be the major difficulty, particularly if it is to be obtained from non-cooperative targets. In OSINT, the chief difficulty is in identifying relevant, reliable sources from the vast amount of publicly available information.[19] However, this is not as great a challenge for those who know how to access local knowledge and how to leverage human experts who can create new tailored knowledge on the fly.[citation needed]
There are several categories of tools intended for OSINT analysis. The first category includes open source tools to query multiple search engines simultaneously such as IntelTechniques or search engines that provide results separately such as All in One or DuckDuckGo. This category also includes social media search engines and search engines of domains and people such as Pipl.com, Whois.net, Website.informer. The second category is designed for big data analytics platforms such as DataWalk which combine OSINT insight with local, internal data for further visual analysis and to conduct link analysis to identify connections across a large volume of records.
A list of some tools used for collecting open source intelligence:
There are a large number of open-source activities taking place throughout the US Government. Frequently, these open-source activities are described as ""media monitoring"", ""media analysis"", ""internet research"" and ""public surveys"" but are open source nonetheless.
The Library of Congress sponsors the Federal Research Division (FRD) which conducts a great deal of tailored open-source research on a fee-for-service basis for the executive branch.
The US Intelligence Community's open-source activities (known as the National Open Source Enterprise) are dictated by Intelligence Community Directive 301 promulgated by the Director of National Intelligence.[20] The Directive establishes the authorities and responsibilities of the Assistant Deputy Director of National Intelligence for Open Source (ADDNI/OS), the DNI's Open Source Center and the National Open Source Committee.
Prior to the establishment of the National Open Source Enterprise, the Foreign Broadcast Information Service (FBIS), established in 1941, was the government's primary open-source unit, transcribing and translating foreign broadcasts. It absorbed the Defense Department's Joint Publications Research Service (JPRS), which did a similar function with foreign printed materials, including newspapers, magazines, and technical journals.
The former Under-Secretary of Defense for Intelligence, Dr. Stephen Cambone encouraged in part by the Defense Science Board reports on strategic communication and transition to and from hostilities, created the Defense Open Source Program (DOSP). The current under-secretary of defense for intelligence is assigned executive agency for this program to the Defense Intelligence Agency.
U.S. military offices that engage in OSINT activities include:
The Department of Homeland Security has an active open-source intelligence unit. In congressional testimony before the House Homeland Security Committee's Intelligence, Information Sharing and Terrorism Risk Assessment Subcommittee, Undersecretary of Homeland Security Charles Allen indicated on February 14, 2007, that he had established the ""Domestic Open Source Enterprise"" to support the Department's OSINT needs and that of state, local,Ο and tribal partners.
The law enforcement OSINT community applies open-source intelligence (OSINT) to the prediction, prevention, investigation, and prosecution of criminals including terrorists. Additionally, fusion centers around the US are increasingly utilizing OSINT to support their intelligence generation and investigations.
Examples of successful law enforcement OSINT include Scotland Yard OSINT; Royal Canadian Mounted Police (RCMP) OSINT.
INTERPOL and EUROPOL experimented with OSINT units for a time, but they appear to have atrophied with the departure of their individual champions.
New York Police Department (NYPD) is known to have an OSINT unit, as does the Los Angeles County Sheriff's Department, housed within the Emergency Operations Bureau and affiliated with the LA Joint Regional Intelligence Center.
Business OSINT encompasses Commercial Intelligence, Competitor Intelligence, and Business Intelligence, and is often a chief area of practice of private intelligence agencies.
Businesses may use information brokers and private investigators to collect and analyze relevant information for business purposes which may include the media, deep web, web 2.0 and commercial content.
Another related business group within the United States that relies upon OSINT is the commercial bail-bond industry. This related industry, servicing the court system, is apart from the above Business Intelligence sector.
OSINT is useful to bail-bond agencies that employ a private fugitive recovery agency to locate and apprehend their absent client; i.e., a criminal defendant who has failed to appear for court and subsequently a warrant for arrest was issued. OSINT is the first method of discovery to help locate the defendant when initial interviewing of the bond co-signers, defendant's relatives and friends is lacking. OSINT gathering leads the investigator to discover an alternate hypothesis to analyze and then match relevant data for making a prediction regarding the fugitive's location; e.g., data is scrubbed from web access on Facebook entries, Twitter messages, and Snapchat.
Should those methods fail, the next step is to seek the specialized behavioral intelligence services that reference OSINT to aid in establishing the veracity of subjects during the forensic interview and is used to create a behavioral profile. OSINT data is correlated with interview data to include a variety behavioral patterns; e.g., a list of daily personal contacts, habits of activities, visited places of interest, vehicles used, favorite group involvements, etc. According to the director, psychologist and forensic interviewer at MN-Behavioral Intelligence Agency, (2016) OSINT data base has to be critically filtered and analyzed before it can be applied within investigative interviewing and interrogation.
A main hindrance to practical OSINT is the volume of information it has to deal with (""information explosion""). The amount of data being distributed increases at a rate that it becomes difficult to evaluate sources in intelligence analysis.
Accredited journalists have some protection in asking questions, and researching for recognized media outlets. Even so, they can be imprisoned, even executed, for seeking out OSINT. Private individuals illegally collecting data for a foreign military or intelligence agency is considered espionage in most countries. Of course, espionage that is not treason (i.e. betraying one's country of citizenship) has been a tool of statecraft since ancient times.[21]
CITEREFUsername_Search | access-date=2018-05-29}} An OSINT Deep Web Search for usersnames and email addresses
"
1028003975,2021-06-11 08:26:52,"Open-source intelligence (OSINT) is a multi-factor (qualitative, quantitative) methodology for collecting, analyzing and making decisions about data accessible in publicly available sources to be used in an intelligence context. In the intelligence community, the term ""open"" refers to overt, publicly available sources (as opposed to covert or clandestine sources). OSINT under one name or another has been around for hundreds of years. With the advent of instant communications and rapid information transfer, a great deal of actionable and predictive intelligence can now be obtained from public, unclassified sources. It is not related to open-source software or collective intelligence.
OSINT is the collection and analysis of information that is gathered from public, or open, sources.[1] OSINT is primarily used in national security, law enforcement, and business intelligence functions and is of value to analysts who use non-sensitive intelligence in answering classified, unclassified, or proprietary intelligence requirements across the previous intelligence disciplines.
OSINT sources can be divided up into six different categories of information flow:[2]
OSINT is distinguished from research in that it applies the process of intelligence to create tailored knowledge supportive of a specific decision by a specific individual or group.[3]
OSINT is defined by both the U.S. Director of National Intelligence and the U.S. Department of Defense (DoD), as intelligence ""produced from publicly available information that is collected, exploited, and disseminated in a timely manner to an appropriate audience for the purpose of addressing a specific intelligence requirement.""[4]  As defined by NATO, OSINT is intelligence ""derived from publicly available information, as well as other unclassified information that has limited public distribution or access.""[5]
According to political scientist Jeffrey T. Richelson, “open source acquisition involves procuring verbal, written, or electronically transmitted material that can be obtained legally. In addition to documents and videos available via the Internet or provided by a human source, others are obtained after U.S. or allied forces have taken control of a facility or site formerly operated by a foreign government or terrorist group.”[6]
Security researcher Mark M. Lowenthal defines OSINT as “any and all information that can be derived from overt collection: all types of media, government reports and other documents, scientific research and reports, commercial vendors of information, the Internet, and so on. The main qualifiers to open-source information are that it does not require any type of clandestine collection techniques to obtain it and that it must be obtained through means that entirely meet the copyright and commercial requirements of the vendors where applicable.""[7]
OSINT in the United States traces its origins to the creation of the Foreign Broadcast Monitoring Service (FBMS), an agency responsible for the monitoring of foreign broadcasts. An example of their work is reflected in the application of the correlation of changes in the price of oranges in Paris with that of railway bridges being bombed successfully.[8]
The Aspin-Brown Commission stated in 1996 that US access to open sources was ""severely deficient"" and that this should be a ""top priority"" for both funding and DCI attention.[9]
In July 2004, following the September 11 attacks, the 9/11 Commission recommended the creation of an open-source intelligence agency.[10] In March 2005, the Iraq Intelligence Commission recommended[1] the creation of an open-source directorate at the CIA.
Following these recommendations, in November 2005 the Director of National Intelligence announced the creation of the DNI Open Source Center. The Center was established to collect information available from ""the Internet, databases, press, radio, television, video, geospatial data, photos and commercial imagery.""[11] In addition to collecting openly available information, it would train analysts to make better use of this information. The center absorbed the CIA's previously existing Foreign Broadcast Information Service (FBIS), originally established in 1941, with FBIS head Douglas Naquin named as director of the center.[12] Then, following the events of 9/11 the Intelligence Reform and Terrorism Prevention Act merged FBIS and other research elements into the Office of the Director of National Intelligence creating the Open Source Enterprise.
Furthermore, the private sector has invested in tools which aid in OSINT collection and analysis. Specifically, In-Q-Tel, a Central Intelligence Agency supported venture capital firm in Arlington, VA assisted companies develop web-monitoring and predictive analysis tools.
In December 2005, the Director of National Intelligence appointed Eliot A. Jardines as the Assistant Deputy Director of National Intelligence for Open Source to serve as the Intelligence Community's senior intelligence officer for open source and to provide strategy, guidance and oversight for the National Open Source Enterprise.[13] Mr. Jardines has established the National Open Source Enterprise[14] and authored intelligence community directive 301. In 2008, Mr. Jardines returned to the private sector and was succeeded by Dan Butler who is ADDNI/OS[15] and previously Mr. Jardines' Senior Advisor for Policy.[16]
OSINT is valuable because it has less rigorous processing and exploitation processes and timelines than more technical intelligence disciplines such as HUMINT, SIGINT, MASINT, GEOINT, etc. Additionally, OSINT collects a valuable variety of opinions because it encompasses a great variety of sources.
According to the Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction report submitted in March 2005, OSINT must be included in the all-source intelligence process for the following reasons (as stated in the report):
OSINT is a highly diverse form of intelligence collection and analysis. It does not have its own agency, however, units are scattered within the Department of Defense and the State Department.[18] Most OSINT collectors need to take precautions while collecting information from the Internet. This can come in the form of using a VPN to anonymize their identity and collect information more discreetly. This is where evaluating sources becomes important to the overall OSINT collection and analysis process. An OSINT analyst needs intelligence evaluation to determine a true process or expose a false process that would affect predicting the future. Finally, the analysts need to find use of the evaluated intelligence so that it can be incorporated into a finished classified, unclassified, or proprietary intelligence product.
Information collection in OSINT is generally a different problem from collection in other intelligence disciplines where obtaining the raw information to be analyzed may be the major difficulty, particularly if it is to be obtained from non-cooperative targets. In OSINT, the chief difficulty is in identifying relevant, reliable sources from the vast amount of publicly available information.[19] However, this is not as great a challenge for those who know how to access local knowledge and how to leverage human experts who can create new tailored knowledge on the fly.[citation needed]
There are several categories of tools intended for OSINT analysis. The first category includes open source tools to query multiple search engines simultaneously such as IntelTechniques or search engines that provide results separately such as All in One or DuckDuckGo. This category also includes social media search engines and search engines of domains and people such as Pipl.com, Whois.net, Website.informer. The second category is designed for big data analytics platforms such as DataWalk which combine OSINT insight with local, internal data for further visual analysis and to conduct link analysis to identify connections across a large volume of records.
A list of some tools used for collecting open source intelligence:
There are a large number of open-source activities taking place throughout the US Government. Frequently, these open-source activities are described as ""media monitoring"", ""media analysis"", ""internet research"" and ""public surveys"" but are open source nonetheless.
The Library of Congress sponsors the Federal Research Division (FRD) which conducts a great deal of tailored open-source research on a fee-for-service basis for the executive branch.
The US Intelligence Community's open-source activities (known as the National Open Source Enterprise) are dictated by Intelligence Community Directive 301 promulgated by the Director of National Intelligence.[20] The Directive establishes the authorities and responsibilities of the Assistant Deputy Director of National Intelligence for Open Source (ADDNI/OS), the DNI's Open Source Center and the National Open Source Committee.
Prior to the establishment of the National Open Source Enterprise, the Foreign Broadcast Information Service (FBIS), established in 1941, was the government's primary open-source unit, transcribing and translating foreign broadcasts. It absorbed the Defense Department's Joint Publications Research Service (JPRS), which did a similar function with foreign printed materials, including newspapers, magazines, and technical journals.
The former Under-Secretary of Defense for Intelligence, Dr. Stephen Cambone encouraged in part by the Defense Science Board reports on strategic communication and transition to and from hostilities, created the Defense Open Source Program (DOSP). The current under-secretary of defense for intelligence is assigned executive agency for this program to the Defense Intelligence Agency.
U.S. military offices that engage in OSINT activities include:
The Department of Homeland Security has an active open-source intelligence unit. In congressional testimony before the House Homeland Security Committee's Intelligence, Information Sharing and Terrorism Risk Assessment Subcommittee, Undersecretary of Homeland Security Charles Allen indicated on February 14, 2007, that he had established the ""Domestic Open Source Enterprise"" to support the Department's OSINT needs and that of state, local,Ο and tribal partners.
The law enforcement OSINT community applies open-source intelligence (OSINT) to the prediction, prevention, investigation, and prosecution of criminals including terrorists. Additionally, fusion centers around the US are increasingly utilizing OSINT to support their intelligence generation and investigations.
Examples of successful law enforcement OSINT include Scotland Yard OSINT; Royal Canadian Mounted Police (RCMP) OSINT.
INTERPOL and EUROPOL experimented with OSINT units for a time, but they appear to have atrophied with the departure of their individual champions.
New York Police Department (NYPD) is known to have an OSINT unit, as does the Los Angeles County Sheriff's Department, housed within the Emergency Operations Bureau and affiliated with the LA Joint Regional Intelligence Center.
Business OSINT encompasses Commercial Intelligence, Competitor Intelligence, and Business Intelligence, and is often a chief area of practice of private intelligence agencies.
Businesses may use information brokers and private investigators to collect and analyze relevant information for business purposes which may include the media, deep web, web 2.0 and commercial content.
Another related business group within the United States that relies upon OSINT is the commercial bail-bond industry. This related industry, servicing the court system, is apart from the above Business Intelligence sector.
OSINT is useful to bail-bond agencies that employ a private fugitive recovery agency to locate and apprehend their absent client; i.e., a criminal defendant who has failed to appear for court and subsequently a warrant for arrest was issued. OSINT is the first method of discovery to help locate the defendant when initial interviewing of the bond co-signers, defendant's relatives and friends is lacking. OSINT gathering leads the investigator to discover an alternate hypothesis to analyze and then match relevant data for making a prediction regarding the fugitive's location; e.g., data is scrubbed from web access on Facebook entries, Twitter messages, and Snapchat.
Should those methods fail, the next step is to seek the specialized behavioral intelligence services that reference OSINT to aid in establishing the veracity of subjects during the forensic interview and is used to create a behavioral profile. OSINT data is correlated with interview data to include a variety behavioral patterns; e.g., a list of daily personal contacts, habits of activities, visited places of interest, vehicles used, favorite group involvements, etc. According to the director, psychologist and forensic interviewer at MN-Behavioral Intelligence Agency, (2016) OSINT data base has to be critically filtered and analyzed before it can be applied within investigative interviewing and interrogation.
A main hindrance to practical OSINT is the volume of information it has to deal with (""information explosion""). The amount of data being distributed increases at a rate that it becomes difficult to evaluate sources in intelligence analysis.
Accredited journalists have some protection in asking questions, and researching for recognized media outlets. Even so, they can be imprisoned, even executed, for seeking out OSINT. Private individuals illegally collecting data for a foreign military or intelligence agency is considered espionage in most countries. Of course, espionage that is not treason (i.e. betraying one's country of citizenship) has been a tool of statecraft since ancient times.[21]
CITEREFUsername_Search | access-date=2018-05-29}} An OSINT Deep Web Search for usersnames and email addresses
"
1028003996,2021-06-11 08:27:07,"Open-source intelligence (OSINT) is a multi-factor (qualitative, quantitative) methodology for collecting, analyzing and making decisions about data accessible in publicly available sources to be used in an intelligence context. In the intelligence community, the term ""open"" refers to overt, publicly available sources (as opposed to covert or clandestine sources). OSINT under one name or another has been around for hundreds of years. With the advent of instant communications and rapid information transfer, a great deal of actionable and predictive intelligence can now be obtained from public, unclassified sources. It is not related to open-source software or collective intelligence.
OSINT is the collection and analysis of information that is gathered from public, or open, sources.[1] OSINT is primarily used in national security, law enforcement, and business intelligence functions and is of value to analysts who use non-sensitive intelligence in answering classified, unclassified, or proprietary intelligence requirements across the previous intelligence disciplines.
OSINT sources can be divided up into six different categories of information flow:[2]
OSINT is distinguished from research in that it applies the process of intelligence to create tailored knowledge supportive of a specific decision by a specific individual or group.[3]
OSINT is defined by both the U.S. Director of National Intelligence and the U.S. Department of Defense (DoD), as intelligence ""produced from publicly available information that is collected, exploited, and disseminated in a timely manner to an appropriate audience for the purpose of addressing a specific intelligence requirement.""[4]  As defined by NATO, OSINT is intelligence ""derived from publicly available information, as well as other unclassified information that has limited public distribution or access.""[5]
According to political scientist Jeffrey T. Richelson, “open source acquisition involves procuring verbal, written, or electronically transmitted material that can be obtained legally. In addition to documents and videos available via the Internet or provided by a human source, others are obtained after U.S. or allied forces have taken control of a facility or site formerly operated by a foreign government or terrorist group.”[6]
Security researcher Mark M. Lowenthal defines OSINT as “any and all information that can be derived from overt collection: all types of media, government reports and other documents, scientific research and reports, commercial vendors of information, the Internet, and so on. The main qualifiers to open-source information are that it does not require any type of clandestine collection techniques to obtain it and that it must be obtained through means that entirely meet the copyright and commercial requirements of the vendors where applicable.""[7]
OSINT in the United States traces its origins to the creation of the Foreign Broadcast Monitoring Service (FBMS), an agency responsible for the monitoring of foreign broadcasts. An example of their work is reflected in the application of the correlation of changes in the price of oranges in Paris with that of railway bridges being bombed successfully.[8]
The Aspin-Brown Commission stated in 1996 that US access to open sources was ""severely deficient"" and that this should be a ""top priority"" for both funding and DCI attention.[9]
In July 2004, following the September 11 attacks, the 9/11 Commission recommended the creation of an open-source intelligence agency.[10] In March 2005, the Iraq Intelligence Commission recommended[1] the creation of an open-source directorate at the CIA.
Following these recommendations, in November 2005 the Director of National Intelligence announced the creation of the DNI Open Source Center. The Center was established to collect information available from ""the Internet, databases, press, radio, television, video, geospatial data, photos and commercial imagery.""[11] In addition to collecting openly available information, it would train analysts to make better use of this information. The center absorbed the CIA's previously existing Foreign Broadcast Information Service (FBIS), originally established in 1941, with FBIS head Douglas Naquin named as director of the center.[12] Then, following the events of 9/11 the Intelligence Reform and Terrorism Prevention Act merged FBIS and other research elements into the Office of the Director of National Intelligence creating the Open Source Enterprise.
Furthermore, the private sector has invested in tools which aid in OSINT collection and analysis. Specifically, In-Q-Tel, a Central Intelligence Agency supported venture capital firm in Arlington, VA assisted companies develop web-monitoring and predictive analysis tools.
In December 2005, the Director of National Intelligence appointed Eliot A. Jardines as the Assistant Deputy Director of National Intelligence for Open Source to serve as the Intelligence Community's senior intelligence officer for open source and to provide strategy, guidance and oversight for the National Open Source Enterprise.[13] Mr. Jardines has established the National Open Source Enterprise[14] and authored intelligence community directive 301. In 2008, Mr. Jardines returned to the private sector and was succeeded by Dan Butler who is ADDNI/OS[15] and previously Mr. Jardines' Senior Advisor for Policy.[16]
OSINT is valuable because it has less rigorous processing and exploitation processes and timelines than more technical intelligence disciplines such as HUMINT, SIGINT, MASINT, GEOINT, etc. Additionally, OSINT collects a valuable variety of opinions because it encompasses a great variety of sources.
According to the Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction report submitted in March 2005, OSINT must be included in the all-source intelligence process for the following reasons (as stated in the report):
OSINT is a highly diverse form of intelligence collection and analysis. It does not have its own agency, however, units are scattered within the Department of Defense and the State Department.[18] Most OSINT collectors need to take precautions while collecting information from the Internet. This can come in the form of using a VPN to anonymize their identity and collect information more discreetly. This is where evaluating sources becomes important to the overall OSINT collection and analysis process. An OSINT analyst needs intelligence evaluation to determine a true process or expose a false process that would affect predicting the future. Finally, the analysts need to find use of the evaluated intelligence so that it can be incorporated into a finished classified, unclassified, or proprietary intelligence product.
Information collection in OSINT is generally a different problem from collection in other intelligence disciplines where obtaining the raw information to be analyzed may be the major difficulty, particularly if it is to be obtained from non-cooperative targets. In OSINT, the chief difficulty is in identifying relevant, reliable sources from the vast amount of publicly available information.[19] However, this is not as great a challenge for those who know how to access local knowledge and how to leverage human experts who can create new tailored knowledge on the fly.[citation needed]
There are several categories of tools intended for OSINT analysis. The first category includes open source tools to query multiple search engines simultaneously such as IntelTechniques or search engines that provide results separately such as All in One or DuckDuckGo. This category also includes social media search engines and search engines of domains and people such as Pipl.com, Whois.net, Website.informer. The second category is designed for big data analytics platforms such as DataWalk which combine OSINT insight with local, internal data for further visual analysis and to conduct link analysis to identify connections across a large volume of records.
A list of some tools used for collecting open source intelligence:
There are a large number of open-source activities taking place throughout the US Government. Frequently, these open-source activities are described as ""media monitoring"", ""media analysis"", ""internet research"" and ""public surveys"" but are open source nonetheless.
The Library of Congress sponsors the Federal Research Division (FRD) which conducts a great deal of tailored open-source research on a fee-for-service basis for the executive branch.
The US Intelligence Community's open-source activities (known as the National Open Source Enterprise) are dictated by Intelligence Community Directive 301 promulgated by the Director of National Intelligence.[20] The Directive establishes the authorities and responsibilities of the Assistant Deputy Director of National Intelligence for Open Source (ADDNI/OS), the DNI's Open Source Center and the National Open Source Committee.
Prior to the establishment of the National Open Source Enterprise, the Foreign Broadcast Information Service (FBIS), established in 1941, was the government's primary open-source unit, transcribing and translating foreign broadcasts. It absorbed the Defense Department's Joint Publications Research Service (JPRS), which did a similar function with foreign printed materials, including newspapers, magazines, and technical journals.
The former Under-Secretary of Defense for Intelligence, Dr. Stephen Cambone encouraged in part by the Defense Science Board reports on strategic communication and transition to and from hostilities, created the Defense Open Source Program (DOSP). The current under-secretary of defense for intelligence is assigned executive agency for this program to the Defense Intelligence Agency.
U.S. military offices that engage in OSINT activities include:
The Department of Homeland Security has an active open-source intelligence unit. In congressional testimony before the House Homeland Security Committee's Intelligence, Information Sharing and Terrorism Risk Assessment Subcommittee, Undersecretary of Homeland Security Charles Allen indicated on February 14, 2007, that he had established the ""Domestic Open Source Enterprise"" to support the Department's OSINT needs and that of state, local,Ο and tribal partners.
The law enforcement OSINT community applies open-source intelligence (OSINT) to the prediction, prevention, investigation, and prosecution of criminals including terrorists. Additionally, fusion centers around the US are increasingly utilizing OSINT to support their intelligence generation and investigations.
Examples of successful law enforcement OSINT include Scotland Yard OSINT; Royal Canadian Mounted Police (RCMP) OSINT.
INTERPOL and EUROPOL experimented with OSINT units for a time, but they appear to have atrophied with the departure of their individual champions.
New York Police Department (NYPD) is known to have an OSINT unit, as does the Los Angeles County Sheriff's Department, housed within the Emergency Operations Bureau and affiliated with the LA Joint Regional Intelligence Center.
Business OSINT encompasses Commercial Intelligence, Competitor Intelligence, and Business Intelligence, and is often a chief area of practice of private intelligence agencies.
Businesses may use information brokers and private investigators to collect and analyze relevant information for business purposes which may include the media, deep web, web 2.0 and commercial content.
Another related business group within the United States that relies upon OSINT is the commercial bail-bond industry. This related industry, servicing the court system, is apart from the above Business Intelligence sector.
OSINT is useful to bail-bond agencies that employ a private fugitive recovery agency to locate and apprehend their absent client; i.e., a criminal defendant who has failed to appear for court and subsequently a warrant for arrest was issued. OSINT is the first method of discovery to help locate the defendant when initial interviewing of the bond co-signers, defendant's relatives and friends is lacking. OSINT gathering leads the investigator to discover an alternate hypothesis to analyze and then match relevant data for making a prediction regarding the fugitive's location; e.g., data is scrubbed from web access on Facebook entries, Twitter messages, and Snapchat.
Should those methods fail, the next step is to seek the specialized behavioral intelligence services that reference OSINT to aid in establishing the veracity of subjects during the forensic interview and is used to create a behavioral profile. OSINT data is correlated with interview data to include a variety behavioral patterns; e.g., a list of daily personal contacts, habits of activities, visited places of interest, vehicles used, favorite group involvements, etc. According to the director, psychologist and forensic interviewer at MN-Behavioral Intelligence Agency, (2016) OSINT data base has to be critically filtered and analyzed before it can be applied within investigative interviewing and interrogation.
A main hindrance to practical OSINT is the volume of information it has to deal with (""information explosion""). The amount of data being distributed increases at a rate that it becomes difficult to evaluate sources in intelligence analysis.
Accredited journalists have some protection in asking questions, and researching for recognized media outlets. Even so, they can be imprisoned, even executed, for seeking out OSINT. Private individuals illegally collecting data for a foreign military or intelligence agency is considered espionage in most countries. Of course, espionage that is not treason (i.e. betraying one's country of citizenship) has been a tool of statecraft since ancient times.[21]
CITEREFUsername_Search | access-date=2018-05-29}} An OSINT Deep Web Search for usersnames and email addresses
"
1028004109,2021-06-11 08:28:23,"Open-source intelligence (OSINT) is a multi-factor (qualitative, quantitative) methodology for collecting, analyzing and making decisions about data accessible in publicly available sources to be used in an intelligence context. In the intelligence community, the term ""open"" refers to overt, publicly available sources (as opposed to covert or clandestine sources). OSINT under one name or another has been around for hundreds of years. With the advent of instant communications and rapid information transfer, a great deal of actionable and predictive intelligence can now be obtained from public, unclassified sources. It is not related to open-source software or collective intelligence.
OSINT is the collection and analysis of information that is gathered from public, or open, sources.[1] OSINT is primarily used in national security, law enforcement, and business intelligence functions and is of value to analysts who use non-sensitive intelligence in answering classified, unclassified, or proprietary intelligence requirements across the previous intelligence disciplines.
OSINT sources can be divided up into six different categories of information flow:[2]
OSINT is distinguished from research in that it applies the process of intelligence to create tailored knowledge supportive of a specific decision by a specific individual or group.[3]
OSINT is defined by both the U.S. Director of National Intelligence and the U.S. Department of Defense (DoD), as intelligence ""produced from publicly available information that is collected, exploited, and disseminated in a timely manner to an appropriate audience for the purpose of addressing a specific intelligence requirement.""[4]  As defined by NATO, OSINT is intelligence ""derived from publicly available information, as well as other unclassified information that has limited public distribution or access.""[5]
According to political scientist Jeffrey T. Richelson, “open source acquisition involves procuring verbal, written, or electronically transmitted material that can be obtained legally. In addition to documents and videos available via the Internet or provided by a human source, others are obtained after U.S. or allied forces have taken control of a facility or site formerly operated by a foreign government or terrorist group.”[6]
Security researcher Mark M. Lowenthal defines OSINT as “any and all information that can be derived from overt collection: all types of media, government reports and other documents, scientific research and reports, commercial vendors of information, the Internet, and so on. The main qualifiers to open-source information are that it does not require any type of clandestine collection techniques to obtain it and that it must be obtained through means that entirely meet the copyright and commercial requirements of the vendors where applicable.""[7]
OSINT in the United States traces its origins to the creation of the Foreign Broadcast Monitoring Service (FBMS), an agency responsible for the monitoring of foreign broadcasts. An example of their work is reflected in the application of the correlation of changes in the price of oranges in Paris with that of railway bridges being bombed successfully.[8]
The Aspin-Brown Commission stated in 1996 that US access to open sources was ""severely deficient"" and that this should be a ""top priority"" for both funding and DCI attention.[9]
In July 2004, following the September 11 attacks, the 9/11 Commission recommended the creation of an open-source intelligence agency.[10] In March 2005, the Iraq Intelligence Commission recommended[1] the creation of an open-source directorate at the CIA.
Following these recommendations, in November 2005 the Director of National Intelligence announced the creation of the DNI Open Source Center. The Center was established to collect information available from ""the Internet, databases, press, radio, television, video, geospatial data, photos and commercial imagery.""[11] In addition to collecting openly available information, it would train analysts to make better use of this information. The center absorbed the CIA's previously existing Foreign Broadcast Information Service (FBIS), originally established in 1941, with FBIS head Douglas Naquin named as director of the center.[12] Then, following the events of 9/11 the Intelligence Reform and Terrorism Prevention Act merged FBIS and other research elements into the Office of the Director of National Intelligence creating the Open Source Enterprise.
Furthermore, the private sector has invested in tools which aid in OSINT collection and analysis. Specifically, In-Q-Tel, a Central Intelligence Agency supported venture capital firm in Arlington, VA assisted companies develop web-monitoring and predictive analysis tools.
In December 2005, the Director of National Intelligence appointed Eliot A. Jardines as the Assistant Deputy Director of National Intelligence for Open Source to serve as the Intelligence Community's senior intelligence officer for open source and to provide strategy, guidance and oversight for the National Open Source Enterprise.[13] Mr. Jardines has established the National Open Source Enterprise[14] and authored intelligence community directive 301. In 2008, Mr. Jardines returned to the private sector and was succeeded by Dan Butler who is ADDNI/OS[15] and previously Mr. Jardines' Senior Advisor for Policy.[16]
OSINT is valuable because it has less rigorous processing and exploitation processes and timelines than more technical intelligence disciplines such as HUMINT, SIGINT, MASINT, GEOINT, etc. Additionally, OSINT collects a valuable variety of opinions because it encompasses a great variety of sources.
According to the Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction report submitted in March 2005, OSINT must be included in the all-source intelligence process for the following reasons (as stated in the report):
OSINT is a highly diverse form of intelligence collection and analysis. It does not have its own agency, however, units are scattered within the Department of Defense and the State Department.[18] Most OSINT collectors need to take precautions while collecting information from the Internet. This can come in the form of using a VPN to anonymize their identity and collect information more discreetly. This is where evaluating sources becomes important to the overall OSINT collection and analysis process. An OSINT analyst needs intelligence evaluation to determine a true process or expose a false process that would affect predicting the future. Finally, the analysts need to find use of the evaluated intelligence so that it can be incorporated into a finished classified, unclassified, or proprietary intelligence product.
Information collection in OSINT is generally a different problem from collection in other intelligence disciplines where obtaining the raw information to be analyzed may be the major difficulty, particularly if it is to be obtained from non-cooperative targets. In OSINT, the chief difficulty is in identifying relevant, reliable sources from the vast amount of publicly available information.[19] However, this is not as great a challenge for those who know how to access local knowledge and how to leverage human experts who can create new tailored knowledge on the fly.[citation needed]
There are several categories of tools intended for OSINT analysis. The first category includes open source tools to query multiple search engines simultaneously such as IntelTechniques or search engines that provide results separately such as All in One or DuckDuckGo. This category also includes social media search engines and search engines of domains and people such as Pipl.com, Whois.net, Website.informer. The second category is designed for big data analytics platforms such as DataWalk which combine OSINT insight with local, internal data for further visual analysis and to conduct link analysis to identify connections across a large volume of records.
A list of some tools used for collecting open source intelligence:
There are a large number of open-source activities taking place throughout the US Government. Frequently, these open-source activities are described as ""media monitoring"", ""media analysis"", ""internet research"" and ""public surveys"" but are open source nonetheless.  One of the largesrt of these is a global reverse username search, https://www.usersearch.org.
The Library of Congress sponsors the Federal Research Division (FRD) which conducts a great deal of tailored open-source research on a fee-for-service basis for the executive branch.
The US Intelligence Community's open-source activities (known as the National Open Source Enterprise) are dictated by Intelligence Community Directive 301 promulgated by the Director of National Intelligence.[20] The Directive establishes the authorities and responsibilities of the Assistant Deputy Director of National Intelligence for Open Source (ADDNI/OS), the DNI's Open Source Center and the National Open Source Committee.
Prior to the establishment of the National Open Source Enterprise, the Foreign Broadcast Information Service (FBIS), established in 1941, was the government's primary open-source unit, transcribing and translating foreign broadcasts. It absorbed the Defense Department's Joint Publications Research Service (JPRS), which did a similar function with foreign printed materials, including newspapers, magazines, and technical journals.
The former Under-Secretary of Defense for Intelligence, Dr. Stephen Cambone encouraged in part by the Defense Science Board reports on strategic communication and transition to and from hostilities, created the Defense Open Source Program (DOSP). The current under-secretary of defense for intelligence is assigned executive agency for this program to the Defense Intelligence Agency.
U.S. military offices that engage in OSINT activities include:
The Department of Homeland Security has an active open-source intelligence unit. In congressional testimony before the House Homeland Security Committee's Intelligence, Information Sharing and Terrorism Risk Assessment Subcommittee, Undersecretary of Homeland Security Charles Allen indicated on February 14, 2007, that he had established the ""Domestic Open Source Enterprise"" to support the Department's OSINT needs and that of state, local,Ο and tribal partners.
The law enforcement OSINT community applies open-source intelligence (OSINT) to the prediction, prevention, investigation, and prosecution of criminals including terrorists. Additionally, fusion centers around the US are increasingly utilizing OSINT to support their intelligence generation and investigations.
Examples of successful law enforcement OSINT include Scotland Yard OSINT; Royal Canadian Mounted Police (RCMP) OSINT.
INTERPOL and EUROPOL experimented with OSINT units for a time, but they appear to have atrophied with the departure of their individual champions.
New York Police Department (NYPD) is known to have an OSINT unit, as does the Los Angeles County Sheriff's Department, housed within the Emergency Operations Bureau and affiliated with the LA Joint Regional Intelligence Center.
Business OSINT encompasses Commercial Intelligence, Competitor Intelligence, and Business Intelligence, and is often a chief area of practice of private intelligence agencies.
Businesses may use information brokers and private investigators to collect and analyze relevant information for business purposes which may include the media, deep web, web 2.0 and commercial content.
Another related business group within the United States that relies upon OSINT is the commercial bail-bond industry. This related industry, servicing the court system, is apart from the above Business Intelligence sector.
OSINT is useful to bail-bond agencies that employ a private fugitive recovery agency to locate and apprehend their absent client; i.e., a criminal defendant who has failed to appear for court and subsequently a warrant for arrest was issued. OSINT is the first method of discovery to help locate the defendant when initial interviewing of the bond co-signers, defendant's relatives and friends is lacking. OSINT gathering leads the investigator to discover an alternate hypothesis to analyze and then match relevant data for making a prediction regarding the fugitive's location; e.g., data is scrubbed from web access on Facebook entries, Twitter messages, and Snapchat.
Should those methods fail, the next step is to seek the specialized behavioral intelligence services that reference OSINT to aid in establishing the veracity of subjects during the forensic interview and is used to create a behavioral profile. OSINT data is correlated with interview data to include a variety behavioral patterns; e.g., a list of daily personal contacts, habits of activities, visited places of interest, vehicles used, favorite group involvements, etc. According to the director, psychologist and forensic interviewer at MN-Behavioral Intelligence Agency, (2016) OSINT data base has to be critically filtered and analyzed before it can be applied within investigative interviewing and interrogation.
A main hindrance to practical OSINT is the volume of information it has to deal with (""information explosion""). The amount of data being distributed increases at a rate that it becomes difficult to evaluate sources in intelligence analysis.
Accredited journalists have some protection in asking questions, and researching for recognized media outlets. Even so, they can be imprisoned, even executed, for seeking out OSINT. Private individuals illegally collecting data for a foreign military or intelligence agency is considered espionage in most countries. Of course, espionage that is not treason (i.e. betraying one's country of citizenship) has been a tool of statecraft since ancient times.[21]
CITEREFUsername_Search | access-date=2018-05-29}} An OSINT Deep Web Search for usersnames and email addresses
"
1028004221,2021-06-11 08:29:35,"Open-source intelligence (OSINT) is a multi-factor (qualitative, quantitative) methodology for collecting, analyzing and making decisions about data accessible in publicly available sources to be used in an intelligence context. In the intelligence community, the term ""open"" refers to overt, publicly available sources (as opposed to covert or clandestine sources). OSINT under one name or another has been around for hundreds of years. With the advent of instant communications and rapid information transfer, a great deal of actionable and predictive intelligence can now be obtained from public, unclassified sources. It is not related to open-source software or collective intelligence.
OSINT is the collection and analysis of information that is gathered from public, or open, sources.[1] OSINT is primarily used in national security, law enforcement, and business intelligence functions and is of value to analysts who use non-sensitive intelligence in answering classified, unclassified, or proprietary intelligence requirements across the previous intelligence disciplines.
OSINT sources can be divided up into six different categories of information flow:[2]
OSINT is distinguished from research in that it applies the process of intelligence to create tailored knowledge supportive of a specific decision by a specific individual or group.[3]
OSINT is defined by both the U.S. Director of National Intelligence and the U.S. Department of Defense (DoD), as intelligence ""produced from publicly available information that is collected, exploited, and disseminated in a timely manner to an appropriate audience for the purpose of addressing a specific intelligence requirement.""[4]  As defined by NATO, OSINT is intelligence ""derived from publicly available information, as well as other unclassified information that has limited public distribution or access.""[5]
According to political scientist Jeffrey T. Richelson, “open source acquisition involves procuring verbal, written, or electronically transmitted material that can be obtained legally. In addition to documents and videos available via the Internet or provided by a human source, others are obtained after U.S. or allied forces have taken control of a facility or site formerly operated by a foreign government or terrorist group.”[6]
Security researcher Mark M. Lowenthal defines OSINT as “any and all information that can be derived from overt collection: all types of media, government reports and other documents, scientific research and reports, commercial vendors of information, the Internet, and so on. The main qualifiers to open-source information are that it does not require any type of clandestine collection techniques to obtain it and that it must be obtained through means that entirely meet the copyright and commercial requirements of the vendors where applicable.""[7]
OSINT in the United States traces its origins to the creation of the Foreign Broadcast Monitoring Service (FBMS), an agency responsible for the monitoring of foreign broadcasts. An example of their work is reflected in the application of the correlation of changes in the price of oranges in Paris with that of railway bridges being bombed successfully.[8]
The Aspin-Brown Commission stated in 1996 that US access to open sources was ""severely deficient"" and that this should be a ""top priority"" for both funding and DCI attention.[9]
In July 2004, following the September 11 attacks, the 9/11 Commission recommended the creation of an open-source intelligence agency.[10] In March 2005, the Iraq Intelligence Commission recommended[1] the creation of an open-source directorate at the CIA.
Following these recommendations, in November 2005 the Director of National Intelligence announced the creation of the DNI Open Source Center. The Center was established to collect information available from ""the Internet, databases, press, radio, television, video, geospatial data, photos and commercial imagery.""[11] In addition to collecting openly available information, it would train analysts to make better use of this information. The center absorbed the CIA's previously existing Foreign Broadcast Information Service (FBIS), originally established in 1941, with FBIS head Douglas Naquin named as director of the center.[12] Then, following the events of 9/11 the Intelligence Reform and Terrorism Prevention Act merged FBIS and other research elements into the Office of the Director of National Intelligence creating the Open Source Enterprise.
Furthermore, the private sector has invested in tools which aid in OSINT collection and analysis. Specifically, In-Q-Tel, a Central Intelligence Agency supported venture capital firm in Arlington, VA assisted companies develop web-monitoring and predictive analysis tools.
In December 2005, the Director of National Intelligence appointed Eliot A. Jardines as the Assistant Deputy Director of National Intelligence for Open Source to serve as the Intelligence Community's senior intelligence officer for open source and to provide strategy, guidance and oversight for the National Open Source Enterprise.[13] Mr. Jardines has established the National Open Source Enterprise[14] and authored intelligence community directive 301. In 2008, Mr. Jardines returned to the private sector and was succeeded by Dan Butler who is ADDNI/OS[15] and previously Mr. Jardines' Senior Advisor for Policy.[16]
OSINT is valuable because it has less rigorous processing and exploitation processes and timelines than more technical intelligence disciplines such as HUMINT, SIGINT, MASINT, GEOINT, etc. Additionally, OSINT collects a valuable variety of opinions because it encompasses a great variety of sources.
According to the Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction report submitted in March 2005, OSINT must be included in the all-source intelligence process for the following reasons (as stated in the report):
OSINT is a highly diverse form of intelligence collection and analysis. It does not have its own agency, however, units are scattered within the Department of Defense and the State Department.[18] Most OSINT collectors need to take precautions while collecting information from the Internet. This can come in the form of using a VPN to anonymize their identity and collect information more discreetly. This is where evaluating sources becomes important to the overall OSINT collection and analysis process. An OSINT analyst needs intelligence evaluation to determine a true process or expose a false process that would affect predicting the future. Finally, the analysts need to find use of the evaluated intelligence so that it can be incorporated into a finished classified, unclassified, or proprietary intelligence product.
Information collection in OSINT is generally a different problem from collection in other intelligence disciplines where obtaining the raw information to be analyzed may be the major difficulty, particularly if it is to be obtained from non-cooperative targets. In OSINT, the chief difficulty is in identifying relevant, reliable sources from the vast amount of publicly available information.[19] However, this is not as great a challenge for those who know how to access local knowledge and how to leverage human experts who can create new tailored knowledge on the fly.[citation needed]
There are several categories of tools intended for OSINT analysis. The first category includes open source tools to query multiple search engines simultaneously such as IntelTechniques or search engines that provide results separately such as All in One or DuckDuckGo. This category also includes social media search engines and search engines of domains and people such as Pipl.com, Whois.net, Website.informer. The second category is designed for big data analytics platforms such as DataWalk which combine OSINT insight with local, internal data for further visual analysis and to conduct link analysis to identify connections across a large volume of records.
A list of some tools used for collecting open source intelligence:
There are a large number of open-source activities taking place throughout the US Government. Frequently, these open-source activities are described as ""media monitoring"", ""media analysis"", ""internet research"" and ""public surveys"" but are open source nonetheless.  One of the largesrt of these is a global reverse username search, https://www.usersearch.org.
The Library of Congress sponsors the Federal Research Division (FRD) which conducts a great deal of tailored open-source research on a fee-for-service basis for the executive branch.
The US Intelligence Community's open-source activities (known as the National Open Source Enterprise) are dictated by Intelligence Community Directive 301 promulgated by the Director of National Intelligence.[20] The Directive establishes the authorities and responsibilities of the Assistant Deputy Director of National Intelligence for Open Source (ADDNI/OS), the DNI's Open Source Center and the National Open Source Committee.
Prior to the establishment of the National Open Source Enterprise, the Foreign Broadcast Information Service (FBIS), established in 1941, was the government's primary open-source unit, transcribing and translating foreign broadcasts. It absorbed the Defense Department's Joint Publications Research Service (JPRS), which did a similar function with foreign printed materials, including newspapers, magazines, and technical journals.
The former Under-Secretary of Defense for Intelligence, Dr. Stephen Cambone encouraged in part by the Defense Science Board reports on strategic communication and transition to and from hostilities, created the Defense Open Source Program (DOSP). The current under-secretary of defense for intelligence is assigned executive agency for this program to the Defense Intelligence Agency.
U.S. military offices that engage in OSINT activities include:
The Department of Homeland Security has an active open-source intelligence unit. In congressional testimony before the House Homeland Security Committee's Intelligence, Information Sharing and Terrorism Risk Assessment Subcommittee, Undersecretary of Homeland Security Charles Allen indicated on February 14, 2007, that he had established the ""Domestic Open Source Enterprise"" to support the Department's OSINT needs and that of state, local,Ο and tribal partners.
The law enforcement OSINT community applies open-source intelligence (OSINT) to the prediction, prevention, investigation, and prosecution of criminals including terrorists. Additionally, fusion centers around the US are increasingly utilizing OSINT to support their intelligence generation and investigations.
Examples of successful law enforcement OSINT include Scotland Yard OSINT; Royal Canadian Mounted Police (RCMP) OSINT.
INTERPOL and EUROPOL experimented with OSINT units for a time, but they appear to have atrophied with the departure of their individual champions.
New York Police Department (NYPD) is known to have an OSINT unit, as does the Los Angeles County Sheriff's Department, housed within the Emergency Operations Bureau and affiliated with the LA Joint Regional Intelligence Center.
Business OSINT encompasses Commercial Intelligence, Competitor Intelligence, and Business Intelligence, and is often a chief area of practice of private intelligence agencies.
Businesses may use information brokers and private investigators to collect and analyze relevant information for business purposes which may include the media, deep web, web 2.0 and commercial content.
Another related business group within the United States that relies upon OSINT is the commercial bail-bond industry. This related industry, servicing the court system, is apart from the above Business Intelligence sector.
OSINT is useful to bail-bond agencies that employ a private fugitive recovery agency to locate and apprehend their absent client; i.e., a criminal defendant who has failed to appear for court and subsequently a warrant for arrest was issued. OSINT is the first method of discovery to help locate the defendant when initial interviewing of the bond co-signers, defendant's relatives and friends is lacking. OSINT gathering leads the investigator to discover an alternate hypothesis to analyze and then match relevant data for making a prediction regarding the fugitive's location; e.g., data is scrubbed from web access on Facebook entries, Twitter messages, and Snapchat.
Should those methods fail, the next step is to seek the specialized behavioral intelligence services that reference OSINT to aid in establishing the veracity of subjects during the forensic interview and is used to create a behavioral profile. OSINT data is correlated with interview data to include a variety behavioral patterns; e.g., a list of daily personal contacts, habits of activities, visited places of interest, vehicles used, favorite group involvements, etc. According to the director, psychologist and forensic interviewer at MN-Behavioral Intelligence Agency, (2016) OSINT data base has to be critically filtered and analyzed before it can be applied within investigative interviewing and interrogation.
A main hindrance to practical OSINT is the volume of information it has to deal with (""information explosion""). The amount of data being distributed increases at a rate that it becomes difficult to evaluate sources in intelligence analysis.
Accredited journalists have some protection in asking questions, and researching for recognized media outlets. Even so, they can be imprisoned, even executed, for seeking out OSINT. Private individuals illegally collecting data for a foreign military or intelligence agency is considered espionage in most countries. Of course, espionage that is not treason (i.e. betraying one's country of citizenship) has been a tool of statecraft since ancient times.[21]
CITEREFUsername_Search | access-date=2018-05-29}} An OSINT Deep Web Search for usersnames and email addresses
"
1028006997,2021-06-11 08:58:26,"Open-source intelligence (OSINT) is a multi-factor (qualitative, quantitative) methodology for collecting, analyzing and making decisions about data accessible in publicly available sources to be used in an intelligence context. In the intelligence community, the term ""open"" refers to overt, publicly available sources (as opposed to covert or clandestine sources). OSINT under one name or another has been around for hundreds of years. With the advent of instant communications and rapid information transfer, a great deal of actionable and predictive intelligence can now be obtained from public, unclassified sources. It is not related to open-source software or collective intelligence.
OSINT is the collection and analysis of information that is gathered from public, or open, sources.[1] OSINT is primarily used in national security, law enforcement, and business intelligence functions and is of value to analysts who use non-sensitive intelligence in answering classified, unclassified, or proprietary intelligence requirements across the previous intelligence disciplines.
OSINT sources can be divided up into six different categories of information flow:[2]
OSINT is distinguished from research in that it applies the process of intelligence to create tailored knowledge supportive of a specific decision by a specific individual or group.[3]
OSINT is defined by both the U.S. Director of National Intelligence and the U.S. Department of Defense (DoD), as intelligence ""produced from publicly available information that is collected, exploited, and disseminated in a timely manner to an appropriate audience for the purpose of addressing a specific intelligence requirement.""[4]  As defined by NATO, OSINT is intelligence ""derived from publicly available information, as well as other unclassified information that has limited public distribution or access.""[5]
According to political scientist Jeffrey T. Richelson, “open source acquisition involves procuring verbal, written, or electronically transmitted material that can be obtained legally. In addition to documents and videos available via the Internet or provided by a human source, others are obtained after U.S. or allied forces have taken control of a facility or site formerly operated by a foreign government or terrorist group.”[6]
Security researcher Mark M. Lowenthal defines OSINT as “any and all information that can be derived from overt collection: all types of media, government reports and other documents, scientific research and reports, commercial vendors of information, the Internet, and so on. The main qualifiers to open-source information are that it does not require any type of clandestine collection techniques to obtain it and that it must be obtained through means that entirely meet the copyright and commercial requirements of the vendors where applicable.""[7]
OSINT in the United States traces its origins to the creation of the Foreign Broadcast Monitoring Service (FBMS), an agency responsible for the monitoring of foreign broadcasts. An example of their work is reflected in the application of the correlation of changes in the price of oranges in Paris with that of railway bridges being bombed successfully.[8]
The Aspin-Brown Commission stated in 1996 that US access to open sources was ""severely deficient"" and that this should be a ""top priority"" for both funding and DCI attention.[9]
In July 2004, following the September 11 attacks, the 9/11 Commission recommended the creation of an open-source intelligence agency.[10] In March 2005, the Iraq Intelligence Commission recommended[1] the creation of an open-source directorate at the CIA.
Following these recommendations, in November 2005 the Director of National Intelligence announced the creation of the DNI Open Source Center. The Center was established to collect information available from ""the Internet, databases, press, radio, television, video, geospatial data, photos and commercial imagery.""[11] In addition to collecting openly available information, it would train analysts to make better use of this information. The center absorbed the CIA's previously existing Foreign Broadcast Information Service (FBIS), originally established in 1941, with FBIS head Douglas Naquin named as director of the center.[12] Then, following the events of 9/11 the Intelligence Reform and Terrorism Prevention Act merged FBIS and other research elements into the Office of the Director of National Intelligence creating the Open Source Enterprise.
Furthermore, the private sector has invested in tools which aid in OSINT collection and analysis. Specifically, In-Q-Tel, a Central Intelligence Agency supported venture capital firm in Arlington, VA assisted companies develop web-monitoring and predictive analysis tools.
In December 2005, the Director of National Intelligence appointed Eliot A. Jardines as the Assistant Deputy Director of National Intelligence for Open Source to serve as the Intelligence Community's senior intelligence officer for open source and to provide strategy, guidance and oversight for the National Open Source Enterprise.[13] Mr. Jardines has established the National Open Source Enterprise[14] and authored intelligence community directive 301. In 2008, Mr. Jardines returned to the private sector and was succeeded by Dan Butler who is ADDNI/OS[15] and previously Mr. Jardines' Senior Advisor for Policy.[16]
OSINT is valuable because it has less rigorous processing and exploitation processes and timelines than more technical intelligence disciplines such as HUMINT, SIGINT, MASINT, GEOINT, etc. Additionally, OSINT collects a valuable variety of opinions because it encompasses a great variety of sources.
According to the Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction report submitted in March 2005, OSINT must be included in the all-source intelligence process for the following reasons (as stated in the report):
OSINT is a highly diverse form of intelligence collection and analysis. It does not have its own agency, however, units are scattered within the Department of Defense and the State Department.[18] Most OSINT collectors need to take precautions while collecting information from the Internet. This can come in the form of using a VPN to anonymize their identity and collect information more discreetly. This is where evaluating sources becomes important to the overall OSINT collection and analysis process. An OSINT analyst needs intelligence evaluation to determine a true process or expose a false process that would affect predicting the future. Finally, the analysts need to find use of the evaluated intelligence so that it can be incorporated into a finished classified, unclassified, or proprietary intelligence product.
Information collection in OSINT is generally a different problem from collection in other intelligence disciplines where obtaining the raw information to be analyzed may be the major difficulty, particularly if it is to be obtained from non-cooperative targets. In OSINT, the chief difficulty is in identifying relevant, reliable sources from the vast amount of publicly available information.[19] However, this is not as great a challenge for those who know how to access local knowledge and how to leverage human experts who can create new tailored knowledge on the fly.[citation needed]
There are several categories of tools intended for OSINT analysis. The first category includes open source tools to query multiple search engines simultaneously such as IntelTechniques or search engines that provide results separately such as All in One or DuckDuckGo. This category also includes social media search engines and search engines of domains and people such as Pipl.com, Whois.net, Website.informer. The second category is designed for big data analytics platforms such as DataWalk which combine OSINT insight with local, internal data for further visual analysis and to conduct link analysis to identify connections across a large volume of records.
A list of some tools used for collecting open source intelligence:
There are a large number of open-source activities taking place throughout the US Government. Frequently, these open-source activities are described as ""media monitoring"", ""media analysis"", ""internet research"" and ""public surveys"" but are open source nonetheless.
The Library of Congress sponsors the Federal Research Division (FRD) which conducts a great deal of tailored open-source research on a fee-for-service basis for the executive branch.
The US Intelligence Community's open-source activities (known as the National Open Source Enterprise) are dictated by Intelligence Community Directive 301 promulgated by the Director of National Intelligence.[20] The Directive establishes the authorities and responsibilities of the Assistant Deputy Director of National Intelligence for Open Source (ADDNI/OS), the DNI's Open Source Center and the National Open Source Committee.
Prior to the establishment of the National Open Source Enterprise, the Foreign Broadcast Information Service (FBIS), established in 1941, was the government's primary open-source unit, transcribing and translating foreign broadcasts. It absorbed the Defense Department's Joint Publications Research Service (JPRS), which did a similar function with foreign printed materials, including newspapers, magazines, and technical journals.
The former Under-Secretary of Defense for Intelligence, Dr. Stephen Cambone encouraged in part by the Defense Science Board reports on strategic communication and transition to and from hostilities, created the Defense Open Source Program (DOSP). The current under-secretary of defense for intelligence is assigned executive agency for this program to the Defense Intelligence Agency.
U.S. military offices that engage in OSINT activities include:
The Department of Homeland Security has an active open-source intelligence unit. In congressional testimony before the House Homeland Security Committee's Intelligence, Information Sharing and Terrorism Risk Assessment Subcommittee, Undersecretary of Homeland Security Charles Allen indicated on February 14, 2007, that he had established the ""Domestic Open Source Enterprise"" to support the Department's OSINT needs and that of state, local,Ο and tribal partners.
The law enforcement OSINT community applies open-source intelligence (OSINT) to the prediction, prevention, investigation, and prosecution of criminals including terrorists. Additionally, fusion centers around the US are increasingly utilizing OSINT to support their intelligence generation and investigations.
Examples of successful law enforcement OSINT include Scotland Yard OSINT; Royal Canadian Mounted Police (RCMP) OSINT.
INTERPOL and EUROPOL experimented with OSINT units for a time, but they appear to have atrophied with the departure of their individual champions.
New York Police Department (NYPD) is known to have an OSINT unit, as does the Los Angeles County Sheriff's Department, housed within the Emergency Operations Bureau and affiliated with the LA Joint Regional Intelligence Center.
Business OSINT encompasses Commercial Intelligence, Competitor Intelligence, and Business Intelligence, and is often a chief area of practice of private intelligence agencies.
Businesses may use information brokers and private investigators to collect and analyze relevant information for business purposes which may include the media, deep web, web 2.0 and commercial content.
Another related business group within the United States that relies upon OSINT is the commercial bail-bond industry. This related industry, servicing the court system, is apart from the above Business Intelligence sector.
OSINT is useful to bail-bond agencies that employ a private fugitive recovery agency to locate and apprehend their absent client; i.e., a criminal defendant who has failed to appear for court and subsequently a warrant for arrest was issued. OSINT is the first method of discovery to help locate the defendant when initial interviewing of the bond co-signers, defendant's relatives and friends is lacking. OSINT gathering leads the investigator to discover an alternate hypothesis to analyze and then match relevant data for making a prediction regarding the fugitive's location; e.g., data is scrubbed from web access on Facebook entries, Twitter messages, and Snapchat.
Should those methods fail, the next step is to seek the specialized behavioral intelligence services that reference OSINT to aid in establishing the veracity of subjects during the forensic interview and is used to create a behavioral profile. OSINT data is correlated with interview data to include a variety behavioral patterns; e.g., a list of daily personal contacts, habits of activities, visited places of interest, vehicles used, favorite group involvements, etc. According to the director, psychologist and forensic interviewer at MN-Behavioral Intelligence Agency, (2016) OSINT data base has to be critically filtered and analyzed before it can be applied within investigative interviewing and interrogation.
A main hindrance to practical OSINT is the volume of information it has to deal with (""information explosion""). The amount of data being distributed increases at a rate that it becomes difficult to evaluate sources in intelligence analysis.
Accredited journalists have some protection in asking questions, and researching for recognized media outlets. Even so, they can be imprisoned, even executed, for seeking out OSINT. Private individuals illegally collecting data for a foreign military or intelligence agency is considered espionage in most countries. Of course, espionage that is not treason (i.e. betraying one's country of citizenship) has been a tool of statecraft since ancient times.[21]
CITEREFUsername_Search | access-date=2018-05-29}} An OSINT Deep Web Search for usersnames and email addresses
"
1028311199,2021-06-13 06:28:34,"Open-source intelligence (OSINT) is a multi-factor (qualitative, quantitative) methodology for collecting, analyzing and making decisions about data accessible in publicly available sources to be used in an intelligence context. In the intelligence community, the term ""open"" refers to overt, publicly available sources (as opposed to covert or clandestine sources). OSINT under one name or another has been around for hundreds of years. With the advent of instant communications and rapid information transfer, a great deal of actionable and predictive intelligence can now be obtained from public, unclassified sources. It is not related to open-source software or collective intelligence.
OSINT is the collection and analysis of information that is gathered from public, or open, sources.[1] OSINT is primarily used in national security, law enforcement, and business intelligence functions and is of value to analysts who use non-sensitive intelligence in answering classified, unclassified, or proprietary intelligence requirements across the previous intelligence disciplines.
OSINT sources can be divided up into six different categories of information flow:[2]
OSINT is distinguished from research in that it applies the process of intelligence to create tailored knowledge supportive of a specific decision by a specific individual or group.[3]
OSINT is defined by both the U.S. Director of National Intelligence and the U.S. Department of Defense (DoD), as intelligence ""produced from publicly available information that is collected, exploited, and disseminated in a timely manner to an appropriate audience for the purpose of addressing a specific intelligence requirement.""[4]  As defined by NATO, OSINT is intelligence ""derived from publicly available information, as well as other unclassified information that has limited public distribution or access.""[5]
According to political scientist Jeffrey T. Richelson, “open source acquisition involves procuring verbal, written, or electronically transmitted material that can be obtained legally. In addition to documents and videos available via the Internet or provided by a human source, others are obtained after U.S. or allied forces have taken control of a facility or site formerly operated by a foreign government or terrorist group.”[6]
Security researcher Mark M. Lowenthal defines OSINT as “any and all information that can be derived from overt collection: all types of media, government reports and other documents, scientific research and reports, commercial vendors of information, the Internet, and so on. The main qualifiers to open-source information are that it does not require any type of clandestine collection techniques to obtain it and that it must be obtained through means that entirely meet the copyright and commercial requirements of the vendors where applicable.""[7]
OSINT in the United States traces its origins to the creation of the Foreign Broadcast Monitoring Service (FBMS), an agency responsible for the monitoring of foreign broadcasts. An example of their work is reflected in the application of the correlation of changes in the price of oranges in Paris with that of railway bridges being bombed successfully.[8]
The Aspin-Brown Commission stated in 1996 that US access to open sources was ""severely deficient"" and that this should be a ""top priority"" for both funding and DCI attention.[9]
In July 2004, following the September 11 attacks, the 9/11 Commission recommended the creation of an open-source intelligence agency.[10] In March 2005, the Iraq Intelligence Commission recommended[1] the creation of an open-source directorate at the CIA.
Following these recommendations, in November 2005 the Director of National Intelligence announced the creation of the DNI Open Source Center. The Center was established to collect information available from ""the Internet, databases, press, radio, television, video, geospatial data, photos and commercial imagery.""[11] In addition to collecting openly available information, it would train analysts to make better use of this information. The center absorbed the CIA's previously existing Foreign Broadcast Information Service (FBIS), originally established in 1941, with FBIS head Douglas Naquin named as director of the center.[12] Then, following the events of 9/11 the Intelligence Reform and Terrorism Prevention Act merged FBIS and other research elements into the Office of the Director of National Intelligence creating the Open Source Enterprise.
Furthermore, the private sector has invested in tools which aid in OSINT collection and analysis. Specifically, In-Q-Tel, a Central Intelligence Agency supported venture capital firm in Arlington, VA assisted companies develop web-monitoring and predictive analysis tools.
In December 2005, the Director of National Intelligence appointed Eliot A. Jardines as the Assistant Deputy Director of National Intelligence for Open Source to serve as the Intelligence Community's senior intelligence officer for open source and to provide strategy, guidance and oversight for the National Open Source Enterprise.[13] Mr. Jardines has established the National Open Source Enterprise[14] and authored intelligence community directive 301. In 2008, Mr. Jardines returned to the private sector and was succeeded by Dan Butler who is ADDNI/OS[15] and previously Mr. Jardines' Senior Advisor for Policy.[16]
OSINT is valuable because it has less rigorous processing and exploitation processes and timelines than more technical intelligence disciplines such as HUMINT, SIGINT, MASINT, GEOINT, etc. Additionally, OSINT collects a valuable variety of opinions because it encompasses a great variety of sources.
According to the Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction report submitted in March 2005, OSINT must be included in the all-source intelligence process for the following reasons (as stated in the report):
OSINT is a highly diverse form of intelligence collection and analysis. It does not have its own agency, however, units are scattered within the Department of Defense and the State Department.[18] Most OSINT collectors need to take precautions while collecting information from the Internet. This can come in the form of using a VPN to anonymize their identity and collect information more discreetly. This is where evaluating sources becomes important to the overall OSINT collection and analysis process. An OSINT analyst needs intelligence evaluation to determine a true process or expose a false process that would affect predicting the future. Finally, the analysts need to find use of the evaluated intelligence so that it can be incorporated into a finished classified, unclassified, or proprietary intelligence product.
Information collection in OSINT is generally a different problem from collection in other intelligence disciplines where obtaining the raw information to be analyzed may be the major difficulty, particularly if it is to be obtained from non-cooperative targets. In OSINT, the chief difficulty is in identifying relevant, reliable sources from the vast amount of publicly available information.[19] However, this is not as great a challenge for those who know how to access local knowledge and how to leverage human experts who can create new tailored knowledge on the fly.[citation needed]
There are several categories of tools intended for OSINT analysis. The first category includes open source tools to query multiple search engines simultaneously such as IntelTechniques or search engines that provide results separately such as All in One or DuckDuckGo. This category also includes social media search engines and search engines of domains and people such as Pipl.com, Whois.net, Website.informer. The second category is designed for big data analytics platforms such as DataWalk which combine OSINT insight with local, internal data for further visual analysis and to conduct link analysis to identify connections across a large volume of records.
A list of some tools used for collecting open source intelligence:
There are a large number of open-source activities taking place throughout the US Government. Frequently, these open-source activities are described as ""media monitoring"", ""media analysis"", ""internet research"" and ""public surveys"" but are open source nonetheless.
The Library of Congress sponsors the Federal Research Division (FRD) which conducts a great deal of tailored open-source research on a fee-for-service basis for the executive branch.
The US Intelligence Community's open-source activities (known as the National Open Source Enterprise) are dictated by Intelligence Community Directive 301 promulgated by the Director of National Intelligence.[20] The Directive establishes the authorities and responsibilities of the Assistant Deputy Director of National Intelligence for Open Source (ADDNI/OS), the DNI's Open Source Center and the National Open Source Committee.
Prior to the establishment of the National Open Source Enterprise, the Foreign Broadcast Information Service (FBIS), established in 1941, was the government's primary open-source unit, transcribing and translating foreign broadcasts. It absorbed the Defense Department's Joint Publications Research Service (JPRS), which did a similar function with foreign printed materials, including newspapers, magazines, and technical journals.
The former Under-Secretary of Defense for Intelligence, Dr. Stephen Cambone encouraged in part by the Defense Science Board reports on strategic communication and transition to and from hostilities, created the Defense Open Source Program (DOSP). The current under-secretary of defense for intelligence is assigned executive agency for this program to the Defense Intelligence Agency.
U.S. military offices that engage in OSINT activities include:
The Department of Homeland Security has an active open-source intelligence unit. In congressional testimony before the House Homeland Security Committee's Intelligence, Information Sharing and Terrorism Risk Assessment Subcommittee, Undersecretary of Homeland Security Charles Allen indicated on February 14, 2007, that he had established the ""Domestic Open Source Enterprise"" to support the Department's OSINT needs and that of state, local,Ο and tribal partners.
The law enforcement OSINT community applies open-source intelligence (OSINT) to the prediction, prevention, investigation, and prosecution of criminals including terrorists. Additionally, fusion centers around the US are increasingly utilizing OSINT to support their intelligence generation and investigations.
Examples of successful law enforcement OSINT include Scotland Yard OSINT; Royal Canadian Mounted Police (RCMP) OSINT.
INTERPOL and EUROPOL experimented with OSINT units for a time, but they appear to have atrophied with the departure of their individual champions.
New York Police Department (NYPD) is known to have an OSINT unit, as does the Los Angeles County Sheriff's Department, housed within the Emergency Operations Bureau and affiliated with the LA Joint Regional Intelligence Center.
Business OSINT encompasses Commercial Intelligence, Competitor Intelligence, and Business Intelligence, and is often a chief area of practice of private intelligence agencies.
Businesses may use information brokers and private investigators to collect and analyze relevant information for business purposes which may include the media, deep web, web 2.0 and commercial content.
Another related business group within the United States that relies upon OSINT is the commercial bail-bond industry. This related industry, servicing the court system, is apart from the above Business Intelligence sector.
OSINT is useful to bail-bond agencies that employ a private fugitive recovery agency to locate and apprehend their absent client; i.e., a criminal defendant who has failed to appear for court and subsequently a warrant for arrest was issued. OSINT is the first method of discovery to help locate the defendant when initial interviewing of the bond co-signers, defendant's relatives and friends is lacking. OSINT gathering leads the investigator to discover an alternate hypothesis to analyze and then match relevant data for making a prediction regarding the fugitive's location; e.g., data is scrubbed from web access on Facebook entries, Twitter messages, and Snapchat.
Should those methods fail, the next step is to seek the specialized behavioral intelligence services that reference OSINT to aid in establishing the veracity of subjects during the forensic interview and is used to create a behavioral profile. OSINT data is correlated with interview data to include a variety behavioral patterns; e.g., a list of daily personal contacts, habits of activities, visited places of interest, vehicles used, favorite group involvements, etc. According to the director, psychologist and forensic interviewer at MN-Behavioral Intelligence Agency, (2016) OSINT data base has to be critically filtered and analyzed before it can be applied within investigative interviewing and interrogation.
A main hindrance to practical OSINT is the volume of information it has to deal with (""information explosion""). The amount of data being distributed increases at a rate that it becomes difficult to evaluate sources in intelligence analysis.
Accredited journalists have some protection in asking questions, and researching for recognized media outlets. Even so, they can be imprisoned, even executed, for seeking out OSINT. Private individuals illegally collecting data for a foreign military or intelligence agency is considered espionage in most countries. Of course, espionage that is not treason (i.e. betraying one's country of citizenship) has been a tool of statecraft since ancient times.[21]
Best Website for analyze twitter trends is https://www.alldaytrends.com
CITEREFUsername_Search | access-date=2018-05-29}} An OSINT Deep Web Search for usersnames and email addresses
"
1028311245,2021-06-13 06:29:08,"Open-source intelligence (OSINT) is a multi-factor (qualitative, quantitative) methodology for collecting, analyzing and making decisions about data accessible in publicly available sources to be used in an intelligence context. In the intelligence community, the term ""open"" refers to overt, publicly available sources (as opposed to covert or clandestine sources). OSINT under one name or another has been around for hundreds of years. With the advent of instant communications and rapid information transfer, a great deal of actionable and predictive intelligence can now be obtained from public, unclassified sources. It is not related to open-source software or collective intelligence.
OSINT is the collection and analysis of information that is gathered from public, or open, sources.[1] OSINT is primarily used in national security, law enforcement, and business intelligence functions and is of value to analysts who use non-sensitive intelligence in answering classified, unclassified, or proprietary intelligence requirements across the previous intelligence disciplines.
OSINT sources can be divided up into six different categories of information flow:[2]
OSINT is distinguished from research in that it applies the process of intelligence to create tailored knowledge supportive of a specific decision by a specific individual or group.[3]
OSINT is defined by both the U.S. Director of National Intelligence and the U.S. Department of Defense (DoD), as intelligence ""produced from publicly available information that is collected, exploited, and disseminated in a timely manner to an appropriate audience for the purpose of addressing a specific intelligence requirement.""[4]  As defined by NATO, OSINT is intelligence ""derived from publicly available information, as well as other unclassified information that has limited public distribution or access.""[5]
According to political scientist Jeffrey T. Richelson, “open source acquisition involves procuring verbal, written, or electronically transmitted material that can be obtained legally. In addition to documents and videos available via the Internet or provided by a human source, others are obtained after U.S. or allied forces have taken control of a facility or site formerly operated by a foreign government or terrorist group.”[6]
Security researcher Mark M. Lowenthal defines OSINT as “any and all information that can be derived from overt collection: all types of media, government reports and other documents, scientific research and reports, commercial vendors of information, the Internet, and so on. The main qualifiers to open-source information are that it does not require any type of clandestine collection techniques to obtain it and that it must be obtained through means that entirely meet the copyright and commercial requirements of the vendors where applicable.""[7]
OSINT in the United States traces its origins to the creation of the Foreign Broadcast Monitoring Service (FBMS), an agency responsible for the monitoring of foreign broadcasts. An example of their work is reflected in the application of the correlation of changes in the price of oranges in Paris with that of railway bridges being bombed successfully.[8]
The Aspin-Brown Commission stated in 1996 that US access to open sources was ""severely deficient"" and that this should be a ""top priority"" for both funding and DCI attention.[9]
In July 2004, following the September 11 attacks, the 9/11 Commission recommended the creation of an open-source intelligence agency.[10] In March 2005, the Iraq Intelligence Commission recommended[1] the creation of an open-source directorate at the CIA.
Following these recommendations, in November 2005 the Director of National Intelligence announced the creation of the DNI Open Source Center. The Center was established to collect information available from ""the Internet, databases, press, radio, television, video, geospatial data, photos and commercial imagery.""[11] In addition to collecting openly available information, it would train analysts to make better use of this information. The center absorbed the CIA's previously existing Foreign Broadcast Information Service (FBIS), originally established in 1941, with FBIS head Douglas Naquin named as director of the center.[12] Then, following the events of 9/11 the Intelligence Reform and Terrorism Prevention Act merged FBIS and other research elements into the Office of the Director of National Intelligence creating the Open Source Enterprise.
Furthermore, the private sector has invested in tools which aid in OSINT collection and analysis. Specifically, In-Q-Tel, a Central Intelligence Agency supported venture capital firm in Arlington, VA assisted companies develop web-monitoring and predictive analysis tools.
In December 2005, the Director of National Intelligence appointed Eliot A. Jardines as the Assistant Deputy Director of National Intelligence for Open Source to serve as the Intelligence Community's senior intelligence officer for open source and to provide strategy, guidance and oversight for the National Open Source Enterprise.[13] Mr. Jardines has established the National Open Source Enterprise[14] and authored intelligence community directive 301. In 2008, Mr. Jardines returned to the private sector and was succeeded by Dan Butler who is ADDNI/OS[15] and previously Mr. Jardines' Senior Advisor for Policy.[16]
OSINT is valuable because it has less rigorous processing and exploitation processes and timelines than more technical intelligence disciplines such as HUMINT, SIGINT, MASINT, GEOINT, etc. Additionally, OSINT collects a valuable variety of opinions because it encompasses a great variety of sources.
According to the Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction report submitted in March 2005, OSINT must be included in the all-source intelligence process for the following reasons (as stated in the report):
OSINT is a highly diverse form of intelligence collection and analysis. It does not have its own agency, however, units are scattered within the Department of Defense and the State Department.[18] Most OSINT collectors need to take precautions while collecting information from the Internet. This can come in the form of using a VPN to anonymize their identity and collect information more discreetly. This is where evaluating sources becomes important to the overall OSINT collection and analysis process. An OSINT analyst needs intelligence evaluation to determine a true process or expose a false process that would affect predicting the future. Finally, the analysts need to find use of the evaluated intelligence so that it can be incorporated into a finished classified, unclassified, or proprietary intelligence product.
Information collection in OSINT is generally a different problem from collection in other intelligence disciplines where obtaining the raw information to be analyzed may be the major difficulty, particularly if it is to be obtained from non-cooperative targets. In OSINT, the chief difficulty is in identifying relevant, reliable sources from the vast amount of publicly available information.[19] However, this is not as great a challenge for those who know how to access local knowledge and how to leverage human experts who can create new tailored knowledge on the fly.[citation needed]
There are several categories of tools intended for OSINT analysis. The first category includes open source tools to query multiple search engines simultaneously such as IntelTechniques or search engines that provide results separately such as All in One or DuckDuckGo. This category also includes social media search engines and search engines of domains and people such as Pipl.com, Whois.net, Website.informer. The second category is designed for big data analytics platforms such as DataWalk which combine OSINT insight with local, internal data for further visual analysis and to conduct link analysis to identify connections across a large volume of records.
A list of some tools used for collecting open source intelligence:
There are a large number of open-source activities taking place throughout the US Government. Frequently, these open-source activities are described as ""media monitoring"", ""media analysis"", ""internet research"" and ""public surveys"" but are open source nonetheless.
The Library of Congress sponsors the Federal Research Division (FRD) which conducts a great deal of tailored open-source research on a fee-for-service basis for the executive branch.
The US Intelligence Community's open-source activities (known as the National Open Source Enterprise) are dictated by Intelligence Community Directive 301 promulgated by the Director of National Intelligence.[20] The Directive establishes the authorities and responsibilities of the Assistant Deputy Director of National Intelligence for Open Source (ADDNI/OS), the DNI's Open Source Center and the National Open Source Committee.
Prior to the establishment of the National Open Source Enterprise, the Foreign Broadcast Information Service (FBIS), established in 1941, was the government's primary open-source unit, transcribing and translating foreign broadcasts. It absorbed the Defense Department's Joint Publications Research Service (JPRS), which did a similar function with foreign printed materials, including newspapers, magazines, and technical journals.
The former Under-Secretary of Defense for Intelligence, Dr. Stephen Cambone encouraged in part by the Defense Science Board reports on strategic communication and transition to and from hostilities, created the Defense Open Source Program (DOSP). The current under-secretary of defense for intelligence is assigned executive agency for this program to the Defense Intelligence Agency.
U.S. military offices that engage in OSINT activities include:
The Department of Homeland Security has an active open-source intelligence unit. In congressional testimony before the House Homeland Security Committee's Intelligence, Information Sharing and Terrorism Risk Assessment Subcommittee, Undersecretary of Homeland Security Charles Allen indicated on February 14, 2007, that he had established the ""Domestic Open Source Enterprise"" to support the Department's OSINT needs and that of state, local,Ο and tribal partners.
The law enforcement OSINT community applies open-source intelligence (OSINT) to the prediction, prevention, investigation, and prosecution of criminals including terrorists. Additionally, fusion centers around the US are increasingly utilizing OSINT to support their intelligence generation and investigations.
Examples of successful law enforcement OSINT include Scotland Yard OSINT; Royal Canadian Mounted Police (RCMP) OSINT.
INTERPOL and EUROPOL experimented with OSINT units for a time, but they appear to have atrophied with the departure of their individual champions.
New York Police Department (NYPD) is known to have an OSINT unit, as does the Los Angeles County Sheriff's Department, housed within the Emergency Operations Bureau and affiliated with the LA Joint Regional Intelligence Center.
Business OSINT encompasses Commercial Intelligence, Competitor Intelligence, and Business Intelligence, and is often a chief area of practice of private intelligence agencies.
Businesses may use information brokers and private investigators to collect and analyze relevant information for business purposes which may include the media, deep web, web 2.0 and commercial content.
Another related business group within the United States that relies upon OSINT is the commercial bail-bond industry. This related industry, servicing the court system, is apart from the above Business Intelligence sector.
OSINT is useful to bail-bond agencies that employ a private fugitive recovery agency to locate and apprehend their absent client; i.e., a criminal defendant who has failed to appear for court and subsequently a warrant for arrest was issued. OSINT is the first method of discovery to help locate the defendant when initial interviewing of the bond co-signers, defendant's relatives and friends is lacking. OSINT gathering leads the investigator to discover an alternate hypothesis to analyze and then match relevant data for making a prediction regarding the fugitive's location; e.g., data is scrubbed from web access on Facebook entries, Twitter messages, and Snapchat.
Should those methods fail, the next step is to seek the specialized behavioral intelligence services that reference OSINT to aid in establishing the veracity of subjects during the forensic interview and is used to create a behavioral profile. OSINT data is correlated with interview data to include a variety behavioral patterns; e.g., a list of daily personal contacts, habits of activities, visited places of interest, vehicles used, favorite group involvements, etc. According to the director, psychologist and forensic interviewer at MN-Behavioral Intelligence Agency, (2016) OSINT data base has to be critically filtered and analyzed before it can be applied within investigative interviewing and interrogation.
A main hindrance to practical OSINT is the volume of information it has to deal with (""information explosion""). The amount of data being distributed increases at a rate that it becomes difficult to evaluate sources in intelligence analysis.
Accredited journalists have some protection in asking questions, and researching for recognized media outlets. Even so, they can be imprisoned, even executed, for seeking out OSINT. Private individuals illegally collecting data for a foreign military or intelligence agency is considered espionage in most countries. Of course, espionage that is not treason (i.e. betraying one's country of citizenship) has been a tool of statecraft since ancient times.[21]
CITEREFUsername_Search | access-date=2018-05-29}} An OSINT Deep Web Search for usersnames and email addresses
"
1030841680,2021-06-28 09:34:52,"Open-source intelligence (OSINT) is a multi-factor (qualitative, quantitative) methodology for collecting, analyzing and making decisions about data accessible in publicly available sources to be used in an intelligence context. In the intelligence community, the term ""open"" refers to overt, publicly available sources (as opposed to covert or clandestine sources). OSINT under one name or another has been around for hundreds of years. With the advent of instant communications and rapid information transfer, a great deal of actionable and predictive intelligence can now be obtained from public, unclassified sources. It is not related to open-source software or collective intelligence.
OSINT is the collection and analysis of information that is gathered from public, or open, sources.[1] OSINT is primarily used in national security, law enforcement, and business intelligence functions and is of value to analysts who use non-sensitive intelligence in answering classified, unclassified, or proprietary intelligence requirements across the previous intelligence disciplines.
OSINT sources can be divided up into six different categories of information flow:[2]
OSINT is distinguished from research in that it applies the process of intelligence to create tailored knowledge supportive of a specific decision by a specific individual or group.[3]
OSINT is defined by both the U.S. Director of National Intelligence and the U.S. Department of Defense (DoD), as intelligence ""produced from publicly available information that is collected, exploited, and disseminated in a timely manner to an appropriate audience for the purpose of addressing a specific intelligence requirement.""[4]  As defined by NATO, OSINT is intelligence ""derived from publicly available information, as well as other unclassified information that has limited public distribution or access.""[5]
According to political scientist Jeffrey T. Richelson, “open source acquisition involves procuring verbal, written, or electronically transmitted material that can be obtained legally. In addition to documents and videos available via the Internet or provided by a human source, others are obtained after U.S. or allied forces have taken control of a facility or site formerly operated by a foreign government or terrorist group.”[6]
Security researcher Mark M. Lowenthal defines OSINT as “any and all information that can be derived from overt collection: all types of media, government reports and other documents, scientific research and reports, commercial vendors of information, the Internet, and so on. The main qualifiers to open-source information are that it does not require any type of clandestine collection techniques to obtain it and that it must be obtained through means that entirely meet the copyright and commercial requirements of the vendors where applicable.""[7]
OSINT in the United States traces its origins to the creation of the Foreign Broadcast Monitoring Service (FBMS), an agency responsible for the monitoring of foreign broadcasts. An example of their work is reflected in the application of the correlation of changes in the price of oranges in Paris with that of railway bridges being bombed successfully.[8]
The Aspin-Brown Commission stated in 1996 that US access to open sources was ""severely deficient"" and that this should be a ""top priority"" for both funding and DCI attention.[9]
In July 2004, following the September 11 attacks, the 9/11 Commission recommended the creation of an open-source intelligence agency.[10] In March 2005, the Iraq Intelligence Commission recommended[1] the creation of an open-source directorate at the CIA.
Following these recommendations, in November 2005 the Director of National Intelligence announced the creation of the DNI Open Source Center. The Center was established to collect information available from ""the Internet, databases, press, radio, television, video, geospatial data, photos and commercial imagery.""[11] In addition to collecting openly available information, it would train analysts to make better use of this information. The center absorbed the CIA's previously existing Foreign Broadcast Information Service (FBIS), originally established in 1941, with FBIS head Douglas Naquin named as director of the center.[12] Then, following the events of 9/11 the Intelligence Reform and Terrorism Prevention Act merged FBIS and other research elements into the Office of the Director of National Intelligence creating the Open Source Enterprise.
Furthermore, the private sector has invested in tools which aid in OSINT collection and analysis. Specifically, In-Q-Tel, a Central Intelligence Agency supported venture capital firm in Arlington, VA assisted companies develop web-monitoring and predictive analysis tools.
In December 2005, the Director of National Intelligence appointed Eliot A. Jardines as the Assistant Deputy Director of National Intelligence for Open Source to serve as the Intelligence Community's senior intelligence officer for open source and to provide strategy, guidance and oversight for the National Open Source Enterprise.[13] Mr. Jardines has established the National Open Source Enterprise[14] and authored intelligence community directive 301. In 2008, Mr. Jardines returned to the private sector and was succeeded by Dan Butler who is ADDNI/OS[15] and previously Mr. Jardines' Senior Advisor for Policy.[16]
OSINT is valuable because it has less rigorous processing and exploitation processes and timelines than more technical intelligence disciplines such as HUMINT, SIGINT, MASINT, GEOINT, etc. Additionally, OSINT collects a valuable variety of opinions because it encompasses a great variety of sources.
According to the Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction report submitted in March 2005, OSINT must be included in the all-source intelligence process for the following reasons (as stated in the report):
OSINT is a highly diverse form of intelligence collection and analysis. It does not have its own agency, however, units are scattered within the Department of Defense and the State Department.[18] Most OSINT collectors need to take precautions while collecting information from the Internet. This can come in the form of using a VPN to anonymize their identity and collect information more discreetly. This is where evaluating sources becomes important to the overall OSINT collection and analysis process. An OSINT analyst needs intelligence evaluation to determine a true process or expose a false process that would affect predicting the future. Finally, the analysts need to find use of the evaluated intelligence so that it can be incorporated into a finished classified, unclassified, or proprietary intelligence product.
Information collection in OSINT is generally a different problem from collection in other intelligence disciplines where obtaining the raw information to be analyzed may be the major difficulty, particularly if it is to be obtained from non-cooperative targets. In OSINT, the chief difficulty is in identifying relevant, reliable sources from the vast amount of publicly available information.[19] However, this is not as great a challenge for those who know how to access local knowledge and how to leverage human experts who can create new tailored knowledge on the fly.[citation needed]
There are several categories of tools intended for OSINT analysis. The first category includes open source tools to query multiple search engines simultaneously such as IntelTechniques or search engines that provide results separately such as All in One or DuckDuckGo. This category also includes social media search engines and search engines of domains and people such as Pipl.com, Whois.net, Website.informer. The second category is designed for big data analytics platforms such as DataWalk which combine OSINT insight with local, internal data for further visual analysis and to conduct link analysis to identify connections across a large volume of records.
A list of some tools used for collecting open source intelligence:
There are a large number of open-source activities taking place throughout the US Government. Frequently, these open-source activities are described as ""media monitoring"", ""media analysis"", ""internet research"" and ""public surveys"" but are open source nonetheless.
The Library of Congress sponsors the Federal Research Division (FRD) which conducts a great deal of tailored open-source research on a fee-for-service basis for the executive branch.
The US Intelligence Community's open-source activities (known as the National Open Source Enterprise) are dictated by Intelligence Community Directive 301 promulgated by the Director of National Intelligence.[20] The Directive establishes the authorities and responsibilities of the Assistant Deputy Director of National Intelligence for Open Source (ADDNI/OS), the DNI's Open Source Center and the National Open Source Committee.
Prior to the establishment of the National Open Source Enterprise, the Foreign Broadcast Information Service (FBIS), established in 1941, was the government's primary open-source unit, transcribing and translating foreign broadcasts. It absorbed the Defense Department's Joint Publications Research Service (JPRS), which did a similar function with foreign printed materials, including newspapers, magazines, and technical journals.
The former Under-Secretary of Defense for Intelligence, Dr. Stephen Cambone encouraged in part by the Defense Science Board reports on strategic communication and transition to and from hostilities, created the Defense Open Source Program (DOSP). The current under-secretary of defense for intelligence is assigned executive agency for this program to the Defense Intelligence Agency.
U.S. military offices that engage in OSINT activities include:
The Department of Homeland Security has an active open-source intelligence unit. In congressional testimony before the House Homeland Security Committee's Intelligence, Information Sharing and Terrorism Risk Assessment Subcommittee, Undersecretary of Homeland Security Charles Allen indicated on February 14, 2007, that he had established the ""Domestic Open Source Enterprise"" to support the Department's OSINT needs and that of state, local,Ο and tribal partners.
The law enforcement OSINT community applies open-source intelligence (OSINT) to the prediction, prevention, investigation, and prosecution of criminals including terrorists. Additionally, fusion centers around the US are increasingly utilizing OSINT to support their intelligence generation and investigations.
Examples of successful law enforcement OSINT include Scotland Yard OSINT; Royal Canadian Mounted Police (RCMP) OSINT.
INTERPOL and EUROPOL experimented with OSINT units for a time, but they appear to have atrophied with the departure of their individual champions.
New York Police Department (NYPD) is known to have an OSINT unit, as does the Los Angeles County Sheriff's Department, housed within the Emergency Operations Bureau and affiliated with the LA Joint Regional Intelligence Center.
Business OSINT encompasses Commercial Intelligence, Competitor Intelligence, and Business Intelligence, and is often a chief area of practice of private intelligence agencies.
Businesses may use information brokers and private investigators to collect and analyze relevant information for business purposes which may include the media, deep web, web 2.0 and commercial content.
Another related business group within the United States that relies upon OSINT is the commercial bail-bond industry. This related industry, servicing the court system, is apart from the above Business Intelligence sector.
OSINT is useful to bail-bond agencies that employ a private fugitive recovery agency to locate and apprehend their absent client; i.e., a criminal defendant who has failed to appear for court and subsequently a warrant for arrest was issued. OSINT is the first method of discovery to help locate the defendant when initial interviewing of the bond co-signers, defendant's relatives and friends is lacking. OSINT gathering leads the investigator to discover an alternate hypothesis to analyze and then match relevant data for making a prediction regarding the fugitive's location; e.g., data is scrubbed from web access on Facebook entries, Twitter messages, and Snapchat.
Should those methods fail, the next step is to seek the specialized behavioral intelligence services that reference OSINT to aid in establishing the veracity of subjects during the forensic interview and is used to create a behavioral profile. OSINT data is correlated with interview data to include a variety behavioral patterns; e.g., a list of daily personal contacts, habits of activities, visited places of interest, vehicles used, favorite group involvements, etc. According to the director, psychologist and forensic interviewer at MN-Behavioral Intelligence Agency, (2016) OSINT data base has to be critically filtered and analyzed before it can be applied within investigative interviewing and interrogation.
A main hindrance to practical OSINT is the volume of information it has to deal with (""information explosion""). The amount of data being distributed increases at a rate that it becomes difficult to evaluate sources in intelligence analysis.
Accredited journalists have some protection in asking questions, and researching for recognized media outlets. Even so, they can be imprisoned, even executed, for seeking out OSINT. Private individuals illegally collecting data for a foreign military or intelligence agency is considered espionage in most countries. Of course, espionage that is not treason (i.e. betraying one's country of citizenship) has been a tool of statecraft since ancient times.[21]
CITEREFUsername_Search | access-date=2018-05-29}} An OSINT Deep Web Search for usersnames and email addresses
"
1031313151,2021-06-30 23:36:26,"Open-source intelligence (OSINT) is a multi-factor (qualitative, quantitative) methodology for collecting, analyzing and making decisions about data accessible in publicly available sources to be used in an intelligence context. In the intelligence community, the term ""open"" refers to overt, publicly available sources (as opposed to covert or clandestine sources). OSINT under one name or another has been around for hundreds of years. With the advent of instant communications and rapid information transfer, a great deal of actionable and predictive intelligence can now be obtained from public, unclassified sources. It is not related to open-source software or collective intelligence.
OSINT is the collection and analysis of information that is gathered from public, or open, sources.[1] OSINT is primarily used in national security, law enforcement, and business intelligence functions and is of value to analysts who use non-sensitive intelligence in answering classified, unclassified, or proprietary intelligence requirements across the previous intelligence disciplines.
OSINT sources can be divided up into six different categories of information flow:[2]
OSINT is distinguished from research in that it applies the process of intelligence to create tailored knowledge supportive of a specific decision by a specific individual or group.[3]
SBCTF{dont_7rust_3very7hing_u_r3ad_h3r3}
OSINT is defined by both the U.S. Director of National Intelligence and the U.S. Department of Defense (DoD), as intelligence ""produced from publicly available information that is collected, exploited, and disseminated in a timely manner to an appropriate audience for the purpose of addressing a specific intelligence requirement.""[4]  As defined by NATO, OSINT is intelligence ""derived from publicly available information, as well as other unclassified information that has limited public distribution or access.""[5]
According to political scientist Jeffrey T. Richelson, “open source acquisition involves procuring verbal, written, or electronically transmitted material that can be obtained legally. In addition to documents and videos available via the Internet or provided by a human source, others are obtained after U.S. or allied forces have taken control of a facility or site formerly operated by a foreign government or terrorist group.”[6]
Security researcher Mark M. Lowenthal defines OSINT as “any and all information that can be derived from overt collection: all types of media, government reports and other documents, scientific research and reports, commercial vendors of information, the Internet, and so on. The main qualifiers to open-source information are that it does not require any type of clandestine collection techniques to obtain it and that it must be obtained through means that entirely meet the copyright and commercial requirements of the vendors where applicable.""[7]
OSINT in the United States traces its origins to the creation of the Foreign Broadcast Monitoring Service (FBMS), an agency responsible for the monitoring of foreign broadcasts. An example of their work is reflected in the application of the correlation of changes in the price of oranges in Paris with that of railway bridges being bombed successfully.[8]
The Aspin-Brown Commission stated in 1996 that US access to open sources was ""severely deficient"" and that this should be a ""top priority"" for both funding and DCI attention.[9]
In July 2004, following the September 11 attacks, the 9/11 Commission recommended the creation of an open-source intelligence agency.[10] In March 2005, the Iraq Intelligence Commission recommended[1] the creation of an open-source directorate at the CIA.
Following these recommendations, in November 2005 the Director of National Intelligence announced the creation of the DNI Open Source Center. The Center was established to collect information available from ""the Internet, databases, press, radio, television, video, geospatial data, photos and commercial imagery.""[11] In addition to collecting openly available information, it would train analysts to make better use of this information. The center absorbed the CIA's previously existing Foreign Broadcast Information Service (FBIS), originally established in 1941, with FBIS head Douglas Naquin named as director of the center.[12] Then, following the events of 9/11 the Intelligence Reform and Terrorism Prevention Act merged FBIS and other research elements into the Office of the Director of National Intelligence creating the Open Source Enterprise.
Furthermore, the private sector has invested in tools which aid in OSINT collection and analysis. Specifically, In-Q-Tel, a Central Intelligence Agency supported venture capital firm in Arlington, VA assisted companies develop web-monitoring and predictive analysis tools.
In December 2005, the Director of National Intelligence appointed Eliot A. Jardines as the Assistant Deputy Director of National Intelligence for Open Source to serve as the Intelligence Community's senior intelligence officer for open source and to provide strategy, guidance and oversight for the National Open Source Enterprise.[13] Mr. Jardines has established the National Open Source Enterprise[14] and authored intelligence community directive 301. In 2008, Mr. Jardines returned to the private sector and was succeeded by Dan Butler who is ADDNI/OS[15] and previously Mr. Jardines' Senior Advisor for Policy.[16]
OSINT is valuable because it has less rigorous processing and exploitation processes and timelines than more technical intelligence disciplines such as HUMINT, SIGINT, MASINT, GEOINT, etc. Additionally, OSINT collects a valuable variety of opinions because it encompasses a great variety of sources.
According to the Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction report submitted in March 2005, OSINT must be included in the all-source intelligence process for the following reasons (as stated in the report):
OSINT is a highly diverse form of intelligence collection and analysis. It does not have its own agency, however, units are scattered within the Department of Defense and the State Department.[18] Most OSINT collectors need to take precautions while collecting information from the Internet. This can come in the form of using a VPN to anonymize their identity and collect information more discreetly. This is where evaluating sources becomes important to the overall OSINT collection and analysis process. An OSINT analyst needs intelligence evaluation to determine a true process or expose a false process that would affect predicting the future. Finally, the analysts need to find use of the evaluated intelligence so that it can be incorporated into a finished classified, unclassified, or proprietary intelligence product.
Information collection in OSINT is generally a different problem from collection in other intelligence disciplines where obtaining the raw information to be analyzed may be the major difficulty, particularly if it is to be obtained from non-cooperative targets. In OSINT, the chief difficulty is in identifying relevant, reliable sources from the vast amount of publicly available information.[19] However, this is not as great a challenge for those who know how to access local knowledge and how to leverage human experts who can create new tailored knowledge on the fly.[citation needed]
There are several categories of tools intended for OSINT analysis. The first category includes open source tools to query multiple search engines simultaneously such as IntelTechniques or search engines that provide results separately such as All in One or DuckDuckGo. This category also includes social media search engines and search engines of domains and people such as Pipl.com, Whois.net, Website.informer. The second category is designed for big data analytics platforms such as DataWalk which combine OSINT insight with local, internal data for further visual analysis and to conduct link analysis to identify connections across a large volume of records.
A list of some tools used for collecting open source intelligence:
There are a large number of open-source activities taking place throughout the US Government. Frequently, these open-source activities are described as ""media monitoring"", ""media analysis"", ""internet research"" and ""public surveys"" but are open source nonetheless.
The Library of Congress sponsors the Federal Research Division (FRD) which conducts a great deal of tailored open-source research on a fee-for-service basis for the executive branch.
The US Intelligence Community's open-source activities (known as the National Open Source Enterprise) are dictated by Intelligence Community Directive 301 promulgated by the Director of National Intelligence.[20] The Directive establishes the authorities and responsibilities of the Assistant Deputy Director of National Intelligence for Open Source (ADDNI/OS), the DNI's Open Source Center and the National Open Source Committee.
Prior to the establishment of the National Open Source Enterprise, the Foreign Broadcast Information Service (FBIS), established in 1941, was the government's primary open-source unit, transcribing and translating foreign broadcasts. It absorbed the Defense Department's Joint Publications Research Service (JPRS), which did a similar function with foreign printed materials, including newspapers, magazines, and technical journals.
The former Under-Secretary of Defense for Intelligence, Dr. Stephen Cambone encouraged in part by the Defense Science Board reports on strategic communication and transition to and from hostilities, created the Defense Open Source Program (DOSP). The current under-secretary of defense for intelligence is assigned executive agency for this program to the Defense Intelligence Agency.
U.S. military offices that engage in OSINT activities include:
The Department of Homeland Security has an active open-source intelligence unit. In congressional testimony before the House Homeland Security Committee's Intelligence, Information Sharing and Terrorism Risk Assessment Subcommittee, Undersecretary of Homeland Security Charles Allen indicated on February 14, 2007, that he had established the ""Domestic Open Source Enterprise"" to support the Department's OSINT needs and that of state, local,Ο and tribal partners.
The law enforcement OSINT community applies open-source intelligence (OSINT) to the prediction, prevention, investigation, and prosecution of criminals including terrorists. Additionally, fusion centers around the US are increasingly utilizing OSINT to support their intelligence generation and investigations.
Examples of successful law enforcement OSINT include Scotland Yard OSINT; Royal Canadian Mounted Police (RCMP) OSINT.
INTERPOL and EUROPOL experimented with OSINT units for a time, but they appear to have atrophied with the departure of their individual champions.
New York Police Department (NYPD) is known to have an OSINT unit, as does the Los Angeles County Sheriff's Department, housed within the Emergency Operations Bureau and affiliated with the LA Joint Regional Intelligence Center.
Business OSINT encompasses Commercial Intelligence, Competitor Intelligence, and Business Intelligence, and is often a chief area of practice of private intelligence agencies.
Businesses may use information brokers and private investigators to collect and analyze relevant information for business purposes which may include the media, deep web, web 2.0 and commercial content.
Another related business group within the United States that relies upon OSINT is the commercial bail-bond industry. This related industry, servicing the court system, is apart from the above Business Intelligence sector.
OSINT is useful to bail-bond agencies that employ a private fugitive recovery agency to locate and apprehend their absent client; i.e., a criminal defendant who has failed to appear for court and subsequently a warrant for arrest was issued. OSINT is the first method of discovery to help locate the defendant when initial interviewing of the bond co-signers, defendant's relatives and friends is lacking. OSINT gathering leads the investigator to discover an alternate hypothesis to analyze and then match relevant data for making a prediction regarding the fugitive's location; e.g., data is scrubbed from web access on Facebook entries, Twitter messages, and Snapchat.
Should those methods fail, the next step is to seek the specialized behavioral intelligence services that reference OSINT to aid in establishing the veracity of subjects during the forensic interview and is used to create a behavioral profile. OSINT data is correlated with interview data to include a variety behavioral patterns; e.g., a list of daily personal contacts, habits of activities, visited places of interest, vehicles used, favorite group involvements, etc. According to the director, psychologist and forensic interviewer at MN-Behavioral Intelligence Agency, (2016) OSINT data base has to be critically filtered and analyzed before it can be applied within investigative interviewing and interrogation.
A main hindrance to practical OSINT is the volume of information it has to deal with (""information explosion""). The amount of data being distributed increases at a rate that it becomes difficult to evaluate sources in intelligence analysis.
Accredited journalists have some protection in asking questions, and researching for recognized media outlets. Even so, they can be imprisoned, even executed, for seeking out OSINT. Private individuals illegally collecting data for a foreign military or intelligence agency is considered espionage in most countries. Of course, espionage that is not treason (i.e. betraying one's country of citizenship) has been a tool of statecraft since ancient times.[21]
CITEREFUsername_Search | access-date=2018-05-29}} An OSINT Deep Web Search for usersnames and email addresses
"
1031313155,2021-06-30 23:36:31,"Open-source intelligence (OSINT) is a multi-factor (qualitative, quantitative) methodology for collecting, analyzing and making decisions about data accessible in publicly available sources to be used in an intelligence context. In the intelligence community, the term ""open"" refers to overt, publicly available sources (as opposed to covert or clandestine sources). OSINT under one name or another has been around for hundreds of years. With the advent of instant communications and rapid information transfer, a great deal of actionable and predictive intelligence can now be obtained from public, unclassified sources. It is not related to open-source software or collective intelligence.
OSINT is the collection and analysis of information that is gathered from public, or open, sources.[1] OSINT is primarily used in national security, law enforcement, and business intelligence functions and is of value to analysts who use non-sensitive intelligence in answering classified, unclassified, or proprietary intelligence requirements across the previous intelligence disciplines.
OSINT sources can be divided up into six different categories of information flow:[2]
OSINT is distinguished from research in that it applies the process of intelligence to create tailored knowledge supportive of a specific decision by a specific individual or group.[3]
OSINT is defined by both the U.S. Director of National Intelligence and the U.S. Department of Defense (DoD), as intelligence ""produced from publicly available information that is collected, exploited, and disseminated in a timely manner to an appropriate audience for the purpose of addressing a specific intelligence requirement.""[4]  As defined by NATO, OSINT is intelligence ""derived from publicly available information, as well as other unclassified information that has limited public distribution or access.""[5]
According to political scientist Jeffrey T. Richelson, “open source acquisition involves procuring verbal, written, or electronically transmitted material that can be obtained legally. In addition to documents and videos available via the Internet or provided by a human source, others are obtained after U.S. or allied forces have taken control of a facility or site formerly operated by a foreign government or terrorist group.”[6]
Security researcher Mark M. Lowenthal defines OSINT as “any and all information that can be derived from overt collection: all types of media, government reports and other documents, scientific research and reports, commercial vendors of information, the Internet, and so on. The main qualifiers to open-source information are that it does not require any type of clandestine collection techniques to obtain it and that it must be obtained through means that entirely meet the copyright and commercial requirements of the vendors where applicable.""[7]
OSINT in the United States traces its origins to the creation of the Foreign Broadcast Monitoring Service (FBMS), an agency responsible for the monitoring of foreign broadcasts. An example of their work is reflected in the application of the correlation of changes in the price of oranges in Paris with that of railway bridges being bombed successfully.[8]
The Aspin-Brown Commission stated in 1996 that US access to open sources was ""severely deficient"" and that this should be a ""top priority"" for both funding and DCI attention.[9]
In July 2004, following the September 11 attacks, the 9/11 Commission recommended the creation of an open-source intelligence agency.[10] In March 2005, the Iraq Intelligence Commission recommended[1] the creation of an open-source directorate at the CIA.
Following these recommendations, in November 2005 the Director of National Intelligence announced the creation of the DNI Open Source Center. The Center was established to collect information available from ""the Internet, databases, press, radio, television, video, geospatial data, photos and commercial imagery.""[11] In addition to collecting openly available information, it would train analysts to make better use of this information. The center absorbed the CIA's previously existing Foreign Broadcast Information Service (FBIS), originally established in 1941, with FBIS head Douglas Naquin named as director of the center.[12] Then, following the events of 9/11 the Intelligence Reform and Terrorism Prevention Act merged FBIS and other research elements into the Office of the Director of National Intelligence creating the Open Source Enterprise.
Furthermore, the private sector has invested in tools which aid in OSINT collection and analysis. Specifically, In-Q-Tel, a Central Intelligence Agency supported venture capital firm in Arlington, VA assisted companies develop web-monitoring and predictive analysis tools.
In December 2005, the Director of National Intelligence appointed Eliot A. Jardines as the Assistant Deputy Director of National Intelligence for Open Source to serve as the Intelligence Community's senior intelligence officer for open source and to provide strategy, guidance and oversight for the National Open Source Enterprise.[13] Mr. Jardines has established the National Open Source Enterprise[14] and authored intelligence community directive 301. In 2008, Mr. Jardines returned to the private sector and was succeeded by Dan Butler who is ADDNI/OS[15] and previously Mr. Jardines' Senior Advisor for Policy.[16]
OSINT is valuable because it has less rigorous processing and exploitation processes and timelines than more technical intelligence disciplines such as HUMINT, SIGINT, MASINT, GEOINT, etc. Additionally, OSINT collects a valuable variety of opinions because it encompasses a great variety of sources.
According to the Commission on the Intelligence Capabilities of the United States Regarding Weapons of Mass Destruction report submitted in March 2005, OSINT must be included in the all-source intelligence process for the following reasons (as stated in the report):
OSINT is a highly diverse form of intelligence collection and analysis. It does not have its own agency, however, units are scattered within the Department of Defense and the State Department.[18] Most OSINT collectors need to take precautions while collecting information from the Internet. This can come in the form of using a VPN to anonymize their identity and collect information more discreetly. This is where evaluating sources becomes important to the overall OSINT collection and analysis process. An OSINT analyst needs intelligence evaluation to determine a true process or expose a false process that would affect predicting the future. Finally, the analysts need to find use of the evaluated intelligence so that it can be incorporated into a finished classified, unclassified, or proprietary intelligence product.
Information collection in OSINT is generally a different problem from collection in other intelligence disciplines where obtaining the raw information to be analyzed may be the major difficulty, particularly if it is to be obtained from non-cooperative targets. In OSINT, the chief difficulty is in identifying relevant, reliable sources from the vast amount of publicly available information.[19] However, this is not as great a challenge for those who know how to access local knowledge and how to leverage human experts who can create new tailored knowledge on the fly.[citation needed]
There are several categories of tools intended for OSINT analysis. The first category includes open source tools to query multiple search engines simultaneously such as IntelTechniques or search engines that provide results separately such as All in One or DuckDuckGo. This category also includes social media search engines and search engines of domains and people such as Pipl.com, Whois.net, Website.informer. The second category is designed for big data analytics platforms such as DataWalk which combine OSINT insight with local, internal data for further visual analysis and to conduct link analysis to identify connections across a large volume of records.
A list of some tools used for collecting open source intelligence:
There are a large number of open-source activities taking place throughout the US Government. Frequently, these open-source activities are described as ""media monitoring"", ""media analysis"", ""internet research"" and ""public surveys"" but are open source nonetheless.
The Library of Congress sponsors the Federal Research Division (FRD) which conducts a great deal of tailored open-source research on a fee-for-service basis for the executive branch.
The US Intelligence Community's open-source activities (known as the National Open Source Enterprise) are dictated by Intelligence Community Directive 301 promulgated by the Director of National Intelligence.[20] The Directive establishes the authorities and responsibilities of the Assistant Deputy Director of National Intelligence for Open Source (ADDNI/OS), the DNI's Open Source Center and the National Open Source Committee.
Prior to the establishment of the National Open Source Enterprise, the Foreign Broadcast Information Service (FBIS), established in 1941, was the government's primary open-source unit, transcribing and translating foreign broadcasts. It absorbed the Defense Department's Joint Publications Research Service (JPRS), which did a similar function with foreign printed materials, including newspapers, magazines, and technical journals.
The former Under-Secretary of Defense for Intelligence, Dr. Stephen Cambone encouraged in part by the Defense Science Board reports on strategic communication and transition to and from hostilities, created the Defense Open Source Program (DOSP). The current under-secretary of defense for intelligence is assigned executive agency for this program to the Defense Intelligence Agency.
U.S. military offices that engage in OSINT activities include:
The Department of Homeland Security has an active open-source intelligence unit. In congressional testimony before the House Homeland Security Committee's Intelligence, Information Sharing and Terrorism Risk Assessment Subcommittee, Undersecretary of Homeland Security Charles Allen indicated on February 14, 2007, that he had established the ""Domestic Open Source Enterprise"" to support the Department's OSINT needs and that of state, local,Ο and tribal partners.
The law enforcement OSINT community applies open-source intelligence (OSINT) to the prediction, prevention, investigation, and prosecution of criminals including terrorists. Additionally, fusion centers around the US are increasingly utilizing OSINT to support their intelligence generation and investigations.
Examples of successful law enforcement OSINT include Scotland Yard OSINT; Royal Canadian Mounted Police (RCMP) OSINT.
INTERPOL and EUROPOL experimented with OSINT units for a time, but they appear to have atrophied with the departure of their individual champions.
New York Police Department (NYPD) is known to have an OSINT unit, as does the Los Angeles County Sheriff's Department, housed within the Emergency Operations Bureau and affiliated with the LA Joint Regional Intelligence Center.
Business OSINT encompasses Commercial Intelligence, Competitor Intelligence, and Business Intelligence, and is often a chief area of practice of private intelligence agencies.
Businesses may use information brokers and private investigators to collect and analyze relevant information for business purposes which may include the media, deep web, web 2.0 and commercial content.
Another related business group within the United States that relies upon OSINT is the commercial bail-bond industry. This related industry, servicing the court system, is apart from the above Business Intelligence sector.
OSINT is useful to bail-bond agencies that employ a private fugitive recovery agency to locate and apprehend their absent client; i.e., a criminal defendant who has failed to appear for court and subsequently a warrant for arrest was issued. OSINT is the first method of discovery to help locate the defendant when initial interviewing of the bond co-signers, defendant's relatives and friends is lacking. OSINT gathering leads the investigator to discover an alternate hypothesis to analyze and then match relevant data for making a prediction regarding the fugitive's location; e.g., data is scrubbed from web access on Facebook entries, Twitter messages, and Snapchat.
Should those methods fail, the next step is to seek the specialized behavioral intelligence services that reference OSINT to aid in establishing the veracity of subjects during the forensic interview and is used to create a behavioral profile. OSINT data is correlated with interview data to include a variety behavioral patterns; e.g., a list of daily personal contacts, habits of activities, visited places of interest, vehicles used, favorite group involvements, etc. According to the director, psychologist and forensic interviewer at MN-Behavioral Intelligence Agency, (2016) OSINT data base has to be critically filtered and analyzed before it can be applied within investigative interviewing and interrogation.
A main hindrance to practical OSINT is the volume of information it has to deal with (""information explosion""). The amount of data being distributed increases at a rate that it becomes difficult to evaluate sources in intelligence analysis.
Accredited journalists have some protection in asking questions, and researching for recognized media outlets. Even so, they can be imprisoned, even executed, for seeking out OSINT. Private individuals illegally collecting data for a foreign military or intelligence agency is considered espionage in most countries. Of course, espionage that is not treason (i.e. betraying one's country of citizenship) has been a tool of statecraft since ancient times.[21]
CITEREFUsername_Search | access-date=2018-05-29}} An OSINT Deep Web Search for usersnames and email addresses
"
414518744,2011-02-17 22:45:02,"Product intelligence is defined as an automated system for gathering and analyzing intelligence about the performance of a product being designed and manufactured, such that this data is automatically fed back to the product managers and engineers designing the product, to assist them in the development of the next iteration or version of that product. The goal of product intelligence is to accelerate the rate of product innovation, thereby making the product and its owners more competitive. Product intelligence is usually applied to electronic products, but it is not necessarily limited to electronic products. 
Key points of this definition:
Product intelligence can also include two additional functions:
In 2002, Averna Technologies Inc. began using the term product intelligence to describe the ground-breaking functionality of its Proligent™ software, which at that time, was an add-on software product to National Instruments NI TestStand test management software, which is designed to help test engineers develop automated test and validation systems faster.
The Proligent product name was derived by combining the words product and intelligence. In 2006, Averna made available a developer’s toolkit so that test engineers could integrate the Proligent software directly to any test station using Proligent’s API, thereby broadening the Proligent customer base.
In 2009, Averna expanded the functionality of its Proligent product to include automated versioning and configuration management as well as quality enforcement capabilities. In 2011, Averna decoupled the business intelligence portion of Proligent, so that its highly specialized business intelligence solution could be integrated directly to any test database used by electronics manufacturers.
"
415260959,2011-02-22 03:56:51,"Product intelligence is defined as an automated system for gathering and analyzing intelligence about the performance of a product being designed and manufactured, such that this data is automatically fed back to the product managers and engineers designing the product, to assist them in the development of the next iteration or version of that product. The goal of product intelligence is to accelerate the rate of product innovation, thereby making the product and its owners more competitive. Product intelligence is usually applied to electronic products, but it is not necessarily limited to electronic products. 
Key points of this definition:
Product intelligence can also include two additional functions:
In 2002, Averna Technologies Inc. began using the term product intelligence to describe the ground-breaking functionality of its Proligent™ software, which at that time, was an add-on software product to National Instruments NI TestStand test management software, which is designed to help test engineers develop automated test and validation systems faster.
The Proligent product name was derived by combining the words product and intelligence. In 2006, Averna made available a developer’s toolkit so that test engineers could integrate the Proligent software directly to any test station using Proligent’s API, thereby broadening the Proligent customer base.
In 2009, Averna expanded the functionality of its Proligent product to include automated versioning and configuration management as well as quality enforcement capabilities. In 2011, Averna decoupled the business intelligence portion of Proligent, so that its highly specialized business intelligence solution could be integrated directly to any test database used by electronics manufacturers.
"
428224858,2011-05-09 10:31:40,"Product intelligence is defined as an automated system for gathering and analyzing intelligence about the performance of a product being designed and manufactured, such that this data is automatically fed back to the product managers and engineers designing the product, to assist them in the development of the next iteration or version of that product. The goal of product intelligence is to accelerate the rate of product innovation, thereby making the product and its owners more competitive. Product intelligence is usually applied to electronic products, but it is not necessarily limited to electronic products. 
Key points of this definition:
Product intelligence can also include two additional functions:
In 2002, Averna Technologies Inc. began using the term product intelligence to describe the ground-breaking functionality of its Proligent™ software, which at that time, was an add-on software product to National Instruments NI TestStand test management software, which is designed to help test engineers develop automated test and validation systems faster.
The Proligent product name was derived by combining the words product and intelligence. In 2006, Averna made available a developer’s toolkit so that test engineers could integrate the Proligent software directly to any test station using Proligent’s API, thereby broadening the Proligent customer base.
In 2009, Averna expanded the functionality of its Proligent product to include automated versioning and configuration management as well as quality enforcement capabilities. In 2011, Averna decoupled the business intelligence portion of Proligent, so that its highly specialized business intelligence solution could be integrated directly to any test database used by electronics manufacturers.
"
519825986,2012-10-25 20:52:33,"Product intelligence is defined as an automated system for gathering and analyzing intelligence about the performance of a product being designed and manufactured, such that this data is automatically fed back to the product managers and engineers designing the product, to assist them in the development of the next iteration or version of that product. The goal of product intelligence is to accelerate the rate of product innovation, thereby making the product and its owners more competitive. Product intelligence is usually applied to electronic products, but it is not necessarily limited to electronic products. 
Key points of this definition:
Product intelligence can also include two additional functions:
In 2002, Averna Technologies Inc. began using the term product intelligence to describe the ground-breaking functionality of its Proligent™ software, which at that time, was an add-on software product to National Instruments NI TestStand test management software, which is designed to help test engineers develop automated test and validation systems faster.
The Proligent product name was derived by combining the words product and intelligence. In 2006, Averna made available a developer’s toolkit so that test engineers could integrate the Proligent software directly to any test station using Proligent’s API, thereby broadening the Proligent customer base.
In 2009, Averna expanded the functionality of its Proligent product to include automated versioning and configuration management as well as quality enforcement capabilities. In 2011, Averna decoupled the business intelligence portion of Proligent, so that its highly specialized business intelligence solution could be integrated directly to any test database used by electronics manufacturers.
"
676930901,2015-08-20 01:26:52,"Product intelligence is defined as an automated system for gathering and analyzing intelligence about the performance of a product being designed and manufactured, such that this data is automatically fed back to the product managers and engineers designing the product, to assist them in the development of the next iteration or version of that product. The goal of product intelligence is to accelerate the rate of product innovation, thereby making the product and its owners more competitive. Product intelligence is usually applied to electronic products, but it is not necessarily limited to electronic products. 
Key points of this definition:
Product intelligence can also include two additional functions:
In 2002, Averna began using the term product intelligence to describe the functionality of its Proligent™ software, which at that time, was an add-on software product to National Instruments NI TestStand test management software, which is designed to help test engineers develop automated test and validation systems faster.
The Proligent product name was derived by combining the words product and intelligence. In 2006, Averna made available a developer’s toolkit so that test engineers could integrate the Proligent software directly to any test station using Proligent’s API, thereby broadening the Proligent customer base.
In 2009, Averna expanded the functionality of its Proligent product to include automated versioning and configuration management as well as quality enforcement capabilities. In 2011, Averna decoupled the business intelligence portion of Proligent, so that its highly specialized business intelligence solution could be integrated directly to any test database used by electronics manufacturers.
"
676930972,2015-08-20 01:27:18,"Product intelligence is defined as an automated system for gathering and analyzing intelligence about the performance of a product being designed and manufactured, such that this data is automatically fed back to the product managers and engineers designing the product, to assist them in the development of the next iteration or version of that product. The goal of product intelligence is to accelerate the rate of product innovation, thereby making the product and its owners more competitive. Product intelligence is usually applied to electronic products, but it is not necessarily limited to electronic products. 
Key points of this definition:
Product intelligence can also include two additional functions:
In 2002, Averna Technologies began using the term product intelligence to describe the functionality of its Proligent™ software, which at that time, was an add-on software product to National Instruments NI TestStand test management software, which is designed to help test engineers develop automated test and validation systems faster.
The Proligent product name was derived by combining the words product and intelligence. In 2006, Averna made available a developer’s toolkit so that test engineers could integrate the Proligent software directly to any test station using Proligent’s API, thereby broadening the Proligent customer base.
In 2009, Averna expanded the functionality of its Proligent product to include automated versioning and configuration management as well as quality enforcement capabilities. In 2011, Averna decoupled the business intelligence portion of Proligent, so that its highly specialized business intelligence solution could be integrated directly to any test database used by electronics manufacturers.
"
676931058,2015-08-20 01:27:46,"Product intelligence is defined as an automated system for gathering and analyzing intelligence about the performance of a product being designed and manufactured, such that this data is automatically fed back to the product managers and engineers designing the product, to assist them in the development of the next iteration or version of that product. The goal of product intelligence is to accelerate the rate of product innovation, thereby making the product and its owners more competitive. Product intelligence is usually applied to electronic products, but it is not necessarily limited to electronic products. 
Key points of this definition:
Product intelligence can also include two additional functions:
In 2002, Averna Technologies began using the term product intelligence to describe the functionality of its Proligent software, which at that time, was an add-on software product to National Instruments NI TestStand test management software, which is designed to help test engineers develop automated test and validation systems faster.
The Proligent product name was derived by combining the words product and intelligence. In 2006, Averna made available a developer’s toolkit so that test engineers could integrate the Proligent software directly to any test station using Proligent’s API, thereby broadening the Proligent customer base.
In 2009, Averna expanded the functionality of its Proligent product to include automated versioning and configuration management as well as quality enforcement capabilities. In 2011, Averna decoupled the business intelligence portion of Proligent, so that its highly specialized business intelligence solution could be integrated directly to any test database used by electronics manufacturers.
"
676931171,2015-08-20 01:28:22,"Product intelligence is defined as an automated system for gathering and analyzing intelligence about the performance of a product being designed and manufactured, such that this data is automatically fed back to the product managers and engineers designing the product, to assist them in the development of the next iteration or version of that product. The goal of product intelligence is to accelerate the rate of product innovation, thereby making the product and its owners more competitive. Product intelligence is usually applied to electronic products, but it is not necessarily limited to electronic products. 
Key points of this definition:
Product intelligence can also include two additional functions:
In 2002, Averna Technologies began using the term product intelligence to describe the functionality of its Proligent software, which at that time, was an add-on software product to National Instruments NI TestStand test management software, which is designed to help test engineers develop automated test and validation systems faster.
The Proligent product name was derived by combining the words product and intelligence. In 2006, Averna made available a developer’s toolkit so that test engineers could integrate the Proligent software directly to any test station using Proligent’s API, thereby broadening the Proligent customer base.
In 2009, Averna expanded the functionality of its Proligent product to include automated versioning and configuration management as well as quality enforcement capabilities. In 2011, Averna decoupled the business intelligence portion of Proligent, so that its highly specialized business intelligence solution could be integrated directly to any test database used by electronics manufacturers.
"
676931257,2015-08-20 01:29:01,"Product intelligence is defined as an automated system for gathering and analyzing intelligence about the performance of a product being designed and manufactured, such that this data is automatically fed back to the product managers and engineers designing the product, to assist them in the development of the next iteration or version of that product. The goal of product intelligence is to accelerate the rate of product innovation, thereby making the product and its owners more competitive. Product intelligence is usually applied to electronic products, but it is not necessarily limited to electronic products. 
Key points of this definition:
Product intelligence can also include two additional functions:
In 2002, Averna Technologies began using the term product intelligence to describe the functionality of its Proligent software, which at that time, was an add-on software product to National Instruments NI TestStand test management software, which is designed to help test engineers develop automated test and validation systems faster.
The Proligent product name was derived by combining the words product and intelligence. In 2006, Averna made available a developer’s toolkit so that test engineers could integrate the Proligent software directly to any test station using Proligent’s API, thereby broadening the Proligent customer base.
In 2009, Averna expanded the functionality of its Proligent product to include automated versioning and configuration management as well as quality enforcement capabilities. In 2011, Averna decoupled the business intelligence portion of Proligent, so that its highly specialized business intelligence solution could be integrated directly to any test database used by electronics manufacturers.
"
920700118,2019-10-11 09:31:59,"Product intelligence is defined as an automated system for gathering and analyzing intelligence about the performance of a product being designed and manufactured, such that this data is automatically fed back to the product managers and engineers designing the product, to assist them in the development of the next iteration or version of that product. The goal of product intelligence is to accelerate the rate of product innovation, thereby making the product and its owners more competitive and increasing customer satisfaction.[1] Product intelligence is often applied to electronic products, but it is not necessarily limited to electronic products. 
Key points of this definition:
Product intelligence can also include two additional functions:
In 2002, Averna Technologies began using the term product intelligence to describe the functionality of its Proligent software, which at that time was an add-on software product to National Instruments'[2] NI TestStand[3] test management software, which is designed to help test engineers develop automated test and validation systems faster.
The Proligent product name was derived by combining the words product and intelligence. In 2006, Averna made available a developer’s toolkit so that test engineers could integrate the Proligent software directly to any test station using Proligent’s API, thereby broadening the Proligent customer base.
In 2009, Averna expanded the functionality of its Proligent product to include automated versioning and configuration management as well as quality enforcement capabilities. In 2011, Averna decoupled the business intelligence portion of Proligent, so that its specialized business intelligence solution could be integrated directly to any test database used by electronics manufacturers.
"
920701603,2019-10-11 09:50:29,"Product intelligence is defined as an automated system for gathering and analyzing intelligence about the performance of a product being designed and manufactured, such that this data is automatically fed back to the product managers and engineers designing the product, to assist them in the development of the next iteration or version of that product. The goal of product intelligence is to accelerate the rate of product innovation, thereby making the product and its owners more competitive and increasing customer satisfaction.[1] Product intelligence is often applied to electronic products, but it is not necessarily limited to electronic products. 
Key points of this definition:
Product intelligence can also include two additional functions:
In 2002, Averna Technologies began using the term product intelligence to describe the functionality of its Proligent software, which at that time was an add-on software product to National Instruments'[2] NI TestStand[3] test management software, which is designed to help test engineers develop automated test and validation systems faster.
The Proligent product name was derived by combining the words product and intelligence. In 2006, Averna made available a developer’s toolkit so that test engineers could integrate the Proligent software directly to any test station using Proligent’s API, thereby broadening the Proligent customer base.
In 2009, Averna expanded the functionality of its Proligent product to include automated versioning and configuration management as well as quality enforcement capabilities. In 2011, Averna decoupled the business intelligence portion of Proligent, so that its specialized business intelligence solution could be integrated directly to any test database used by electronics manufacturers.
"
944256723,2020-03-06 17:52:30,"Product intelligence is defined as an automated system for gathering and analyzing intelligence about the performance of a product being designed and manufactured, such that this data is automatically fed back to the product managers and engineers designing the product, to assist them in the development of the next iteration or version of that product. The goal of product intelligence is to accelerate the rate of product innovation, thereby making the product and its owners more competitive and increasing customer satisfaction.[1] Product intelligence is often applied to electronic products, but it is not necessarily limited to electronic products. 
Key points of this definition:
Product intelligence can also include two additional functions:
In 2002, Averna Technologies began using the term product intelligence to describe the functionality of its Proligent software, which at that time was an add-on software product to National Instruments'[2] NI TestStand[3] test management software, which is designed to help test engineers develop automated test and validation systems faster.
The Proligent product name was derived by combining the words product and intelligence. In 2006, Averna made available a developer's toolkit so that test engineers could integrate the Proligent software directly to any test station using Proligent's API, thereby broadening the Proligent customer base.
In 2009, Averna expanded the functionality of its Proligent product to include automated versioning and configuration management as well as quality enforcement capabilities. In 2011, Averna decoupled the business intelligence portion of Proligent, so that its specialized business intelligence solution could be integrated directly to any test database used by electronics manufacturers.
"
947941918,2020-03-29 08:51:36,"Product intelligence is defined as an automated system for gathering and analyzing intelligence about the performance of a product being designed and manufactured, such that this data is automatically fed back to the product managers and engineers designing the product, to assist them in the development of the next iteration or version of that product. The goal of product intelligence is to accelerate the rate of product innovation, thereby making the product and its owners more competitive and increasing customer satisfaction.[1] Product intelligence is often applied to electronic products, but it is not necessarily limited to electronic products. 
Key points of this definition:
Product intelligence can also include two additional functions:
In 2002, Averna Technologies[2] began using the term product intelligence to describe the functionality of its Proligent software, which at that time was an add-on software product to National Instruments'[3] NI TestStand[4] test management software, which is designed to help test engineers develop automated test and validation systems faster.
The Proligent product name was derived by combining the words product and intelligence. In 2006, Averna made available a developer's toolkit so that test engineers could integrate the Proligent software directly to any test station using Proligent's API, thereby broadening the Proligent customer base.
In 2009, Averna expanded the functionality of its Proligent product to include automated versioning and configuration management as well as quality enforcement capabilities. In 2011, Averna decoupled the business intelligence portion of Proligent, so that its specialized business intelligence solution could be integrated directly to any test database used by electronics manufacturers.
"
951353093,2020-04-16 19:14:14,"Product intelligence is defined as an automated system for gathering and analyzing intelligence about the performance of a product being designed and manufactured, such that this data is automatically fed back to the product managers and engineers designing the product, to assist them in the development of the next iteration or version of that product. The goal of product intelligence is to accelerate the rate of product innovation, thereby making the product and its owners more competitive and increasing customer satisfaction.[1] Product intelligence is often applied to electronic products, but it is not necessarily limited to electronic products. 
Key points of this definition:
Product intelligence can also include two additional functions:
In 2002, Averna Technologies[2] began using the term product intelligence to describe the functionality of its Proligent software, which at that time was an add-on software product to National Instruments'[3] NI TestStand[4] test management software, which is designed to help test engineers develop automated test and validation systems faster.
The Proligent product name was derived by combining the words product and intelligence. In 2006, Averna made available a developer's toolkit so that test engineers could integrate the Proligent software directly to any test station using Proligent's API, thereby broadening the Proligent customer base.
In 2009, Averna expanded the functionality of its Proligent product to include automated versioning and configuration management as well as quality enforcement capabilities. In 2011, Averna decoupled the business intelligence portion of Proligent, so that its specialized business intelligence solution could be integrated directly to any test database used by electronics manufacturers.
"
951441234,2020-04-17 05:20:29,"Product intelligence is defined as an automated system for gathering and analyzing intelligence about the performance of a product being designed and manufactured, such that this data is automatically fed back to the product managers and engineers designing the product, to assist them in the development of the next iteration or version of that product. The goal of product intelligence is to accelerate the rate of product innovation, thereby making the product and its owners more competitive and increasing customer satisfaction.[1] Product intelligence is often applied to electronic products, but it is not necessarily limited to electronic products. 
Key points of this definition:
Product intelligence can also include two additional functions:
In 2002, Averna Technologies[2] began using the term product intelligence to describe the functionality of its Proligent software, which at that time was an add-on software product to National Instruments'[3] NI TestStand[4] test management software, which is designed to help test engineers develop automated test and validation systems faster.
The Proligent product name was derived by combining the words product and intelligence. In 2006, Averna made available a developer's toolkit so that test engineers could integrate the Proligent software directly to any test station using Proligent's API, thereby broadening the Proligent customer base.
In 2009, Averna expanded the functionality of its Proligent product to include automated versioning and configuration management as well as quality enforcement capabilities. In 2011, Averna decoupled the business intelligence portion of Proligent, so that its specialized business intelligence solution could be integrated directly to any test database used by electronics manufacturers.
"
964741318,2020-06-27 09:10:05,"Product intelligence is defined as an automated system for gathering and analyzing intelligence about the performance of a product being designed and manufactured, such that this data is automatically fed back to the product managers and engineers designing the product, to assist them in the development of the next iteration or version of that product. The goal of product intelligence is to accelerate the rate of product innovation, thereby making the product and its owners more competitive and increasing customer satisfaction.[1] Product intelligence is often applied to electronic products, but it is not necessarily limited to electronic products.
Key points of this definition:
Product intelligence can also include two additional functions:
In 2002, Averna Technologies[2] began using the term product intelligence to describe the functionality of its Proligent software, which at that time was an add-on software product to National Instruments'[3] NI TestStand[4] test management software, which is designed to help test engineers develop automated test and validation systems faster.
The Proligent product name was derived by combining the words product and intelligence. In 2006, Averna made available a developer's toolkit so that test engineers could integrate the Proligent software directly to any test station using Proligent's API, thereby broadening the Proligent customer base.
In 2009, Averna expanded the functionality of its Proligent product to include automated versioning and configuration management as well as quality enforcement capabilities. In 2011, Averna decoupled the business intelligence portion of Proligent, so that its specialized business intelligence solution could be integrated directly to any test database used by electronics manufacturers.
"
1017053352,2021-04-10 14:47:06,"Product intelligence is defined as an automated system for gathering and analyzing intelligence about the performance of a product being designed and manufactured, such that this data is automatically fed back to the product managers and engineers designing the product, to assist them in the development of the next iteration or version of that product. The goal of product intelligence is to accelerate the rate of product innovation, thereby making the product and its owners more competitive and increasing customer satisfaction.[1] Product intelligence is often applied to electronic products, but it is not necessarily limited to electronic products.
Key points of this definition:
Product intelligence can also include two additional functions:
In 2002, Averna Technologies[2] began using the term product intelligence to describe the functionality of its Proligent software, which at that time was an add-on software product to National Instruments'[3] NI TestStand[4] test management software, which is designed to help test engineers develop automated test and validation systems faster.
The Proligent product name was derived by combining the words product and intelligence. In 2006, Averna made available a developer's toolkit so that test engineers could integrate the Proligent software directly to any test station using Proligent's API, thereby broadening the Proligent customer base.
In 2009, Averna expanded the functionality of its Proligent product to include automated versioning and configuration management as well as quality enforcement capabilities. In 2011, Averna decoupled the business intelligence portion of Proligent, so that its specialized business intelligence solution could be integrated directly to any test database used by electronics manufacturers.
"
